<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-29T12:11:02.443Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>KaiQiang Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2017-360-人机大战比赛记录</title>
    <link href="http://yoursite.com/2018/10/15/2017-360-%E4%BA%BA%E6%9C%BA%E5%A4%A7%E6%88%98%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/10/15/2017-360-人机大战比赛记录/</id>
    <published>2018-10-15T01:50:06.000Z</published>
    <updated>2018-11-29T12:11:02.443Z</updated>
    
    <content type="html"><![CDATA[<p>CCF 大数据与计算智能大赛（BDCI）<br>【赛题四】360搜索-AlphaGo之后“人机大战”Round 2 ——机器写作与人类写作的巅峰对决<br>比赛时间2017.10-2017.12<br><a id="more"></a></p><h1 id="赛事背景"><a href="#赛事背景" class="headerlink" title="赛事背景"></a>赛事背景</h1><p>如果说AlphaGo和人类棋手的对决拉响了“人机大战”的序曲，在人类更为通识的写作领域，即将上演更为精彩的机器写作和人类写作的对决。人类拥有数万年的书写历史，人类写作蕴藏无穷的信息、情感和思想。但随着深度学习、自然语言处理等人工智能技术发展，机器写作在语言组织、语法和逻辑处理方面几乎可以接近人类水平。360搜索智能写作助手也在此背景下应运而生。<br>本次CCF大数据和人工智能大赛上，360搜索智能写作助手（机器写作）和人类写作将狭路相逢，如何辨别出一篇文章是通过庞大数据算法训练出来的机器写作的，还是浸染漫长书写历史的人类创作的？我们拭目以待！<br>本次赛题任务：挑战者能够设计出优良的算法模型从海量的文章中区分出文章是机器写作还是人类写作。</p><h1 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h1><h2 id="传统文本分类"><a href="#传统文本分类" class="headerlink" title="传统文本分类"></a>传统文本分类</h2><p>文本分类问题算是自然语言处理领域中一个非常经典的问题，目前形成了一套经典算法，这个阶段的主要是人工特征工程+浅层分类模型。训练文本分类器过程见下图：<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战1.png" alt="人机大战1"></p><h2 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h2><h3 id="CNN-方法"><a href="#CNN-方法" class="headerlink" title="CNN 方法"></a>CNN 方法</h3><p>我们首先设计了基于词项和字符的卷积神经网络模型 (Char-CNN、Word-CNN、WordChar-CNN) 来提取局部感受野的特征。根据观察，词序级别的错误或颠倒是负样本中出现次数最多的问题，而卷积神经网络中的卷积+池化操作能够很好的捕获这种 N-Gram 的错误。Char-CNN 首先对单个字进行向量表示，随后输入 三通道的卷积神经网络通过池化层提取特征，最后进行二分类的 softmax 得到每一类的概率。Word-CNN 和 WordChar-CNN 的模型与 Char-CNN 类似，只是在输入数据上，Word-CNN 读取的是分词后的词向量表示而 WordChar-CNN 综合考虑了词项和字符的特征，相当于前两个模型的集成。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战3.png" alt="人机大战3"></p><h3 id="RNN-Attention方法"><a href="#RNN-Attention方法" class="headerlink" title="RNN + Attention方法"></a>RNN + Attention方法</h3><p>除了上述词序问题，负样本还有很多语句重复或语句不连贯的问题。而带有 Attention 机制的循环神经网络模型十分适合提取这样的特征，因此我们针对此类样本设计了加入注意力机 制的循环神经网络模型 (Word-HAN) 模型，首先对分词后的词项进行向量表示，随后通过双向 GRU 来提取句子每个时序的特征，使用 Attention 机制来提取更大视野窗又的不连贯特征以及缓解长距离依赖问题，这样就可以得到句子的向量表示，最后再使用相同的方法获得文章的向量表示并进行二分类操作。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战4.png" alt="人机大战4"></p><h3 id="CNN-RNN-Attention-方法"><a href="#CNN-RNN-Attention-方法" class="headerlink" title="CNN + RNN + Attention 方法"></a>CNN + RNN + Attention 方法</h3><p>我们还思考了如何在不过分牺牲训练时间的同时尽可能地提高循环神经网络的表现性能。我们认为，传统的 LSTM、GRU 模型对句子语义较为敏感，能够很好的保留句子的语义。但是对感知相邻句子之间的错误并不敏感，如果能让它结合卷积神经网络的优点，就可以增强模型的特征提取能力。基于这种想法，我们设计了 Word-HCNN 模型，结构如图所示。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战5.png" alt="人机大战5"></p><p>Word-HCNN 综合了 Word-HANN 的和 Word-CNN 的网络结构。首先通过双向 GRU+Attention 的方法将每个句子表示成为向量，再通过多通道卷积提取相邻句子之间特征，这是句子级别的”N-Gram”，模型的输出层依旧是一个 2 分类的 softmax。</p><h1 id="样本数据分析"><a href="#样本数据分析" class="headerlink" title="样本数据分析"></a>样本数据分析</h1><p>因为分类问题中类别平衡与否对最终的分类结果会产生很大的影响，所以我们首先统计了类别分布情况。两类样本大致平衡。<br>与此同时，我们还对文章长度分布进行了统计，总体来看，几乎所有文章的长度都在 1500 个词以内，这符合我们的正常认知。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战2.png" alt="人机大战2"></p><h1 id="数据预处理与模型初始参数"><a href="#数据预处理与模型初始参数" class="headerlink" title="数据预处理与模型初始参数"></a>数据预处理与模型初始参数</h1><p>考虑到卷积神经网络只能处理固定维度的输入数据，所以我们对于长度为 $n$ 的文章，首先对文章进行截断或者补齐到固定长度 $T$，如果 $n &gt; T$，则只保留前 $T$ 个 单词，如果 $n &lt; T$，则在文章末尾添加 padding 来补齐长度。预处理之后，每篇文章都包含 T 个词。</p><p>尽管 RNN 能够处理变长的时序数据，但是考虑到矩阵操作的便利，我们依然需要固定输入的维度对数据进行预处理，与卷积神经网络不同的是，循环神经网络是以句子作为处理单元，而不是把文章中的所有词都拼在一起。对于包含 $s$ 个句子的文章，首先进行句子级别的截断或者补齐到固定数目 $S$ ，如果 $s &gt; S$ ，则只保留前 $S$ 个句子，如果 s &lt; S，则添加 padding 补齐句子数目。同样的操作也作用于每个句子，对于长度为 $n$ 的句子，首先对句子进行截断或者补齐到固定长度 $T$ ， 如果$n&gt;T$，则只保留前 $T$ 个单词，如果$n&lt;T$，则在句子末尾添加 padding 补齐长度。预处理之后，每篇文章都被包含 $S$ 个句子，每个句子都包含 $T$ 个词。</p><p>将数据集按照 6:2:2 的比例拆分成训练集，验证集和测试集三部分。文章截断 补齐句子数为 45，句子截断补齐词个数为 48，分词工具使用 jieba，输入层使用 wiki 语料通过 word2vec 预训练得到的 100 维向量，只保留频率最高的前 10000 个单词，其他单词均映射为 <unk>，Batch size 为 64。三通道卷积层使用 1 维卷积，卷积核尺寸分别为 3、4、5，每种卷积核的滤波器数目均为 256，在最大池化层之后加入 0.2 的 Dropout，GRU 的隐藏层维数为 128，Attention 的权重矩阵 $W$ 的维度是 128 × 128。输出层使用了包含 256 个神经元的单隐层全连接神经网络，并且进行了 Batch Normalization，模型所有参数的初始化都从 $[−0.1, 0.1]$ 中均匀采样，使用 adam 优化模型，损失函数为多类交叉熵损失，迭代 15 轮。</unk></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CCF 大数据与计算智能大赛（BDCI）&lt;br&gt;【赛题四】360搜索-AlphaGo之后“人机大战”Round 2 ——机器写作与人类写作的巅峰对决&lt;br&gt;比赛时间2017.10-2017.12&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP 中的attention和self-attention</title>
    <link href="http://yoursite.com/2018/10/11/NLP-%E4%B8%AD%E7%9A%84attention%E5%92%8Cself-attention/"/>
    <id>http://yoursite.com/2018/10/11/NLP-中的attention和self-attention/</id>
    <published>2018-10-11T07:08:14.000Z</published>
    <updated>2018-10-11T09:06:44.474Z</updated>
    
    <content type="html"><![CDATA[<p>本文参考张俊林博士的《深度学习中的注意力机制》和苏剑林的《Attention is All You Need》浅读（简介+代码）》，简单总结一下 NLP 中用到的 attention 和 self-attention。<br><a id="more"></a></p><h1 id="attention-的发展趋势"><a href="#attention-的发展趋势" class="headerlink" title="attention 的发展趋势"></a>attention 的发展趋势</h1><p>attention 机制的发展如下图所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention1.png" alt="self-attention1"><br>Attention机制最早是在视觉图像领域提出来的，但是真正火起来应该算是2014年google mind团队的论文《Recurrent Models of Visual Attention》，他们在RNN模型上使用了attention机制来进行图像分类。随后，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行，他们的工作算是第一个将attention机制应用到NLP领域中。接着attention机制被广泛应用在基于RNN/CNN等神经网络模型的各种NLP任务中。2017年，google机器翻译团队发表的《Attention is all you need》中大量使用了自注意力（self-attention）机制来学习文本表示。</p><h1 id="NLP-中的序列编码"><a href="#NLP-中的序列编码" class="headerlink" title="NLP 中的序列编码"></a>NLP 中的序列编码</h1><p>深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵$X=(x_1,x_2,…,x_t)$，其中$x_i$都代表着第$i$个词的词向量（行向量），维度为$d$维，故$X\in\mathbb{R}^{n×d}$。这样的话，问题就变成了编码这些序列了。</p><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：</p><script type="math/tex; mode=display">y_t=f(y_{t−1},x_t)</script><p>不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是<strong>无法并行</strong>，因此速度较慢，这是递归的天然缺陷。另外苏剑林提到：</p><blockquote><p>RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。</p></blockquote><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是</p><script type="math/tex; mode=display">y_t=f(x_{t−1},x_t,x_{t+1})</script><p>在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例。CNN方便并行，而且容易捕捉到一些全局的结构信息。</p><h2 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h2><p>Google的大作提供了第三个思路：纯Attention！单靠注意力就可以！RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：</p><script type="math/tex; mode=display">y_t=f(x_t,A,B)</script><p>其中$A$,$B$是另外一个序列（矩阵）。如果都取$A=B=X$，那么就称为Self-Attention，它的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$！</p><h1 id="attention-1"><a href="#attention-1" class="headerlink" title="attention"></a>attention</h1><p><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention2.png" alt="self-attention2"></p><p>将Source中的构成元素想象成是由一系列的&lt; Key,Value &gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><script type="math/tex; mode=display">Attention(Query,Source)=\sum_{i=1}^{L_x}{Similarity(Query,Key_i)*Value_i}</script><p>其中， $L_x=||Source||$代表Source的长度。在机器翻译中，Source中的Key和Value指向的是同一个东西（输入句子中每个单词对应的语义编码）。</p><h2 id="attention计算过程"><a href="#attention计算过程" class="headerlink" title="attention计算过程"></a>attention计算过程</h2><ol><li>根据Query和Key计算两者的相似性或者相关性；</li><li>对第一阶段的原始分值进行归一化处理，得到权重系数；</li><li>根据权重系数对Value进行加权求和。</li></ol><p><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention3.png" alt="self-attention3"></p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个$Key_i$，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention4.png" alt="self-attention4"><br>第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><script type="math/tex; mode=display">a_i=Softmax(Sim_i)=\frac{e^{Sim_i}}{\sum_{j=1}^{L_x}e^{Sim_j}}</script><p>第二阶段的计算结果$a_i$即为$Value_i$对应的权重系数，然后进行加权求和即可得到Attention数值：</p><script type="math/tex; mode=display">Attention(Query,Source)=\sum_{i=1}^{L_x}{a_i*Value_i}</script><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h1><p>Google的论文模型的整体结构如下图，还是由编码器和解码器组成，在编码器的一个网络块中，由一个多头attention子层和一个前馈神经网络子层组成，整个编码器栈式搭建了N个块。类似于编码器，只是解码器的一个网络块中多了一个多头attention层。为了更好的优化深度网络，整个网络使用了残差连接和对层进行了规范化（Add&amp;Norm）。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention6.png" alt="self-attention6"></p><h2 id="scaled-dot-Product-attention"><a href="#scaled-dot-Product-attention" class="headerlink" title="scaled dot-Product attention"></a>scaled dot-Product attention</h2><p>对比我在前面背景知识里提到的attention的一般形式，其实scaled dot-Product attention就是我们常用的使用点积进行相似度计算的attention，只是多了一个$\sqrt{d_k}$，$\sqrt{d_k}$为$K$的维度,起到调节作用，使得内积不至于太大。</p><script type="math/tex; mode=display">Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}</script><p><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention5.png" alt="self-attention5"></p><h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/self-attention7.png" alt="self-attention7"></p><p>多头attention（Multi-head attention）结构如上图，Query，Key，Value首先经过一个线性变换，然后输入到scaled dot-Product attention，注意这里要做h次，其实也就是所谓的多头，每一次算一个头。而且每次Q，K，V进行线性变换的参数W是不一样的。</p><script type="math/tex; mode=display">head_i = Attention(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)</script><p>然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。</p><script type="math/tex; mode=display">MultiHead(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Concat(head_1,...,head_h)</script><p>可以看到，google提出来的多头attention的不同之处在于进行了h次计算而不仅仅算一次，论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。这里的设计类似于 CNN 的多个卷积核。</p><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h2><p>所谓Self Attention，其实就是$Attention(X,X,X)$，$X$就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。更准确来说，Google所用的是Self Multi-Head Attention：</p><script type="math/tex; mode=display">\boldsymbol{Y}=MultiHead(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})</script><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>但是这样的模型并不能捕捉<strong>序列的顺序</strong>！换句话说，如果将$K,V$按行打乱顺序（相当于句子中的词序打乱），那么Attention的结果还是一样的。这就表明了，到目前为止，Attention模型顶多是一个非常精妙的“词袋模型”而已。</p><p>于是Google再又提出了Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。</p><p>在以往的Position Embedding中，基本都是根据任务训练出来的向量。而Google直接给出了一个构造Position Embedding的公式:</p><script type="math/tex; mode=display">\left\{\begin{aligned}&PE_{2i}(p)=\sin\Big(p/10000^{2i/{d_{pos}}}\Big)\\ &PE_{2i+1}(p)=\cos\Big(p/10000^{2i/{d_{pos}}}\Big) \end{aligned}\right.</script><p>将id为$p$的位置映射为一个$d_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE_i(p)$。</p><p>结合位置向量和词向量有几个可选方案，可以把它们拼接起来作为一个新向量，也可以把位置向量定义为跟词向量一样大小，然后两者加起来。FaceBook的论文和Google论文中用的都是后者。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="attention-2"><a href="#attention-2" class="headerlink" title="attention:"></a>attention:</h2><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras:"></a>Keras:</h3><p><a href="https://github.com/fuliucansheng/360/blob/master/models/deepzoo.py" target="_blank" rel="noopener">https://github.com/fuliucansheng/360/blob/master/models/deepzoo.py</a></p><h2 id="self-attention-1"><a href="#self-attention-1" class="headerlink" title="self-attention:"></a>self-attention:</h2><h3 id="tf的实现："><a href="#tf的实现：" class="headerlink" title="tf的实现："></a>tf的实现：</h3><p><a href="https://github.com/bojone/attention/blob/master/attention_tf.py" target="_blank" rel="noopener">https://github.com/bojone/attention/blob/master/attention_tf.py</a></p><h3 id="Keras版"><a href="#Keras版" class="headerlink" title="Keras版:"></a>Keras版:</h3><p><a href="https://github.com/bojone/attention/blob/master/attention_keras.py" target="_blank" rel="noopener">https://github.com/bojone/attention/blob/master/attention_keras.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文参考张俊林博士的《深度学习中的注意力机制》和苏剑林的《Attention is All You Need》浅读（简介+代码）》，简单总结一下 NLP 中用到的 attention 和 self-attention。&lt;br&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>创建第一个Fabric应用中各文件作用</title>
    <link href="http://yoursite.com/2018/10/10/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFabric%E5%BA%94%E7%94%A8%E4%B8%AD%E5%90%84%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8/"/>
    <id>http://yoursite.com/2018/10/10/创建第一个Fabric应用中各文件作用/</id>
    <published>2018-10-10T09:09:12.000Z</published>
    <updated>2018-10-11T07:10:26.476Z</updated>
    
    <content type="html"><![CDATA[<p>本文接着上次创建第一个 Fabric 应用,简单介绍一下流程中各个文件的作用.<br><a id="more"></a></p><h2 id="startFabric-sh脚本中做的事情"><a href="#startFabric-sh脚本中做的事情" class="headerlink" title="./startFabric.sh脚本中做的事情:"></a>./startFabric.sh脚本中做的事情:</h2><h3 id="1-清除keystore"><a href="#1-清除keystore" class="headerlink" title="1.清除keystore"></a>1.清除keystore</h3><pre><code>`rm -rf ./hfc-key-store`</code></pre><h3 id="2-启动-Fabric-创建一个channel-并把peer添加到channel"><a href="#2-启动-Fabric-创建一个channel-并把peer添加到channel" class="headerlink" title="2.启动 Fabric; 创建一个channel,并把peer添加到channel"></a>2.启动 Fabric; 创建一个channel,并把peer添加到channel</h3><ul><li>在 ../basic-network中执行./start.sh</li><li>启动 Fabric: 在start.sh中</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.yml up -d ca.example.com orderer.example.com peer0.org1.example.com couchdb</span><br></pre></td></tr></table></figure><ul><li>创建 channel:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp&quot; peer0.org1.example.com peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx</span><br></pre></td></tr></table></figure><ul><li>把 peer 加入到刚刚创建的 channel 中:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp&quot; peer0.org1.example.com peer channel join -b mychannel.block</span><br></pre></td></tr></table></figure><h3 id="3-启动CLI-container-安装并实例化-chaincode"><a href="#3-启动CLI-container-安装并实例化-chaincode" class="headerlink" title="3.启动CLI container,安装并实例化 chaincode"></a>3.启动CLI container,安装并实例化 chaincode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp&quot; cli peer chaincode install -n fabcar -v 1.0 -p &quot;$CC_SRC_PATH&quot; -l &quot;$LANGUAGE&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp&quot; cli peer chaincode instantiate -o orderer.example.com:7050 -C mychannel -n fabcar -l &quot;$LANGUAGE&quot; -v 1.0 -c &apos;&#123;&quot;Args&quot;:[&quot;&quot;]&#125;&apos; -P &quot;OR (&apos;Org1MSP.member&apos;,&apos;Org2MSP.member&apos;)&quot;</span><br></pre></td></tr></table></figure><h3 id="4-调用initLedger函数在-ledger-上生成10个cars"><a href="#4-调用initLedger函数在-ledger-上生成10个cars" class="headerlink" title="4.调用initLedger函数在 ledger 上生成10个cars"></a>4.调用initLedger函数在 ledger 上生成10个cars</h3><p>initLedger 函数定义在fabric-samples/chaincode/fabcar/go/fabcar.go 中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp&quot; cli peer chaincode invoke -o orderer.example.com:7050 -C mychannel -n fabcar -c &apos;&#123;&quot;function&quot;:&quot;initLedger&quot;,&quot;Args&quot;:[&quot;&quot;]&#125;&apos;</span><br></pre></td></tr></table></figure><h2 id="enrollAdmin-js做的事情"><a href="#enrollAdmin-js做的事情" class="headerlink" title="enrollAdmin.js做的事情:"></a>enrollAdmin.js做的事情:</h2><p>注册 admin 用户</p><h2 id="registerUser-js做的事情"><a href="#registerUser-js做的事情" class="headerlink" title="registerUser.js做的事情:"></a>registerUser.js做的事情:</h2><p>使用 admin 用户注册和登记用户 user1</p><h2 id="query-js-做的事情"><a href="#query-js-做的事情" class="headerlink" title="query.js 做的事情:"></a>query.js 做的事情:</h2><p> 查询 ledger</p><h2 id="invoke-js做的事情"><a href="#invoke-js做的事情" class="headerlink" title="invoke.js做的事情:"></a>invoke.js做的事情:</h2><p>更新 ledger</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文接着上次创建第一个 Fabric 应用,简单介绍一下流程中各个文件的作用.&lt;br&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
      <category term="Fabric" scheme="http://yoursite.com/tags/Fabric/"/>
    
  </entry>
  
  <entry>
    <title>创建第一个Fabric应用</title>
    <link href="http://yoursite.com/2018/10/08/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFabric%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2018/10/08/创建第一个Fabric应用/</id>
    <published>2018-10-08T06:15:06.000Z</published>
    <updated>2018-10-11T07:09:22.208Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上创建了第一个Fabric应用fabcar。<br><a id="more"></a></p><h2 id="设置开发环境"><a href="#设置开发环境" class="headerlink" title="设置开发环境"></a>设置开发环境</h2><p>首先配置开发环境，下载fabric-samples代码以及镜像文件（与搭建网络操作一样）。<br>进入facar目录，有以下文件：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar1.png" alt="facar1"><br>在开始之前，我们先做些清理工作。执行以下命令以关闭旧的或者启动着的容器：<br><code>$ docker rm -f $(docker ps -aq)</code><br>清除网络中的缓存：<br><code>$ docker network prune</code></p><h2 id="安装客户端-amp-启动网络"><a href="#安装客户端-amp-启动网络" class="headerlink" title="安装客户端&amp;启动网络"></a>安装客户端&amp;启动网络</h2><p><strong>接下来的操作都在facar目录下</strong><br>执行以下命令为应用安装Fabric依赖。fabric-ca-client允许我们的app连接CA server并且检索身份材料。fabric-client允许我们获取身份材料，并且与节点和排序服务对话。<br><code>$ npm install</code><br>这一步在此处会有延迟，耐心等待一下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar2.png" alt="facar2"><br>使用startFabric.sh脚本启动网络。这个命令会初始化各种Fabric实体，还会启动一个使用Golang编写的智能合约容器。<br><code>$ ./startFabric.sh</code><br>这个脚本做了以下事情(详细介绍在<a href="https://zkq1314.github.io/2018/10/10/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFabric%E5%BA%94%E7%94%A8%E4%B8%AD%E5%90%84%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8/#more" target="_blank" rel="noopener">这里</a>)：</p><ul><li>启动一个peer节点，order节点，证书和CLI容器</li><li>创建一个channel，并把peer添加到channel</li><li>安装智能合约（例如chaincode）到peer的文件系统，然后初始化chaincode到channel，实例化一个chaincode容器</li><li>调用initLedger方法来初始化10辆汽车到channel账本</li></ul><p>可以通过<code>$ docker ps</code>命令来查看当前脚本运行后的进程情况.</p><h2 id="登记管理员用户"><a href="#登记管理员用户" class="headerlink" title="登记管理员用户"></a>登记管理员用户</h2><p>当我们启动网络时，我们通过Certificate Authority注册了一个管理员用户admin。现在我们需要向CA服务器发送一个登记请求，然后为其取回一个登记证书。这里我们不深入登记的细节，只要知道这个证书是构成管理员用户的必要条件。我们随后会使用这个管理员来注册和登记新的用户。现在向CA服务器发送管理员登记请求：<br><code>$ node enrollAdmin.js</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar3.png" alt="facar3"></p><p>这行代码会调用一个证书签名请求(CSR)，最后会在项目根目录生成一个新的文件夹<strong>hfc-key-stor</strong>e，里面包含了证书和密钥材料。当我们的app需要创建和读取不同身份用户时，需要定位到此文件夹。</p><h2 id="User1的注册和登记-Register-and-Enroll-user1"><a href="#User1的注册和登记-Register-and-Enroll-user1" class="headerlink" title="User1的注册和登记(Register and Enroll user1)"></a>User1的注册和登记(Register and Enroll user1)</h2><p>使用刚刚生成的管理员证书，我们再一次联通CA服务器来注册和登记一个新用户。user1是我们用来查询和更新账本的用户。这里着重说明的是，admin发起了新用户的注册和登记工作(就好像admin扮演了登记员的角色)。现在为admin发起登记和注册请求：<br><code>$ node registerUser.js</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar4.png" alt="facar4"><br>和管理员登记一样，上面的指令会调用CSR然后将证书和密钥放入<strong>hfc-key-store</strong>文件夹中。现在我们有了两个用户的身份材料。</p><h2 id="查询账本-Querying-the-Ledger"><a href="#查询账本-Querying-the-Ledger" class="headerlink" title="查询账本(Querying the Ledger)"></a>查询账本(Querying the Ledger)</h2><p>查询是指如何从账本中读取数据。您可以查询单个或者多个键的值，如果账本是以类似于JSON这样的数据存储格式写入的，则可以执行更复杂的搜索（如查找包含某些关键字的所有资产）。</p><p>下图是一个查询流程的示意图：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar5.png" alt="facar5"><br>首先，运行query.js 程序<code>$ node query.js</code>，返回账本上所有汽车列表。我们使用user1作为签名实体。我们的程序中已经指定了user1为签名实体：<br><code>fabric_client.getUserContext(&#39;user1&#39;, true);</code></p><p>user1的登记材料已经放在了hfc-key-store文件夹中，我们只需要简单的告诉程序去获取它就行了。在定义了用户对象后，我们继续读取账本的流程。queryAllCars这个方法已经被提前定义在了app中，它可以查询所有的cars.返回应如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar6.png" alt="facar6"><br>这里有10辆车，一辆属于Adriana的黑色Tesla Model S、一辆属于Brad的红色Ford Mustang、一辆属于Pari的紫罗兰色Fiat Punto等等。账本是基于Key/Value 的，在这里，关键字是从CAR0到CAR9。</p><p>query.js中包括：</p><ol><li>初始部分定义了变量，如链码，通道名称和网络端点。在我们的app中，这些变量已经定义好了，但是在真实的开发中，这些变量应该又开发者指定。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar7.png" alt="facar7"></li><li>构建查询的代码块<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar8.png" alt="facar8"><br>当程序运行时，它会调用节点上的fabcar链码，执行queryAllCars函数，不传任何参数。</li></ol><p>我们再转至到fabric-samples子目录chaincode/fabcar/go，并在编辑器中打开fabcar.go，查看queryAllCars函数是如何与账本进行交互的。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar9.png" alt="facar9"><br>这里定义了queryAllCars的范围。在CAR0和CAR999的每辆车。因此，我们理论上可以创建1,000辆汽车，queryAllCars函数将会显示出每一辆汽车的信息。</p><p>现在我们返回query.js程序并编辑请求构造函数以查询特定的车辆。为达此目的，我们将函数queryAllCars更改为queryCar并将特定的“Key” 传递给args参数。在这里，我们使用CAR4。 所以我们编辑后的query.js程序现在应该包含以下内容：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar10.png" alt="facar10"><br>再次执行程序得到一辆车的结果：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar11.png" alt="facar11"><br>这样，我们就从查询所有车变成了只查询一辆车：Adriana的黑色Tesla Model S。使用queryCar函数，我们可以查询任意关键字（例如CAR0），并获得与该车相对应的制造厂商、型号、颜色和所有者。</p><h2 id="更新账本-Updating-the-Ledger"><a href="#更新账本-Updating-the-Ledger" class="headerlink" title="更新账本(Updating the Ledger)"></a>更新账本(Updating the Ledger)</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/facar12.png" alt="facar12"></p><ol><li>账本更新是从生成交易提案的应用程序开始的。就像查询一样，我们将会构造一个请求，用来识别要进行交易的通道ID、函数以及智能合约。该程序然后调用channel.SendTransactionProposalAPI将交易建议发送给peer(s)进行认证。</li><li>网络（即endorsing peer）返回一个提案答复，应用程序以此来创建和签署交易请求。该请求通过调用channel.sendTransaction API发送到排序服务器。排序服务器将把交易打包进区块，然后将区块“发送”到通道上的所有peers进行认证。（在我们的例子中，我们只有一个endorsing peer。）</li><li>最后，应用程序使用eh.setPeerAddr API连接到peer的事务监听端口，并调用eh.registerTxEvent注册与特定交易ID相关联的事务。该API使得应用程序获得事务的结果（即成功提交或不成功）。把它当作一个通知机制。<br>我们初始调用的目标是简单地创建一个新的汽车。我们有一个独立的用于这些交易的JavaScript程序 - invoke.js。就像查询一样，使用编辑器打开程序并转到构建调用的代码块：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar13.png" alt="facar13"><br>我们可以调用函数createCar或者changeCarOwner。首先我们创建一个红色的Chevy Volt，并把它归属于Nick。在账本中我们的Key值已经用到了CAR9 ，所以这里我们将使用CAR10。更新代码块如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar14.png" alt="facar14"><br>保存并运行程序，得到输出：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar15.png" alt="facar15"><br>回到query.js，然后修改参数CAR4为CAR10。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar16.png" alt="facar16"><br>得到以上输出说明我们CAR10创建成功。<br>最后，我们来调用最后一个函数changeCarOwner。Nick很慷慨，他想把他的Chevy Volt送给Dave。所以，我们简单编辑invoke.js 如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar17.png" alt="facar17"><br>第一个参数定义了哪辆车被变更主人。第二个参数定义了新主人姓名。<br>保存并执行，得到输出结果：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar18.png" alt="facar18"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/facar19.png" alt="facar19"><br>真实情况下，链码需要权限控制。例如只有某些具有权限的人才能创造新车，也应该只有车主才能转让汽车所有权。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上创建了第一个Fabric应用fabcar。&lt;br&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
      <category term="Fabric" scheme="http://yoursite.com/tags/Fabric/"/>
    
  </entry>
  
  <entry>
    <title>Hyperledger fabric Building Your First Network on MacOS</title>
    <link href="http://yoursite.com/2018/09/30/Hyperledger-fabric-Building-Your-First-Network-on-MacOS/"/>
    <id>http://yoursite.com/2018/09/30/Hyperledger-fabric-Building-Your-First-Network-on-MacOS/</id>
    <published>2018-09-30T03:26:04.000Z</published>
    <updated>2018-11-12T09:27:56.994Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上实现了Building Your First Network。<br><a id="more"></a></p><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>按照官网的要求<a href="https://hyperledger-fabric.readthedocs.io/en/latest/prereqs.html" target="_blank" rel="noopener">Prerequisites</a>，需要安装docker，Go语言以及Node等。本机配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker --version</span><br><span class="line">Docker version 18.06.1-ce, build e68fc7a</span><br><span class="line">$ docker-compose --version</span><br><span class="line">docker-compose version 1.22.0, build f46880f</span><br><span class="line">$ go version</span><br><span class="line">go version go1.11 darwin/amd64</span><br><span class="line">$ node --version</span><br><span class="line">v10.8.0</span><br><span class="line">$ npm --version</span><br><span class="line">6.2.0</span><br></pre></td></tr></table></figure><p>其中，</p><ul><li>docker我下载的安装包安装的，安装过程中也安装了docker-compose.</li><li><p>go是通过brew install go安装的，记得添加环境变量。<br><code>vim ~/.bash_profile</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export GOPATH=$HOME/go</span><br><span class="line">export PATH=$PATH:$GOPATH/bin</span><br></pre></td></tr></table></figure></li><li><p>node是下载的pkg安装的。</p></li></ul><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>工具都准备好之后，接下来开始下载镜像，把这个<a href="https://raw.githubusercontent.com/hyperledger/fabric/v1.0.5/scripts/bootstrap.sh" target="_blank" rel="noopener">页面</a>下的内容全部保存到新建的images.sh文件中。</p><p>因为我们使用的是1.1.0版本，把其中的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export VERSION=$&#123;1:-1.0.5&#125;</span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export VERSION=$&#123;1:-1.1.0-preview&#125;</span><br></pre></td></tr></table></figure><p>然后执行，下载镜像（用华严的网一直无法下载，vpn也开了，晚上回到学校用的教育网下载没问题）：</p><p><code>sh images.sh</code></p><p>下载好之后，用以下命令来查看镜像是否都下载成功：</p><p><code>docker images</code></p><p><img src="http://p5vuwy2ht.bkt.clouddn.com/byfn1.png" alt="byfn1"></p><h2 id="下载Fabric-Samples源码"><a href="#下载Fabric-Samples源码" class="headerlink" title="下载Fabric-Samples源码"></a>下载Fabric-Samples源码</h2><p>从<a href="https://github.com/hyperledger/fabric-samples" target="_blank" rel="noopener">此处</a>下载。</p><h2 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h2><p><code>cd first-network</code><br><code>./byfn.sh generate</code><br><code>./byfn.sh up</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/byfn2.png" alt="byfn2"><br>出现以下画面说明示例运行成功。<br><code>./byfn.sh down</code></p><h2 id="examples-e2e-cli"><a href="#examples-e2e-cli" class="headerlink" title="examples/e2e_cli"></a>examples/e2e_cli</h2><p>参考 <a href="https://www.jianshu.com/p/e108cf655c0f" target="_blank" rel="noopener">https://www.jianshu.com/p/e108cf655c0f</a><br>从git上拉取Hyperledger Fabric:<br><code>git clone git@github.com:hyperledger/fabric.git</code><br>进入项目文件夹，查看 tag：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">git tag</span><br><span class="line">baseimage-v0.0.11</span><br><span class="line">v0.6.0-preview</span><br><span class="line">v0.6.1-preview</span><br><span class="line">v1.0.0</span><br><span class="line">v1.0.0-alpha</span><br><span class="line">v1.0.0-alpha2</span><br><span class="line">v1.0.0-beta</span><br><span class="line">v1.0.0-rc1</span><br><span class="line">v1.0.1</span><br><span class="line">v1.0.2</span><br><span class="line">v1.0.3</span><br><span class="line">v1.0.4</span><br><span class="line">v1.0.5</span><br><span class="line">v1.0.6</span><br><span class="line">v1.1.0</span><br><span class="line">v1.1.0-alpha</span><br><span class="line">v1.1.0-preview</span><br><span class="line">v1.1.0-rc1</span><br></pre></td></tr></table></figure><p>上面的tag表示相应的fabric项目的版本， fabric项目现在还处于早期发展阶段， 修改频繁且不一定能向下兼容， 所以在继续之前请先确定一个版本，避免后面踩坑。 笔者在这里使用v1.0.0。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git checkout v1.0.0</span><br><span class="line">git branch</span><br><span class="line">* (HEAD detached at v1.0.0)</span><br><span class="line">release-1.1</span><br></pre></td></tr></table></figure><h2 id="启动项目"><a href="#启动项目" class="headerlink" title="启动项目"></a>启动项目</h2><p>进入fabric文件夹下的examples/e2e_cli, 下面我们要测试e2e_cli这个demo。<br>执行download-dockerimage.sh，程序将会通过docker拉取项目所需镜像, 为了统一版本，请指定拉取镜像的版本号:</p><p><code>chmod +x download-dockerimages.sh</code><br><code>./download-dockerimages.sh -c x86_64-1.0.0 -f x86_64-1.0.0</code></p><p>现在执行完整脚本:</p><p><code>./network_setup.sh up &lt;channel-ID&gt;</code></p><p>执行成功后使用以下命令终止网络：</p><p><code>./network_setup.sh down</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上实现了Building Your First Network。&lt;br&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>Hyperledger fabric v1.0.0 环境部署过程</title>
    <link href="http://yoursite.com/2018/09/27/Hyperledger-fabric-v1-0-0-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/</id>
    <published>2018-09-27T02:23:36.000Z</published>
    <updated>2018-09-30T06:26:29.815Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于 Hyperledger fabric 的官方文档来搭建其实验环境，但官方文档对于很多步骤都有省略，结合网上比较新的博客，在一台 Ubuntu 14.04 机器(没用通过测试，换成了16.04的虚拟机)上来安装部署 fabric 的环境。<br><a id="more"></a><br>查看系统版本<br><code>$ cat /proc/version</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署1.png" alt="Fabric环境部署1"><br>切换至root用户<br><code>sudo -i</code></p><h2 id="更换-apt-源"><a href="#更换-apt-源" class="headerlink" title="更换 apt 源"></a>更换 apt 源</h2><p>先备份 sources.list 文件：<br><code>$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</code><br>再修改 sources.list 文件(全部替换即可)，换成阿里云的国内源：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/apt/sources.list</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class="line">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure></p><p>最后更新一下源：<br><code>$ sudo apt-get update</code></p><h2 id="安装-curl"><a href="#安装-curl" class="headerlink" title="安装 curl"></a>安装 curl</h2><p>Ubuntu一般默认是安装了 curl 的，可以通过以下命令验证：</p><p><code>$ curl -V</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署2.png" alt="Fabric环境部署2"></p><p>如果没有安装，则通过 apt-get 安装：</p><p><code>$ sudo apt-get install curl</code></p><h2 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h2><ul><li><p>由于 apt 源使用HTTPS以确保软件下载过程中不被篡改。因此，我们首先需要添加使用HTTPS传输的软件包以及CA证书。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    software-properties-common</span><br></pre></td></tr></table></figure></li><li><p>为了确认所下载软件包的合法性，需要添加软件源的 GPG 秘钥<br><code>$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</code></p></li><li><p>然后，我们需要向 sources.list 中添加 Docker 软件源</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo add-apt-repository \</span><br><span class="line">    &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \</span><br><span class="line">    $(lsb_release -cs) \</span><br><span class="line">    stable&quot;</span><br></pre></td></tr></table></figure><p>  以上命令会添加稳定版本的Docker CE apt 镜像源。</p></li><li><p>更新 apt 软件包缓存，并安装 docker-ce：</p><p>  <code>$ sudo apt-get update</code><br>  <code>$ sudo apt-get install docker-ce</code></p></li><li><p>查看 Docker 版本：</p><p>  <code>$ docker -v</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署3.png" alt="Fabric环境部署3"></p><p>  满足官方文档中 Docker version 17.06.2-ce or greater is required 的要求。</p></li><li><p>启动 Docker CE<br>Ubuntu 16.04 使用：<br><code>$ sudo systemctl enable docker</code><br><code>$ sudo systemctl start docker</code></p><p>  Ubuntu 14.04 使用：<br><code>$service docker start</code></p></li><li><p>建立 docker 用户组</p><p>  默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。<br>  <code>$ sudo groupadd docker</code><br>其实一般按照上面的方法安装 Docker 后就已经创建好 docker 用户组了，可以使用 <code>$ cat /etc/group | grep docker</code> 命令来验证，所以就不需要再建立 docker 用户组了，再建立也会报错提示用户组已存在的。</p></li><li><p>将当前用户加入 docker 用户组：</p><p>  <code>$ sudo usermod -aG docker $USER</code><br>下次登录时即可方便的使用 docker 命令。</p></li><li><p>测试 Docker 是否安装正确</p><p>  <code>$ docker run hello-world</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署4.png" alt="Fabric环境部署4"></p></li><li><p>配置镜像加速器</p><p>  国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：</p><ol><li>Docker 官方提供的中国 registry mirror <a href="https://registry.docker-cn.com" target="_blank" rel="noopener">https://registry.docker-cn.com</a></li><li>七牛云加速器 <a href="https://reg-mirror.qiniu.com/" target="_blank" rel="noopener">https://reg-mirror.qiniu.com/</a></li></ol></li></ul><pre><code>当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。我们以 Docker 官方加速器 https://registry.docker-cn.com 为例进行介绍。在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在则创建）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    &#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https://registry.docker-cn.com&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>之后重新启动服务</code></pre><p>Ubuntu16.04:<br><code>$ sudo systemctl daemon-reload</code><br><code>$ sudo systemctl restart docker</code><br>Ubuntu14.04：<br><code>$ service docker restart</code></p><h2 id="安装-Docker-Compose"><a href="#安装-Docker-Compose" class="headerlink" title="安装 Docker Compose"></a>安装 Docker Compose</h2><p>通过二进制包来安装，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。</p><pre><code>`$ sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose``$ sudo chmod +x /usr/local/bin/docker-compose`</code></pre><ul><li><p>查看 Docker compose 版本</p><p>  <code>$ docker-compose --version</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署5.png" alt="Fabric环境部署5"></p><p>  满足官方文档中 Docker Compose version 1.14.0 or greater 的要求。</p></li></ul><h2 id="安装-Go-语言环境"><a href="#安装-Go-语言环境" class="headerlink" title="安装 Go 语言环境"></a>安装 Go 语言环境</h2><p>Hyperledger Fabric 在很多组件中使用了 Go 语言，并且 Hyperledger fabric 1.2.0 要求使用的是 GO version 1.10.x ，所以需要在我们的环境中安装对应的 Go 语言。</p><ul><li><p>从官网下载 1.10.x 版本的 Linux 平台的源码包</p><p>  <code>$ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz</code></p></li><li><p>解压到指定目录</p><p>  <code>$ sudo tar zxvf go1.10.3.linux-amd64.tar.gz -C /usr/local/</code></p></li><li><p>先创建 Go 的工作目录</p><p>  <code>$ mkdir ~/go</code></p></li><li><p>配置环境变量</p><p>  <code>$ vi ~/.bashrc</code><br>添加</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export GOROOT=/usr/local/go</span><br><span class="line">export GOPATH=/home/zkq/go</span><br><span class="line">export PATH=$PATH:$GOROOT/bin:$GOPATH/bin</span><br></pre></td></tr></table></figure><p>  保存并使生效：</p><p>  <code>$ source ~/.bashrc</code></p></li><li><p>测试 Go 的 demo 程序</p><p>  <code>$ cd ~/go</code></p><p>  <code>$ vi hello.go</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import &quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">fmt.Printf(&quot;hello world\n&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  <code>$ go build hello.go</code></p><p>  <code>$ ./hello</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署6.png" alt="Fabric环境部署6"></p><p>  在这里遇到一个问题（在虚拟机上没遇到这个问题）：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署7.png" alt="Fabric环境部署7"><br>在网上查到的解决办法是：<a href="https://stackoverflow.com/questions/21510714/go-cannot-find-package-fmt-error" target="_blank" rel="noopener">解决办法</a><br><code>$ unset GOROOT</code></p></li></ul><h2 id="Fabric-源码下载"><a href="#Fabric-源码下载" class="headerlink" title="Fabric 源码下载"></a>Fabric 源码下载</h2><ul><li><p>首先创建存放源码的文件夹：</p><p>  <code>$ mkdir -p ~/go/src/github.com/hyperledger</code></p></li><li>进入刚创建的目录：<br><code>$cd ~/go/src/github.com/hyperledger</code></li><li><p>使用 Git 下载完整源码（有点慢）：</p><p>  <code>$ git clone https://github.com/hyperledger/fabric.git</code></p></li><li><p>进入 fabric 目录查看版本分支并切换分支：</p><p>  <code>$ cd fabric</code><br><code>$ git branch</code></p></li><li>release-1.2<br><code>$ git checkout v1.0.0</code><br>由于在 release-1.2 版本中碰到没有解决的问题，所以先切换到 v1.0.0 来完成搭建并测试的过程。</li></ul><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署8.png" alt="Fabric环境部署8"></p><h2 id="Fabric-Docker-镜像下载"><a href="#Fabric-Docker-镜像下载" class="headerlink" title="Fabric Docker 镜像下载"></a>Fabric Docker 镜像下载</h2><p>进入 ~/go/src/github.com/hyperledger/fabrci/examples/e2e_cli/ 目录，完成镜像下载，执行命令：</p><p><code>$ cd ~/go/src/github.com/hyperledger/fabrci/examples/e2e_cli/</code><br><code>$ ls</code><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署9.png" alt="Fabric环境部署9"></p><p><code>$ source download-dockerimages.sh -c x86_64-1.0.0 -f x86_64-1.0.0</code>  (时间较久，耐心等待)<br><code>$ docker image list</code></p><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric环境部署10.png" alt="Fabric环境部署10"></p><h2 id="启动-fabric-网络并完成-chaincode-测试"><a href="#启动-fabric-网络并完成-chaincode-测试" class="headerlink" title="启动 fabric 网络并完成 chaincode 测试"></a>启动 fabric 网络并完成 chaincode 测试</h2><p>还是在刚刚的 e2e_cli 文件加下，执行：</p><p><code>$ ./network_setup.sh up</code></p><p>最后出现上面字符说明 fabric 网络已经启动并完成了 chaincode 的测试。这一步没有成功，所以上面图片暂时没有。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于 Hyperledger fabric 的官方文档来搭建其实验环境，但官方文档对于很多步骤都有省略，结合网上比较新的博客，在一台 Ubuntu 14.04 机器(没用通过测试，换成了16.04的虚拟机)上来安装部署 fabric 的环境。&lt;br&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>HyperLedger Fabric架构篇</title>
    <link href="http://yoursite.com/2018/09/26/HyperLedger-Fabric%E6%9E%B6%E6%9E%84%E7%AF%87/"/>
    <id>http://yoursite.com/2018/09/26/HyperLedger-Fabric架构篇/</id>
    <published>2018-09-26T01:41:09.000Z</published>
    <updated>2018-09-26T03:11:03.818Z</updated>
    
    <content type="html"><![CDATA[<p>调研了一下HyperLedger Fabric的架构，因为它的官网文档并没有及时更新，所以有的内容参考的还是以前版本的文档。<br><a id="more"></a></p><h2 id="chaincode-可以用Go-java开发"><a href="#chaincode-可以用Go-java开发" class="headerlink" title="chaincode(可以用Go,java开发)"></a>chaincode(可以用Go,java开发)</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric1.png" alt="Fabric1"></p><h2 id="Ledger"><a href="#Ledger" class="headerlink" title="Ledger"></a>Ledger</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric2.png" alt="Fabric2"><br>整个区块结构分为文件系统存储的<strong>Block结构</strong>和数据库维护的<strong>State状态</strong>，其中state的存储结构是可以替换的，可选的实现包括各种KV数据库（LevelDB，CouchDB等）</p><h2 id="node有三种："><a href="#node有三种：" class="headerlink" title="node有三种："></a>node有三种：</h2><ul><li>client或submitting-client：负责向背书人（endorsers）提交实际的事务调用，并将交易提议广播到订阅服务</li><li>peer：负责提交交易，维持state，有的peer还担任endorser（审查交易合法性，合法则签名）</li><li>Ordering-service-node or orderer: 负责通信服务</li></ul><h2 id="交易的基本工作流程"><a href="#交易的基本工作流程" class="headerlink" title="交易的基本工作流程"></a>交易的基本工作流程</h2><ol><li>client创建一个transaction，并根据endorse policy发送给某些endorser peers</li><li>peers模拟transaction，并产生认可签名</li><li>client收集到足够的peers对transaction的认可，并交给ordering service，orderer会汇总各client递交过来的transaction交易，排序、打包</li><li>orderer将交易打包成区块block，然后通知所有commit peer，各peer各自验证结果，最后将区块block记录到自己的ledger账本中</li></ol><p>如以下例子所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric8.png" alt="Fabric8"></p><ol><li>客户端向其中1个Peer提交Transaction。注意，这里的Client是真正的客户端。而这里的submiting peer，你可以认为是fabric网络的客户端（对客户端来说，这个submiting peer是服务器）</li><li><p>submiting peer把请求给多个endoser<strong>模拟执行</strong>，然后从每个endoser收集返回结果。也就是图中的步骤 圆圈2,3。在Fabric里面，把这个“模拟执行”的过程，称为“背书”，endoser。 至所以这么叫，是因为如果模拟执行不通过的话，这个Transaction就直接被拒绝了。模拟执行通过，才有机会进入下面的过程，才有机会被区块链网络接受。 所以，“背书”，通俗点讲，就是“告诉整个区块链网络，这个Transaction被我模拟执行过了，有效的，你们要不也试试看？”<br>关键点：<br>发给几个endoser取决于你的endose poclicy怎么配置的，你配置成只有1个endoser也可以。 注意这里只是“模拟执行”。endoser并不会把结果直接写到worldState里面。</p></li><li><p>submitting peer 把这个Transaction + 模拟执行的结果发给Ordering Service。也就是图中的步骤 圆圈 4。<br>关键点：因为有很多个Client往不同的submit peer发送请求，所以这个Ordering Service会收到多笔的Transaction。Ordering Service会对这些Transaction进行打包，形成Block。</p></li><li>Ordering Service再把这个Block广播给每个Peer(此时，Peer充当了另外1个角色，不是endorser，而是Committer)。所有Committer接受到这个Block，真正写入：把交易加入区块链，同时更新WorldState，也就是分布式账本(Ledger)</li><li>给Client发送Event，通知其交易已被执行。</li></ol><h2 id="构建企业级区块链的要素"><a href="#构建企业级区块链的要素" class="headerlink" title="构建企业级区块链的要素"></a>构建企业级区块链的要素</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric4.png" alt="Fabric4"></p><h2 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric5.png" alt="Fabric5"><br>竖着来看，分为3大块： </p><ol><li>最左边的，Memship，就是联盟链特有的身份认证相关的内容。比如组织注册，公钥证书发放，交易签名，验证等等。 </li><li>中间1块：区块链的基础东西，和比特币、以太坊类似，BlockChain、Transaction、Ledger(分布式账本）、P2P协议。</li><li>最右边1块：智能合约。在以太坊里面，称为Smart Contract，这里换了个名字，叫做ChainCode而已。</li></ol><p>0.6运行时架构与1.0运行时架构：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric6.png" alt="Fabric6"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric7.png" alt="Fabric7"><br>下面这张图，展示了Fabric运行时的架构是什么样子。Fabric采用Go语言开发，通常运行在Docker容器里面，每1个Node对应1个Docker容器。</p><ol><li>这里的application/SDK，并不是指我们通常的APP的客户端。而是整个Fabric网络的调用端，通常是我们的Web应用服务器。 </li><li>membership服务，就是上图中的注册登记、身份认证。</li><li>peer就是区块链网络中说的物理上的node。peer有2个角色（对应2个模块），1个叫Endorser，1个叫Committer。peer上面存的是Ledger(区块链 + WorldState)，存的智能合约ChainCode，然后peer之间、peer和客户端之间，用event通信。 </li><li>Order-Service 排序服务。</li></ol><p>Fabric相对于<strong>以太坊</strong>，大部分其实很类似（比如peer，peer上面的账本、worldState）。多了2个东西出来，1个是membership（用于注册、身份认证），1个是order service，用于共识算法。</p><p>0.6版本和1.0版本运行时架构对比：</p><ul><li>分拆Peer的功能，将Blockchain的数据维护和共识服务进行分离，共识服务从Peer节点中完全分离出来，独立为Orderer节点提供共识服务；</li><li>基于新的架构，实现多通道（channel）的结构，实现了更为灵活的业务适应性（业务隔离、安全性等方面）</li><li>支持更强的配置功能和策略管理功能，进一步增强系统的灵活性和适应性；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;调研了一下HyperLedger Fabric的架构，因为它的官网文档并没有及时更新，所以有的内容参考的还是以前版本的文档。&lt;br&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>2018.08.22收获与总结</title>
    <link href="http://yoursite.com/2018/08/22/2018-08-22%E6%94%B6%E8%8E%B7%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/22/2018-08-22收获与总结/</id>
    <published>2018-08-22T13:52:05.000Z</published>
    <updated>2018-08-22T14:06:55.046Z</updated>
    
    <content type="html"><![CDATA[<p>今天处理了一下THUC的新闻数据集，具体代码在下面。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>sklearn里的shuffle可以将数据打乱，我在之前处理南开数据集的时候忽视了这一点。同样，pandas中的sample()也是同样的作用，numpy库中的方法不推荐，会导致内存溢出。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>离开实验室的时候跑了TextCNN的模型，结果到家看了一下结果，从第九个epoch开始准确率都为1（一共15个epochs）。一开始我纳闷为啥准确率这么高，因为在复旦数据集上也就0.86左右的准确率。后来看了一下下面的代码。最后两行是我生成训练集和测试集的方法，仔细看知道了测试集就是训练集的一个子集！怪不得准确率这么高，因为已经告诉你label了啊！明天重新生成一下训练集和测试集。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_to_mini</span><span class="params">(dirname, targetname, per_class_max_docs=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理清华大学语料库，将类别和文档处理成1个txt</span></span><br><span class="line"><span class="string">    dirname:原始路径</span></span><br><span class="line"><span class="string">    targetname:保存路径</span></span><br><span class="line"><span class="string">    per_class_max_docs:每类文档保留的文档数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># f_train = io.open(targetname, 'w', encoding='utf-8')</span></span><br><span class="line">    labels = []</span><br><span class="line">    contents = []</span><br><span class="line">    ids = []</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(dirname):</span><br><span class="line">        print(<span class="string">'path error'</span>)</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> os.listdir(dirname):  <span class="comment"># 分类目录</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        cat_dir = os.path.join(dirname, category)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(cat_dir):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        files = os.listdir(cat_dir)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> cur_file <span class="keyword">in</span> files: <span class="comment">#具体文件</span></span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count &gt; per_class_max_docs:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            filename = os.path.join(cat_dir, cur_file)</span><br><span class="line">            <span class="keyword">with</span> io.open(filename, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                content = f.read().replace(<span class="string">'\n'</span>, <span class="string">''</span>).replace(<span class="string">'\t'</span>, <span class="string">''</span>).replace(<span class="string">'\u3000'</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="comment"># content = is_ustr(content)</span></span><br><span class="line">                <span class="comment"># line_content = category + '\t' + content + '\t' _ str(count) + '\n'</span></span><br><span class="line">                <span class="comment"># f_train.write(category + '\t' + content + '\t' + str(count) + '\n')</span></span><br><span class="line">            labels.append(label_dict[category])</span><br><span class="line">            contents.append(content.replace(<span class="string">'\n'</span>, <span class="string">''</span>))</span><br><span class="line">            ids.append(str(count))</span><br><span class="line">        print(<span class="string">'Finished:'</span>, category)</span><br><span class="line">    <span class="comment"># train_len=int(len(ids)*0.8)</span></span><br><span class="line">    <span class="comment"># train_df = pd.DataFrame(&#123;'label': labels[:train_len], 'content': contents[:train_len], 'id': ids[:train_len]&#125;)</span></span><br><span class="line">        val_df = pd.DataFrame(&#123;<span class="string">'label'</span>: labels[:], <span class="string">'content'</span>: contents[:], <span class="string">'id'</span>: ids[:]&#125;)</span><br><span class="line">    <span class="comment"># train_df.to_csv("/home/zkq/data/small_train.csv", index=False, sep='\t')</span></span><br><span class="line">        val_df = shuffle(val_df) <span class="comment">#打乱每行顺序</span></span><br><span class="line">        val_df.to_csv(targetname , index=<span class="keyword">False</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">'Finished Tran'</span>)</span><br><span class="line">    <span class="comment"># f_train.close()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    label_dict = &#123;<span class="string">'星座'</span>: <span class="string">'0'</span>, <span class="string">'股票'</span>: <span class="string">'1'</span>, <span class="string">'房产'</span>: <span class="string">'2'</span>, <span class="string">'时尚'</span>: <span class="string">'3'</span>, <span class="string">'体育'</span>: <span class="string">'4'</span> , <span class="string">'社会'</span>:<span class="string">'5'</span>,<span class="string">'家居'</span>:<span class="string">'6'</span>,<span class="string">'游戏'</span>:<span class="string">'7'</span>,<span class="string">'彩票'</span>:<span class="string">'8'</span>,<span class="string">'科技'</span>:<span class="string">'9'</span>,<span class="string">'教育'</span>:<span class="string">'10'</span>,<span class="string">'时政'</span>:<span class="string">'11'</span>,<span class="string">'娱乐'</span>:<span class="string">'12'</span>,<span class="string">'财经'</span>:<span class="string">'13'</span>&#125;</span><br><span class="line">    <span class="comment"># category = ['星座', '股票', '房产', '时尚', '体育', '社会', '家居', '游戏', '彩票', '科技', '教育', '时政', '娱乐', '财经']</span></span><br><span class="line">    <span class="comment">#合并为一个文件</span></span><br><span class="line">    <span class="comment"># corpus = load_data_to_mini('/home/zkq/data/THUCNews', '/home/zkq/data/thuc_train.csv', 1000)</span></span><br><span class="line">    corpus = load_data_to_mini(<span class="string">'/home/zkq/data/THUCNews'</span>, <span class="string">'/home/zkq/data/thuc_val.csv'</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天处理了一下THUC的新闻数据集，具体代码在下面。&lt;/p&gt;
&lt;h2 id=&quot;收获&quot;&gt;&lt;a href=&quot;#收获&quot; class=&quot;headerlink&quot; title=&quot;收获&quot;&gt;&lt;/a&gt;收获&lt;/h2&gt;&lt;p&gt;sklearn里的shuffle可以将数据打乱，我在之前处理南开数据集的时候忽视了这一点。同样，pandas中的sample()也是同样的作用，numpy库中的方法不推荐，会导致内存溢出。&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h2&gt;&lt;p&gt;离开实验室的时候跑了TextCNN的模型，结果到家看了一下结果，从第九个epoch开始准确率都为1（一共15个epochs）。一开始我纳闷为啥准确率这么高，因为在复旦数据集上也就0.86左右的准确率。后来看了一下下面的代码。最后两行是我生成训练集和测试集的方法，仔细看知道了测试集就是训练集的一个子集！怪不得准确率这么高，因为已经告诉你label了啊！明天重新生成一下训练集和测试集。&lt;br&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>scipy.stats.spearmanr用法</title>
    <link href="http://yoursite.com/2018/08/13/scipy-stats-spearmanr%E7%94%A8%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/13/scipy-stats-spearmanr用法/</id>
    <published>2018-08-13T08:26:00.000Z</published>
    <updated>2018-08-13T08:28:06.861Z</updated>
    
    <content type="html"><![CDATA[<p>计算<strong>Spearman秩相关系数</strong>和<strong>P值</strong>（非相关性检验）。<br>在计算word similarity时用到的。具体用法是数据集中每行有一对词和人工标注的相关性，如(李白 诗 9.2)。程序先从词向量中读取两个词的向量，求得两个向量的余弦相似性，再用spearmanr求得相关系数和P值。最后的实验分析用的是相关系数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;计算&lt;strong&gt;Spearman秩相关系数&lt;/strong&gt;和&lt;strong&gt;P值&lt;/strong&gt;（非相关性检验）。&lt;br&gt;在计算word similarity时用到的。具体用法是数据集中每行有一对词和人工标注的相关性，如(李白 诗 9.2)。程序先从词向量中读取两个
      
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>keras中的embedding层</title>
    <link href="http://yoursite.com/2018/08/08/keras%E4%B8%AD%E7%9A%84embedding%E5%B1%82/"/>
    <id>http://yoursite.com/2018/08/08/keras中的embedding层/</id>
    <published>2018-08-08T01:23:20.000Z</published>
    <updated>2018-08-08T02:04:32.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="keras中的Embedding层"><a href="#keras中的Embedding层" class="headerlink" title="keras中的Embedding层"></a>keras中的Embedding层</h2><p>将索引映射为固定维度的稠密向量，如[[4],[20]]-&gt;[[0.25,0.1],[0.6,-0.2]]。<br>Embedding层只能作为模型的第一层。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><ol><li>从头训练<br>就像word2vec一样, 这一层是可学习的, 用随机数initialize , 通过BP去调整.</li><li>pre-trained + fine tuning<br>用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 然后继续学习.</li><li>pre-trained + static<br>用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 并且这些参数保持固定, 不参与网络的学习.<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer=<span class="string">'uniform'</span>, embeddings_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, embeddings_constraint=<span class="keyword">None</span>, mask_zero=<span class="keyword">False</span>, input_length=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></li></ol><p>input_dim：大或等于0的整数，字典长度（词汇量），即输入数据最大下标+1<br>output_dim：大于0的整数，代表词向量的维度<br>embeddings_initializer: 初始化方法<br>embeddings_regularizer: 嵌入矩阵的正则项，为Regularizer对象<br>embeddings_constraint: 嵌入矩阵的约束项，为Constraints对象<br>mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为$|vocabulary| + 1$。<br>input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;keras中的Embedding层&quot;&gt;&lt;a href=&quot;#keras中的Embedding层&quot; class=&quot;headerlink&quot; title=&quot;keras中的Embedding层&quot;&gt;&lt;/a&gt;keras中的Embedding层&lt;/h2&gt;&lt;p&gt;将索引映射为固定维度
      
    
    </summary>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>文本分类综述</title>
    <link href="http://yoursite.com/2018/08/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/08/03/文本分类综述/</id>
    <published>2018-08-03T03:58:38.608Z</published>
    <updated>2018-08-03T03:58:38.608Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了神经网络在文本分类方面的一些应用，其中多篇论文被广泛引用。如果有最新的相关研究会及时更新。<br>文章将按照这些论文提出年份展开介绍，发展历史如下图所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B015.jpg" alt="此处输入图片的描述"><br><a id="more"></a></p><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>文章提出的方法类似于word2vec的cbow模型，并在此基础上加上bag of n-grams（考虑单词的顺序关系），下图是FastText文本分类的模型，$w$是语句中的词语，词语的向量相加求平均值作为文本表示然后做一个线性分类。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B02.jpg" alt="此处输入图片的描述"><br>但与cbow不同的是，fasttext不是预测中心词，而是直接预测标签。模型以语句中的词语作为输入，输出语句属于各类别上的概率。<br>    模型比较简单，训练速度很快，但是准确率不高。</p><h2 id="TextCNN2014"><a href="#TextCNN2014" class="headerlink" title="TextCNN2014"></a>TextCNN2014</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B03.jpg" alt="此处输入图片的描述"><br>输入层将一个句子所有单词的词向量拼接成一个矩阵，每一行代表一个词。图中有两个channels，原文中用的是static和non-static，即使用的词向量是否随着训练发生变化；也可以使用不同的词向量方法生成的词向量作为不同channel。对于未登录词的向量，使用0或随机正数来填充。<br>    卷积层的每个卷积核的大小为$filter<em>-size\times embedding</em>-size$。$filter<em>-size$代表卷积核纵向上包含的单词个数，即认为相邻几个词之间有词序关系，代码里使用的是[3，4，5]。$embedding</em>-size$就是词向量的维度。每个卷积核计算完成之后我们就得到了1个列向量，代表着该卷积核从句子中提取出来的特征。<br>池化层使用Max-over-time Pooling的方法。这种方法就是简单地从之前的Feature Map中提出最大的值（文中解释最大值代表着最重要的信号）。可以看出，这种Pooling方式可以解决可变长度的句子输入问题（因为不管Feature Map中有多少个值，只需要提取其中的最大值）。最终池化层的输出为各个Feature Map的最大值，即一个一维的向量。<br>全连接层，为了将pooling层输出的向量转化为我们想要的预测结果，加上一个softmax层。文中还提到了过拟合的问题，在倒数第二层的全连接部分上使用<strong>Dropout</strong>技术，即对全连接层上的权值参数给予L2正则化的限制。这样做的好处是防止隐藏层单元自适应（或者对称），从而减轻过拟合的程度。<br><strong>本文使用的词向量是CBOW在Google News上的训练结果。</strong></p><h2 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h2><p>分析了Recursive Neural Network、RNN、CNN做文本分类的优缺点：<br>Recursive Neural Network效果完全依赖于文本树的构建，并且构建文本树所需的时间是$ O\left( n^2 \right)  $  。并且两个句子的关系也不能通过一颗树表现出来。因此不适合于长句子或者文本。<br>    RNN是有偏的模型，后面的词比前面的词更重要。<br>    CNN卷积核的尺寸难以设置。如果选小了容易造成信息的丢失；如果选大了，会造成巨大的参数空间。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B04.jpg" alt="此处输入图片的描述"><br>卷积层是一个BiRNN（双向LSTM），通过正向和反向循环来构建一个单词的上文和下文，如下式：</p><script type="math/tex; mode=display">c_l\left( w_i \right) =f\left( W^{\left( l \right)}c_l\left( w_{i-1} \right) +W^{\left( sl \right)}e\left( w_{i-1} \right) \right)</script><script type="math/tex; mode=display">c_r\left( w_i \right) =f\left( W^{\left( r \right)}c_r\left( w_{i+1} \right) +W^{\left( sr \right)}e\left( w_{i+1} \right) \right)</script><p>得到上下文表示后，拼接表示当前词：</p><script type="math/tex; mode=display">x_i=\left[ c_l\left( w_i \right) ;e\left( w_i \right) ;c_r\left( w_i \right) \right]</script><p>使用$tanh$函数激活得到：</p><script type="math/tex; mode=display">y_{i}^{\left( 2 \right)}=\tanh \left( W^{\left( 2 \right)}x_i+b^{\left( 2 \right)} \right)</script><p>池化层使用最大池化，使用所有单词在每个维度上的最大值表示文本的信息。最后输出层使用softmax得到分类结果。<br>    <strong>本文使用的词向量是使用Skip-gram训练的中英文Wikipedia。</strong></p><h2 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h2><p>传统的RNN（LSTM）结构：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B05.jpg" alt="此处输入图片的描述"><br>下面的公式为LSTM中各门的公式：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B012.jpg" alt="此处输入图片的描述"><br>文章《Recurrent Neural Network for Text Classification with Multi-Task Learning》中介绍了RNN用于文本分类的模型设计，主要提出了以下三种模型：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B06.jpg" alt="此处输入图片的描述"><br>模型（a）中多任务共享LSTM结构以及公有的词向量；针对于任务$m$，输入$ \hat{x}_t $ 为：</p><script type="math/tex; mode=display">\hat{x}_{t}^{\left( m \right)}=x_{t}^{\left( m \right)}\oplus x_{t}^{\left( s \right)}</script><p>其中，$ x<em>{t}^{\left( m \right)} $ ，$ x</em>{t}^{\left( s \right)} $  分别表示任务私有的词向量和公有的词向量。最后一个时刻的hidden state则作为输入传入softmax。<br>模型（b）每个任务具有自己独立的LSTM层，但是每一时刻所有任务的hidden state则会和下一时刻的character一起作为输入，最后一个时刻的hidden state进行分类。作者修改了候选状态的计算公式：</p><script type="math/tex; mode=display">\tilde{c}_{t}^{\left( m \right)}=\tanh \left( W_{t}^{\left( m \right)}x_t+\sum_{i\in \left\{ m,n \right\}}{g^{\left( i\rightarrow m \right)}U_{c}^{\left( i\rightarrow m \right)}h_{t-1}^{\left( i \right)}} \right)</script><script type="math/tex; mode=display">g^{\left( i\rightarrow m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( i \right)}h_{t-1}^{\left( i \right)} \right)</script><p>模型（c）除了一个共享的BI-LSTM层用于获取共享信息，每个任务有自己独立的LSTM层，LSTM的输入包括每一时刻的character和BI-LSTM的hidden state，同模型2一样，作者修改了候选状态的计算公式：</p><script type="math/tex; mode=display">\tilde{c}_{t}^{\left( m \right)}=\tanh \left( W_{t}^{\left( m \right)}x_t+g^{\left( m \right)}U_{c}^{\left( m \right)}h_{t-1}^{\left( m \right)}+g^{\left( s\rightarrow m \right)}U_{c}^{\left( s \right)}h_{t}^{\left( s \right)} \right)</script><script type="math/tex; mode=display">g^{\left( m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( m \right)}h_{t-1}^{\left( s \right)} \right)</script><script type="math/tex; mode=display">g^{\left( s\rightarrow m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( s\rightarrow m \right)}h_{t}^{\left( s \right)} \right)</script><p><strong>本文使用的词向量是使用Word2Vec训练的Wikipedia语料。</strong></p><h2 id="CharCNN"><a href="#CharCNN" class="headerlink" title="CharCNN"></a>CharCNN</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B07.jpg" alt="此处输入图片的描述"><br>首先对文本编码，使用的字母表共有69个字符，对其使用one-hot编码，外加一个全零向量（用于处理不在该字符表中的字符），所以共70个。经过6个卷积层和3个全连接层得到输出。<br>优点是是不需要使用预训练好的词向量和语法句法结构等信息，并且可以很容易的推广到所有语言。针对于汉语，作者使用拼音代替汉字实现的编码。<br>对于几百上千等小规模数据集，可以优先考虑传统方法，对于百万规模的数据集，CharCNN表现不错。CharCNN适用于用户生成数据(user-generated data)（如拼写错误，表情符号等）。</p><h2 id="GRNN（Conv、LSTM）"><a href="#GRNN（Conv、LSTM）" class="headerlink" title="GRNN（Conv、LSTM）"></a>GRNN（Conv、LSTM）</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B08.jpg" alt="此处输入图片的描述"><br>首先使用CNN/LSTM来建模句子表示，接下来使用双向GRU模型对句子表示进行编码得到文档表示，得到的文档表示用于Softmax情感分类。<br><strong>在上图中，底层的词向量是由word2vec预训练得到。</strong>使用CNN/LSTM学习得到句子的表示，这里会把变长的句子表示表示成相同维度的向量。卷积模型如下：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B09.jpg" alt="此处输入图片的描述"><br>filter的宽度分别取1，2，3来编码unigrams，bigrams和trigrams的语义信息。最后使用一个Average层捕获全局信息并转化为输出向量。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B010.jpg" alt="此处输入图片的描述"><br>使用GRU模型，输入是变长的句子向量，输出固定长度的文本向量，这里会对最后每个单元的输出向量进行取平均操作</p><h2 id="HAN（RNN-Attention）"><a href="#HAN（RNN-Attention）" class="headerlink" title="HAN（RNN+Attention）"></a>HAN（RNN+Attention）</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B011.jpg" alt="此处输入图片的描述"><br>    一个句子中每个单词的重要性不同。一篇文档中每个句子的重要性也不同。因此本文主要思想是，首先考虑文档的分层结构：单词构成句子，句子构成文档，所以建模时也分这两部分进行。其次，不同的单词和句子具有不同的信息量，不能单纯的统一对待所以引入Attention机制。而且引入Attention机制除了提高模型的精确度之外还可以进行单词、句子重要性的分析和可视化，让我们对文本分类的内部有一定了解。模型主要可以分为四个部分，如上图所示：<br>    Word encoder和sentence encoder都是双向GRU，公式如下：<br>Word encoder中公式如下：</p><script type="math/tex; mode=display">x_{it}=W_ew_{it},t\in \left[ \text{1,}T \right]  \\ \vec{h}_{it}=\overrightarrow{GRU}\left( x_{it} \right) ,t\in \left[ \text{1,}T \right]  \\ \vec{h}_{it}=\overleftrightarrow{GRU}\left( x_{it} \right) ,t\in \left[ T,1 \right]</script><p>Word attention中公式如下：</p><script type="math/tex; mode=display">u_{it}=\tanh \left( W_wh_{it}+b_w \right)  \\ \alpha _{it}=\frac{\exp \left( u_{it}^{\top}u_w \right)}{\sum_t^{}{\exp \left( u_{it}^{\top}u_w \right)}} \\ s_i=\sum_t{\alpha _{it}h_{it}}</script><p>Sentence encoder中公式如下：</p><script type="math/tex; mode=display">\vec{h}_i=\overrightarrow{GRU}\left( s_i \right) ,i\in \left[ \text{1,}L \right]  \\ \vec{h}_i=\overleftrightarrow{GRU}\left( s_i \right) ,i\in \left[ L,1 \right]</script><p>Sentence attention中公式如下：</p><script type="math/tex; mode=display">u_i=\tanh \left( W_sh_i+b_s \right)  \\ \alpha _i=\frac{\exp \left( u_{i}^{\top}u_s \right)}{\sum_i^{}{\exp \left( u_{i}^{\top}u_s \right)}} \\ v=\sum_i{\alpha _ih_i}</script><p>每个词语对应的hidden vector的输出经过变换（转置和$tanh$）之后和$u_w$ 相互作用（点积），结果就是每个词语的权重。加权以后就可以产生整个sentence的表示。从高一级的层面来看(hierarchical的由来)，每个document有$L$个句子组成，那么这$L$个句子就可以连接成另一个sequence model, 同样是双向GRU，同样的对输出层进行变换后和 相互作用，产生每个句子的权重，加权以后我们就产生了对整个document的表示。最后用softmax就可以产生对分类的预测。</p><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B013.jpg" alt="此处输入图片的描述"><br>表征学习是自然语言处理中的一个基本问题。文章《Learning Structured Representation for Text Classification via Reinforcement Learning》研究如何学习文本分类的结构化表示。与大多数既不使用结构也不依赖于预定义结构的现有表示模型不同，作者提出了一种强化学习（RL）方法，通过自动地优化结构来学习句子表示。<br>作者在文章中提出两种结构表示模型（模型第二部分）：Information Distilled LSTM (ID-LSTM) 和 Hierarchically Structured LSTM (HS-LSTM)。其中 ID-LSTM 只选择重要的任务相关的单词，HS-LSTM 则去发现句子中的短语结构。两种表示模型中的结构发现被表述为一个顺序决策问题，结构发现的当前决策影响随后的决策，这可以通过策略梯度 RL 来解决。<br>结果表明，这种方法可以通过识别重要的词或任务相关的结构而无需明确的结构注释来学习任务友好的表示，从而获得有竞争力的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了神经网络在文本分类方面的一些应用，其中多篇论文被广泛引用。如果有最新的相关研究会及时更新。&lt;br&gt;文章将按照这些论文提出年份展开介绍，发展历史如下图所示：&lt;br&gt;&lt;img src=&quot;http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B015.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="文本分类" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>词向量综述</title>
    <link href="http://yoursite.com/2018/07/23/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/07/23/词向量综述/</id>
    <published>2018-07-23T08:14:07.000Z</published>
    <updated>2018-08-03T03:45:28.773Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了词向量的多种表示方法，主要为word2vec基础上的多种模型，侧重于中文词向量模型。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%91%E5%B1%95%E5%9B%BE.jpg" alt="此处输入图片的描述"><br>2018年7月23日更新，在NLP课上宗成庆老师在讲词向量时提到，除了基于文本的词汇语义表示模型，还有基于图像、语音、多模态信息学习词汇语义表示等，这一些有空再补充。<br><a id="more"></a></p><h2 id="什么是词向量"><a href="#什么是词向量" class="headerlink" title="什么是词向量"></a>什么是词向量</h2><p>自然语言是符合语法、具有一定的信息、为人类所能理解并进行沟通的语言。但当我们使用计算机来处理自然语言时，计算机是无法直接理解这些符号的（汉字、字母、标点等）。这些符号需要经过数值化后才能输入计算机进行后续的处理。然而只用单个数字来表示单词是毫无意义的，它只是一个id，无法体现单词的属性，因此需要将单词进行向量化。</p><h2 id="词向量的表示"><a href="#词向量的表示" class="headerlink" title="词向量的表示"></a>词向量的表示</h2><p>词向量的表示主要有两种方式：独热编码（One-hot Representation）和分布式表示（Distributed Representation）。</p><h3 id="One-hot表示"><a href="#One-hot表示" class="headerlink" title="One-hot表示"></a>One-hot表示</h3><p>独热编码是最直观、也最常用的方法。是一种稀疏的表示方式。其思路为将每个词都表示成一个很长的向量，该向量的维度等于词表大小，其中只有一个维度的值为1（维度的位置等于词的id），其余维度都为0。举个例子：<br>假设我们从语料库中为每个词分配一个数字id（从0开始分配），得到“话筒”这个词的id为3，“麦克”为8，那么用独热编码为：</p><blockquote><p>话筒：[0 0 0 1 0 0 0 0 0 0 …]<br>麦克: [0 0 0 0 0 0 0 0 1 0 …]</p></blockquote><p>每个词都是茫茫0海中的一个1。如果要编程实现的话，用 Hash 表给每个词分配一个id就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。<br>如此简洁的方法自然有它的缺点：<br>1、向量维度会随着词表增大而增大：</p><ul><li>存储效率低</li><li>若词表扩容，则每个词维度也必须相应增加</li><li>若某个词出现次数很少的话，则相应的权重会容易被错误估计</li></ul><p>2、“词汇鸿沟”问题：每个维度彼此正交，即所有词彼此孤立，无法表示词与词之间的相关信息，例如余弦相似度。由于任意一对向量 的余弦相似度为</p><script type="math/tex; mode=display">\frac{x^{\top}y}{\lVert x \rVert \lVert y \rVert}\in \left[ -\text{1,}1 \right]</script><p>任何一对词的one-hot向量的余弦相似度都为0。</p><h3 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h3><p>1954 年，Harris 提出<strong>分布假说（distributional hypothesis）</strong>，即“上下文相似的词，其语义也相似”，为词的分布表示提供了理论基础。Firth 在1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（a word is characterized by the company it keeps）。<br>基于分布假说，研究人员提出了多种词表示模型：如基于矩阵的LSA 模型、基于聚类的Brown clustering 模型以及最近使用广泛的神经网络词表示模型。神经网络模型生成的词表示通常被称为<strong>词向量（word embedding）</strong>，是一个低维的实数向量表示，通过这种表示，可以直接对词之间的相似度进行刻画。</p><h4 id="基于矩阵的分布式表示"><a href="#基于矩阵的分布式表示" class="headerlink" title="基于矩阵的分布式表示"></a>基于矩阵的分布式表示</h4><p>利用语料库构建一个$ W\times C $ 共现矩阵$F$，矩阵每一行代表一个词，每一列是某种上下文表示方法。$W$是词表大小。矩阵每个单元的值可以是二值（表示二者是否共现），可以是未经处理的共现次数，也可以是经过处理后的共现tf-idf值，等等。很多可衡量两个对象之间关联的指标都可以用来作为矩阵中每个单元的值。<br>由于矩阵每一行的维度大小都等于词表大小，不便计算，所以需要进行降维。降维技术可以减少噪声带来的影响，但也可能损失一部分信息。最常用的分解技术包括奇异值分解（SVD）、非负矩阵分解（NMF）、典型关联分析（Canonical Correlation Analysis，CCA）、Hellinger PCA（HPCA）。<br>基于矩阵的分布表示在这些步骤的基础上，衍生出了若干不同方法，如经典的LSA就是使用tf-idf 作为矩阵元素的值，并使用SVD分解，得到词的低维向量表示。在这类方法中，最新的为GloVe 模型，下文简单介绍这一模型。</p><h5 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h5><p>总体上看，GloVe 模型是一种对“词-词”矩阵进行分解从而得到词表示的方法。矩阵第$i$行第$j$列的值为词$v<em>i$与词$v_j$在语料中的共现次数$ x</em>{ij} $的对数。在矩阵分解步骤，GloVe 模型借鉴了推荐系统中基于隐因子分解（Latent Factor Model）的方法，在计算重构误差时，只考虑共现次数非零的矩阵元素，同时对矩阵中的行和列加入了偏移项。具体为最小化下式：</p><script type="math/tex; mode=display">\sum_{ik}{f\left( x_{ik} \right) \left( w_{i}^{T}w_k+b_i+b_k-\log x_{ik} \right) ^2}</script><p>其中$ w_i $为词$ v_i $ 作为目标词时的词向量，$ w_j $ 为词$ v_j $ 作为上下文时的词向量，$ b_i $ 、$ b_k $ 为针对词表中各词的偏移向量， $ f\left( x \right)  $ 是一个加权函数，对低频的共现词对进行衰减，减少低频噪声带来的误差，定义为：</p><script type="math/tex; mode=display">f\left( x \right) =\begin{cases}     \left( \frac{x}{x_{\max}} \right) ^{\alpha}\ \ \text{如果}x<x_{\max}\\     \text{1                 其他情况}\\ \end{cases}</script><h4 id="基于聚类的分布式表示"><a href="#基于聚类的分布式表示" class="headerlink" title="基于聚类的分布式表示"></a>基于聚类的分布式表示</h4><p>基于聚类的分布式表示也被称为分布聚类，通过聚类的方法构建词与其上下文之间的关系。最经典的方法是布朗聚类（Brown clustering）。布朗聚类是一种层级聚类方法，聚类结果为每个词的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。具体而言，布朗聚类需要最大化以下似然，其中$ c_i $ 为词$ w_i $ 对应的类别：</p><script type="math/tex; mode=display">P\left( w_i|w_{i-1} \right) =P\left( w_i|c_i \right) P\left( c_i|c_{i-1} \right)</script><p>布朗聚类只考虑了相邻词之间的关系，也就是说，每个词只使用它的上一个词，作为上下文信息。</p><h4 id="基于神经网络的分布式表示"><a href="#基于神经网络的分布式表示" class="headerlink" title="基于神经网络的分布式表示"></a>基于神经网络的分布式表示</h4><p>神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。同时，神经网络语言模型对当前词向量的发展方向有着决定性的作用。</p><h5 id="NNLM2001"><a href="#NNLM2001" class="headerlink" title="NNLM2001"></a>NNLM2001</h5><p>2001年，Bengio 等人正式提出神经网络语言模型（Neural Network Language Model，NNLM）。该模型在学习语言模型的同时，也得到了词向量。NNLM对$n$元语言模型进行建模，估算$ P\left( w<em>i|w</em>{i-\left( n-1 \right)},…,w<em>{i-1} \right)  $  的值。也就是对语料中一段长度为$n$ 的序列 $ w</em>{i-\left( n-1 \right)},…,w_{i-1},w_i $ , 元语言模型需要最大化以下似然：</p><script type="math/tex; mode=display">P\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right)</script><p>其中， $ w<em>i $ 为需要通过语言模型预测的词（目标词）。对于整个模型而言，输入为条件部分的整个词序列： $ w</em>{i-\left( n-1 \right)},…,w<em>{i-1} $ ，输出为目标词的分布。<br><strong>神经网络语言模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B01.jpg" alt="神经网络语言模型结构图"><br>输入层将前 $n-1$个词 $ w</em>{i-\left( n-1 \right)},…,w_{i-1} $ 通过查表映射成对应的词向量，然后将这 $n-1$个向量顺序拼接，形成$x$ ：</p><script type="math/tex; mode=display">x=\left[ e\left( w_{i-\left( n-1 \right)} \right) ;...;e\left( w_{i-2} \right) ;e\left( w_{i-1} \right) \right]</script><p>输入层完成对 $x$的拼接后，模型将其依次送入隐藏层$h$ 和输出层 $y$。</p><script type="math/tex; mode=display">h=\tanh \left( b^h+Hx \right)</script><script type="math/tex; mode=display">y=b^y+Wx+Uh</script><p>其中 $ H\in \mathbb{R}^{\left| h \right|\times \left( n-1 \right) \left| e \right|} $ 为输入层到隐藏层的权重矩阵， $ U\in \mathbb{R}^{\left| v \right|\times \left| h \right|} $ 为隐藏层到输出层的权重矩阵，$ \left| V \right| $  表示词表大小， $ \left| e \right| $ 表示词向量维度， $ \left| h \right| $ 为隐藏层维度，$ b^h $  、 $ b^y $ 均为偏置。矩阵$W$ 表示从输入层到输出层的直连边权重矩阵。<br>输出层一共有 $ \left| V \right| $ 个元素，对应下一个词为词表中某个词的可能性。使用softmax将输出值归一化成概率。</p><script type="math/tex; mode=display">p\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right) =\frac{\exp \left( y\left( w_i \right) \right)}{\sum_{k=1}^{\left| V \right|}{\exp \left( y\left( v_k \right) \right)}}</script><p>对于整个预料而言，语言模型需要最大化：</p><script type="math/tex; mode=display">\sum_{w_{i-\left( n-1 \right) :i\in \mathbb{D}}}{\log \left( P\left( w_i \right) |w_{i-\left( n-1 \right)},...,w_{i-1} \right)}</script><p>训练时使用随机梯度下降来优化上述目标。每次迭代，随即从语料中选取一段样本作为训练样本，使用下式进行一次梯度迭代：</p><script type="math/tex; mode=display">\theta \gets \theta +\alpha \frac{\partial \log P\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right)}{\partial \theta}</script><p>其中，$ \alpha  $ 是学习率； $ \theta  $ 为模型中所有参数，包括词向量和网络模型中的权重及偏置。<br>值得注意的是，神经网络语言模型中的词向量出现了两次。在输入层，各词的词向量存在于一个$ \left| e \right|\times \left| V \right| $  维的实数矩阵中，每一列对应一个词向量。隐藏层到输出层的权重矩阵$U$ 的维度是$ \left| V \right|\times \left| h \right| $  ，可看作$ \left| V \right| $  个$ \left| h \right| $  维的行向量，其中每个向量都可以看做某个词的另一种表示$ e’ $  。我们将 $ e\left( w \right)  $ 称为词的上下文表示，将 $ e’\left( w \right)  $ 称为词的目标词表示，通常将$e$ 作为词向量。</p><h5 id="循环神经网络语言模型"><a href="#循环神经网络语言模型" class="headerlink" title="循环神经网络语言模型"></a>循环神经网络语言模型</h5><p>Mikolov等人提出的循环神经网络语言模型（Recurrent Neural Network based Language Model，RNNLM）直接对 $ P\left( w<em>i|w_1,w_2,…,w</em>{i-1} \right)  $ 进行建模。因此，RNNLM 可以利用所有的上文信息，预测下一个词，其模型结构下图所示。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B02.jpg" alt="循环神经网络语言模型结构图"><br>RNNLM的核心在于其隐藏层算法：</p><script type="math/tex; mode=display">h\left( i \right) =\phi \left( e\left( w_i \right) +Wh\left( i-1 \right) \right)</script><p>其中，$ \phi  $  为非线性激活函数；$h(i)$ 表示文本中第 $i$个词$w_i$ 所对应的隐藏层，该隐藏层由当前词的词向量 $ e\left( w_i \right)  $ 以及上一个词对应的隐藏层$ h\left( i-1 \right)  $  结合得到。通过迭代，每个隐藏层都包含了此前所有词的信息。RNNLM的输出层计算方法与NNLM的输出层一致。</p><h5 id="C-amp-W模型"><a href="#C-amp-W模型" class="headerlink" title="C&amp;W模型"></a>C&amp;W模型</h5><p>C&amp;W模型结构图:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B03.jpg" alt="此处输入图片的描述"><br>不同于之前的模型，C&amp;W模型直接以生成词向量为目标。他们没有去近似地求$P(w<em>t |w_1,w_2,…,w</em>(t-1))$，而是直接去尝试近似$P(w<em>1,w_2,…,w_t)$。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续 $n$个词的打分 $f(w</em>(t-n+1),…,w_(t-1),w_t)$打分越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。有了这个对 $f$的假设，C&amp;W 就直接使用 pair-wise 的方法训练词向量。具体而言，就是最小化下面的目标函数。</p><script type="math/tex; mode=display">\sum_{x\in \mathfrak{X}}{\sum_{w\in \mathfrak{D}}{\max\text{\{0,}1-f\left( x \right) +f\left( x^{\left( w \right)} \right) \}}}</script><p> $ \mathfrak{X} $ 为训练集中的所有连续的$n$ 元短语， $ \mathfrak{D} $ 是整个字典。第一个求和枚举了训练语料中的所有的$n$ 元短语，作为正样本。第二个对字典的枚举是构建负样本。 $ x^{\left( w \right)} $ 是将短语$x$ 的最中间的那个词，替换成$w$ 。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当作负样本也不影响大局）。同时，由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类效果。再回顾这个式子， $x$是正样本， $ x^{\left( w \right)} $ 是负样本，$ f\left( x \right)  $  是对正样本的打分，$ f\left( x^{\left( w \right)} \right)  $  是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高1分。$f$ 函数的结构和NNLM中的网络结构基本一致。同样是把窗口中的$n$ 个词对应的词向量串成一个长的向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同之处在于C&amp;W的输出层只有一个节点，表示得分，而不像NNLM那样的有$ \left| V \right| $  个节点，这么做可以大大降低计算复杂度。</p><h5 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h5><p>Mikolov 等人在2013年开发了word2vec工具，包含CBOW（Continuous Bag of Words）和Skip-gram 两种模型。</p><h6 id="CBOW模型"><a href="#CBOW模型" class="headerlink" title="CBOW模型"></a>CBOW模型</h6><p><strong>CBOW模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B04.jpg" alt="此处输入图片的描述"><br>简单来讲，CBOW用上下文作为输入去预测目标词。该模型一方面根据C&amp;W 模型的经验，使用一段文本的中间词作为目标词；另一方面，又以NNLM作为蓝本，并在其基础上做了两个简化。一、CBOW 没有隐藏层，去掉隐藏层之后，模型从神经网络结构直接转化为log 线性结构，与Logistic 回归一致。log 线性结构比三层神经网络结构少了一个矩阵运算，大幅度地提升了模型的训练速度。二、CBOW 去除了上下文各词的词序信息，使用上下文各词词向量的平均值（论文中求和，实际工具求平均），代替神经网络语言模型使用的上文各词词向量的拼接。形式化地，CBOW 模型对于一段训练样本$ w_{i-\left( n-1 \right)},…,w_i $  ，输入为：</p><script type="math/tex; mode=display">x=\frac{1}{n-1}\sum_{w_j\in c}{e\left( wj \right)}</script><p>然后根据上下文表示，对目标词进行预测：</p><script type="math/tex; mode=display">P\left( w|c \right) =\frac{\exp \left( e'\left( w \right) ^Tx \right)}{\sum_{w'\in \mathbb{V}}{\exp \left( e'\left( w' \right) ^Tx \right)}}</script><p>上述二式，目标词$w$ 与上下文$c$ 的定义与C&amp;W模型一致。优化目标与神经网络语言模型一致，即最大化：</p><script type="math/tex; mode=display">\sum_{\left( w,c \right) \in \mathbb{D}}{\log P\left( w|c \right)}</script><h6 id="Skip-gram模型"><a href="#Skip-gram模型" class="headerlink" title="Skip-gram模型"></a>Skip-gram模型</h6><p><strong>Skip-gram模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B05.jpg" alt="此处输入图片的描述"><br>与CBOW相反，Skip-gram用目标词作为输入去预测上下文。为了与CBOW统一，可以将其描述为：每次从目标词$w$ 的上下文$c$ 中选择一个词，将其词向量作为模型的输入$x$ ，也就是上下文的表示，这样Skip-gram的任务也变成了通过上下文预测目标词，优化目标为：</p><script type="math/tex; mode=display">\sum_{\left( w,c \right) \in \mathbb{D}}{\sum_{w_j\in c}{\log P\left( w|w_j \right)}}</script><p>其中，</p><script type="math/tex; mode=display">P\left( w|w_j \right) =\frac{\exp \left( e'\left( w \right) ^Te\left( w_j \right) \right)}{\sum_{w'\in \mathbb{V}}{\exp \left( e'\left( w' \right) ^Te\left( w_j \right) \right)}}</script><h5 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h5><p>word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。<br>方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。</p><h5 id="CWE"><a href="#CWE" class="headerlink" title="CWE"></a>CWE</h5><p><strong>CWE模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B06.jpg" alt="此处输入图片的描述"><br>在word2vec的CBOW模型基础上，在词向量生成部分进行了改进，引入了单个汉字的信息（论文主要针对的是中文），提升了词向量生成的质量。具体实现方法是将CBOW中 $x$的取值办法由上下文词向量相加求均值：</p><script type="math/tex; mode=display">x_o=\frac{1}{2k}\sum_{j=i-k,...,i+k}{x}_j</script><p>变成词语中所有单字的向量与词向量相加求均值：</p><script type="math/tex; mode=display">x=\frac{1}{2}\{w+\frac{1}{N_j}\sum_{k=1}^{N_J}{c_k}\text{)}</script><p>其中， $ N_j $ 是单词中的汉字个数， $w$是字向量。<br>另外文章还提出了三种办法以解决同一汉字在不同词语中的不同语义问题，分别为：Position-based Character Embedding、Cluster-based Character Embedding、Nonparametric Cluster-based Character Embeddings。</p><h5 id="JWE2017"><a href="#JWE2017" class="headerlink" title="JWE2017"></a>JWE2017</h5><p><strong>JWE模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B07.jpg" alt="此处输入图片的描述"><br>根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起联合学习词向量。其中$ w<em>i $  是目标词， $ w</em>{i-1} $ 是目标词左边的词， $ w<em>{i+1} $ 是目标词右边的词；$ c</em>{i-1} $  是目标词左边的词中的单字， $ c<em>{i+1} $ 是目标词右边的词中的单字；$ s</em>{i-1} $  是目标词左边的词拆分的字件，$ s_{i+1} $  是目标词右边的词拆分的字件, $ s_i $ 是目标词的字件；<br>模型的目标是最大化以下对数似然：</p><script type="math/tex; mode=display">L\left( w_i \right) =\sum_{k=1}^3{\log P\left( w_i|h_{i_k} \right)}</script><p>其中, $ h<em>{i_1} $ ， $ h</em>{i<em>2} $ ， $ h</em>{i_3} $ 分别为上下文单词、单字、组件的组合。</p><script type="math/tex; mode=display">P\left( w_i|h_{i_k} \right) =\frac{\exp \left( h_{i_k}^{T}\hat{v}_{w_i} \right)}{\sum_{j=1}^N{\exp \left( h_{i_k}^{T}\hat{v}_{w_j} \right)}}</script><script type="math/tex; mode=display">h_{i_1}=\frac{1}{2T}\sum_{-T\leqslant j\leqslant T,j\ne 0}{v_{w_{i+j}}}</script><p>在以上3个公式中， $ v<em>{w_i} $ ,$ v</em>{c<em>i} $  , $ v</em>{s<em>i} $ 分别为上下文、单字、字件的“<strong>输入向量</strong>”， $ \hat{v}</em>{w<em>j} $ 是“<strong>输出向</strong>量”；$ h</em>{i<em>1} $  为上下文“输入向量”的均值，同理， $ h</em>{i<em>2} $ 为上下文单字“输入向量”的均值，$ h</em>{i_3} $  为上下文字件“输入向量”的均值。<br>给定语料$ D $  ,模型的目标是最大化：</p><script type="math/tex; mode=display">L\left( D \right) =\sum_{w_i\in D}{L\left( w_i \right)} $ #####GWE2017**GWE模型结构图（一）**:![此处输入图片的描述][9]通过convolutional auto-encoder（convAE）提取词中字的glyph特征，取均值后与上下文词向量均值及上下文中字向量的均值合并，得到新的上下文表示，预测目标词。词向量表示为： $$ \vec{w}_{i}^{ctxG}=\vec{w}_i+\frac{1}{\left| C\left( i \right) \right|}\sum_{c_j\in C\left( i \right)}{\left( \vec{c}_j+\vec{g}_j \right)}</script><p><strong>GWE模型结构图（二）</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B09.jpg" alt="此处输入图片的描述"><br>这种模型将当前词中字的glyph特征与上下文结合，一起预测当前词。<br>以下两种模型直接从特征中学习词向量，没有使用上下文信息。</p><p><strong>GWE模型结构图（三）</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B010.jpg" alt="此处输入图片的描述"><br>在Skip-gram的基础上，以特征作为输入，两层GRU，两层全连接ELU，预测当前词的上下文。</p><p><strong>GWE模型结构图（四）</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B011.jpg" alt="此处输入图片的描述"><br>类似于GloVe模型，将原模型中词向量部分改写为当前词的特征。</p><h5 id="cw2vec"><a href="#cw2vec" class="headerlink" title="cw2vec"></a>cw2vec</h5><p>cw2vec在Skip-Gram基础之上进行改进，把词语的n-gram笔画特征信息代替词语进行训练，cw2vec模型如下图：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B012.jpg" alt="此处输入图片的描述"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B013.jpg" alt="此处输入图片的描述"><br> 首先将词语分割，获取中文字符。然后获取笔画信息，并合并笔画信息。将笔画信息数字化。最后提取笔画信息的n-gram特征。</p><h2 id="词向量的评价"><a href="#词向量的评价" class="headerlink" title="词向量的评价"></a>词向量的评价</h2><p>根据来斯惟的博士论文《基于神经网络的词和文档语义向量》，词向量的用法可以分为三大类，分别为：一、利用词向量的语言学特性完成任务；二、将词向量作为特征，提高自然语言处理任务的性能；三、将词向量作为神经网络的初始值，提升神经网络模型的优化效果。他在这三大类用法的基础上，选取了八个有代表性的具体任务，作为词向量的评价指标。</p><h3 id="词向量的语言学特征"><a href="#词向量的语言学特征" class="headerlink" title="词向量的语言学特征"></a>词向量的语言学特征</h3><p>各词向量模型均基于分布假说设计而成，因此无论哪种词向量模型，都会符合分布假说所提出的性质：具有相似上下文的词，会拥有相似的语义，并且其词向量的空间距离更接近。本文选取了三个代表性任务。</p><h4 id="语义相关性-ws"><a href="#语义相关性-ws" class="headerlink" title="语义相关性(ws)"></a>语义相关性(ws)</h4><p>衡量语义相关性最经典的是WordSim353 数据集，该数据集包含了353 个词对，其中每一个词对有至少十位标注者对其进行0 到10 之间的打分，分数越高表示标注人员认为这两个词的语义更相关或者更相似。例如，词对“student,professor”的平均打分为6.81，而词对“professor, cucumber”的打分为0.31。评价时，对于每个词对，本文使用所有标注者打分的平均值作为参考得分$X$ ，以词对的两个词向量的余弦距离作为模型得到的相关性得分$Y$ ，并衡量这两组数值之间的皮尔逊相关系数。皮尔逊相关系数衡量了两个变量之间的线性相关性，值在-1到1之间，如果模型得到的打分与人工标注的打分一致，得分就越高。具体而言，$X$ 和$Y$ 之间的皮尔逊相关系数定义为 $X$和 $Y$之间的协方差与它们标准差的商：</p><script type="math/tex; mode=display">\rho _{X,y}=\frac{cov\left( X,Y \right)}{\sigma _X\sigma _Y}</script><h4 id="同义词检测-tfl"><a href="#同义词检测-tfl" class="headerlink" title="同义词检测(tfl)"></a>同义词检测(tfl)</h4><p>托福考试（TOEFL）数据集包含80 个单选题，每个题目包含一个问题词以及四个选项，要求从四个选项中选出一个与问题词同义的词语。例如：问题“levied”，选项“imposed”、“believed”、“requested”、“correlated”，正确答案为“imposed”。对于每一个问题，需要计算问题词与选项词对应词向量之间的余弦距离，并选用距离最近的选项词，作为答案。在评价词向量时，可以直接使用80 个问题的准确率。</p><h4 id="单词类比-sem、syn"><a href="#单词类比-sem、syn" class="headerlink" title="单词类比(sem、syn)"></a>单词类比(sem、syn)</h4><p>英文单词类比数据集由Mikolov 等人于2013 年的word2vec相关论文中提出，该数据集包含了9000 个语义类比问题以及1 万个句法类比问题。语义类比问题包括国家首都、家庭成员称谓、国家货币等五类问题，如，“‘king’对‘queen’如同‘man’对什么？”，答案为“woman”。句法类比问题有比较级、最高级、名词单复数等九类问题，如“‘dance’对‘dancing’如同‘predict’对什么？”，答案为“predicting”。<br>为了回答这类类比问题，Mikolov 等人根据相似关系词对的词向量之差也相似的特点，提出使用词向量的加减法来完成这一任务。例如，对于问题“‘king’对‘queen’如同‘man’对什么？”，该方法直接从词表中寻找与$ \overrightarrow{queen}-\overrightarrow{king}+\overrightarrow{man} $  最相似的词，作为答案。评价时使用回答问题的准确率。<br>单词类比任务的数据集相对前两个任务规模较大，因此在实验中，结果较为稳定，该指标也成为评价词向量的经典指标。</p><h3 id="词向量用作特征"><a href="#词向量用作特征" class="headerlink" title="词向量用作特征"></a>词向量用作特征</h3><p>词向量可以从无标注文本中学习到句法和词法的特征，很多现有工作直接使用词向量作为机器学习系统的特征，并以此提高系统的性能。</p><h4 id="基于平均词向量的文本分类-avg"><a href="#基于平均词向量的文本分类-avg" class="headerlink" title="基于平均词向量的文本分类(avg)"></a>基于平均词向量的文本分类(avg)</h4><p>该任务直接以文本中各词词向量的加权平均值作为文档的表示，以此为特征，利用Logistic 回归完成文本分类任务。其中权重为文档中各词的词频。可以选用IMDB 数据集做文本分类实验。该数据集包含三部分，其中训练集和测试集各2.5 万篇文档，用来做文本分类的训练和测试；无标注部分共5 万篇文档，用于训练词向量。任务的评价指标为文本分类的准确率。</p><h4 id="命名实体识别-ner"><a href="#命名实体识别-ner" class="headerlink" title="命名实体识别(ner)"></a>命名实体识别(ner)</h4><p>命名实体识别（Named entity recognition，NER）在机器学习框架下，通常作为一个序列标注问题处理。在这一评价指标中，将词向量作为现有命名实体识别系统的额外特征，该系统的性能接近现有系统的最好性能。任务的评价指标为命名实体识别的F1值，测试集可以是CoNLL03 多任务数据集的测试集。</p><h3 id="词向量用作神经网络初始值"><a href="#词向量用作神经网络初始值" class="headerlink" title="词向量用作神经网络初始值"></a>词向量用作神经网络初始值</h3><p>在上一类词向量的用法（将词向量作为特征）中，词向量是模型的固定输入值，在模型的训练过程中，输入值不会改变，只有模型中的参数会改变。然而，将神经网络的初始值赋值为词向量之后，神经网络在训练过程中会改变设置的初始值。因此这两类词向量的用法表面上看非常相似，实质上却是不同的。</p><h4 id="基于卷积神经网络的文本分类-cnn"><a href="#基于卷积神经网络的文本分类-cnn" class="headerlink" title="基于卷积神经网络的文本分类(cnn)"></a>基于卷积神经网络的文本分类(cnn)</h4><p>卷积神经网络（Convolutional neural networks，CNN）是表示文本的有效模型。2014 年，Lebret等人以及Kim等人同时提出用于文本分类任务的卷积神经网络。<br>选取斯坦福情感树库（Stanford Sentiment Treebank）数据集作为文本分类的训练集、验证集和测试集。由于该数据集规模较小，文本分类的效果受网络初始值的影响较大，导致了评价指标的不稳定。为了更客观地评价卷积网络中，不同词向量对文本分类性能的影响，对每一份词向量重复做多次实验。在每次实验中，输入层词表示均初始化为这份词向量，网络结构中的其它参数则初始化为不同的随机值。对于每一次实验，在训练集上训练卷积神经网络，取验证集上准确率最高的点，并报告其在测试集上的准确率。最后将5 组实验的测试集准确率的平均值作为最终的评价指标。</p><h4 id="词性标注-pos"><a href="#词性标注-pos" class="headerlink" title="词性标注(pos)"></a>词性标注(pos)</h4><p>词性标注（part-of-speech tagging）是一个经典的序列标注问题。在这个任务中，使用Collobert 等人提出的网络，对句子中的每个词做序列标注。该任务选用华尔街日报数据集。评价指标为模型在验证集上达到最佳效果时，测试集上的准确率。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了词向量的多种表示方法，主要为word2vec基础上的多种模型，侧重于中文词向量模型。&lt;br&gt;&lt;img src=&quot;http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%91%E5%B1%95%E5%9B%BE.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;2018年7月23日更新，在NLP课上宗成庆老师在讲词向量时提到，除了基于文本的词汇语义表示模型，还有基于图像、语音、多模态信息学习词汇语义表示等，这一些有空再补充。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>GWE:Learning Chinese Word Representations From Glyphs Of Characters读书笔记</title>
    <link href="http://yoursite.com/2018/07/15/GWE%EF%BC%9ALearning-Chinese-Word-Representations-From-Glyphs-Of-Characters%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/</id>
    <published>2018-07-15T06:27:07.000Z</published>
    <updated>2018-07-15T08:04:52.027Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文GWE:Learning Chinese Word Representations From <strong>Glyphs</strong> Of Characters中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Learning Chinese Word Representations From Glyphs Of Characters》是国立台湾大学2017年在EMNLP发表的，在词向量中引入了convolutional auto-encoder（convAE），提取字的信息，提升了词向量的质量。<br>论文中第二部分关于词向量在汉字领域的相关工作研究做的比较充分，在写论文的时候可以适当参考引用。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型主要有两部分，分别是特征提取器和词向量训练模型。</p><h3 id="Character-Bitmap-Feature-Extraction"><a href="#Character-Bitmap-Feature-Extraction" class="headerlink" title="Character Bitmap Feature Extraction"></a>Character Bitmap Feature Extraction</h3><p>本文使用的是Masci等在2011年提出的convAE，结构图如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE1.png" alt="此处输入图片的描述"><br>得到字的glyph特征。</p><h3 id="词向量训练"><a href="#词向量训练" class="headerlink" title="词向量训练"></a>词向量训练</h3><p>针对词向量训练，作者提出了四种不同的模型。</p><h4 id="Enhanced-by-ContextWord-Glyphs"><a href="#Enhanced-by-ContextWord-Glyphs" class="headerlink" title="Enhanced by ContextWord Glyphs"></a>Enhanced by ContextWord Glyphs</h4><p><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE2.png" alt="此处输入图片的描述"><br>这种模型是在CWE的基础上改进而来，词$w_i$的词向量表示为：</p><script type="math/tex; mode=display">\vec{w}_{i}^{ctxG}=\vec{w}_i+\frac{1}{\left| C\left( i \right) \right|}\sum_{c_j\in C\left( i \right)}{\left( \vec{c}_j+\vec{g}_j \right)}</script><p>其中$\vec{g}_j$是由特征提取器提取出的特征。</p><h4 id="Enhanced-by-TargetWord-Glyphs"><a href="#Enhanced-by-TargetWord-Glyphs" class="headerlink" title="Enhanced by TargetWord Glyphs"></a>Enhanced by TargetWord Glyphs</h4><p><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE3.png" alt="此处输入图片的描述"><br>这种模型将当前词中字的特征与上下文结合，一起预测当前词。</p><h4 id="RNN-Skipgram"><a href="#RNN-Skipgram" class="headerlink" title="RNN-Skipgram"></a>RNN-Skipgram</h4><p>以下两种模型直接从特征中学习词向量，没有使用上下文信息。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE4.png" alt="此处输入图片的描述"><br>在Skip-gram的基础上，以特征作为输入，两层GRU，两层全连接ELU，预测当前词的上下文。</p><h4 id="RNN-GloVe"><a href="#RNN-GloVe" class="headerlink" title="RNN-GloVe"></a>RNN-GloVe</h4><p><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE5.png" alt="此处输入图片的描述"><br>类似于GloVe模型，将原模型中词向量部分改写为当前词的特征。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验从Word Similarity、Word Analogy、Case Study三方面对比GWE与其他模型的优劣。其中在有的语料上的表现并不如之前的模型。作者将这种情况归结为“If character in iformation does not play a role in learning word representations, character glyphs may not be useful.”说明不要管模型复杂与否，适合应用场景的才是最好的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文GWE:Learning Chinese Word Representations From &lt;strong&gt;Glyphs&lt;/strong&gt; Of Characters中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>JWE:Joint Embeddings of Chinese Words,Characters,and Fine-grained Subcharacter Components</title>
    <link href="http://yoursite.com/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components读书笔记/</id>
    <published>2018-07-14T09:27:28.000Z</published>
    <updated>2018-07-15T06:30:13.801Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文JWE：Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components》是香港科技大学2017年在EMNLP发表的，同样是在词向量生成部分进行了改进，引入了人工总结的<strong>“字件信息”</strong>，提升了词向量的质量。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/JWE.png" alt="此处输入图片的描述"><br>根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起联合学习词向量。其中$w<em>i$是目标词，$w</em>{i-1}$是目标词左边的词，$w<em>{i+1}$是目标词右边的词；$c</em>{i-1}$是目标词左边的词中的单字，$c<em>{i+1}$是目标词右边的词中的单字；$s</em>{i-1}$是目标词左边的词拆分的字件，$s_{i+1}$是目标词右边的词拆分的字件,$s_i$是目标词的字件；<br>模型的目标是最大化以下对数似然：</p><script type="math/tex; mode=display">L\left( w_i \right)=\sum_{k=1}^3{\log P\left( w_i|h_{i_k} \right)}\tag1</script><p>其中,$h<em>{i_1}$,$h</em>{i<em>2}$,$h</em>{i_3}$分别为上下文单词、单字、组件的组合。</p><script type="math/tex; mode=display">P\left( w_i|h_{i_k} \right) =\frac{\exp \left( h_{i_k}^{T}\hat{v}_{w_i} \right)}{\sum_{j=1}^N{\exp \left( h_{i_k}^{T}\hat{v}_{w_j} \right)}}\tag2</script><script type="math/tex; mode=display">h_{i_1}=\frac{1}{2T}\sum_{-T\leqslant j\leqslant T,j\ne 0}{v_{w_{i+j}}}\tag3</script><p>在以上3个公式中，$v<em>{w_i}$,$v</em>{c<em>i}$,$v</em>{s<em>i}$分别为上下文、单字、字件的<strong>“输入向量”</strong>，$\hat{v}</em>{w<em>j}$是<strong>“输出向量”</strong>；$h</em>{i<em>1}$为上下文“输入向量”的均值，同理，$h</em>{i<em>2}$为上下文单字“输入向量”的均值，$h</em>{i_3}$为上下文字件“输入向量”的均值。<br>给定语料$D$,模型的目标是最大化：</p><script type="math/tex; mode=display">L(D)=\sum_{w_i \in D}{L(w_i)}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章在word similarity evaluation和word analogy tasks两个任务上对比JWE与之前词向量模型。<br>发现在word analogy tasks任务上取得了显著提高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文JWE：Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>Character and Word Embedding读书报告</title>
    <link href="http://yoursite.com/2018/07/14/Character-and-Word-Embedding%E8%AF%BB%E4%B9%A6%E6%8A%A5%E5%91%8A/"/>
    <id>http://yoursite.com/2018/07/14/Character-and-Word-Embedding读书报告/</id>
    <published>2018-07-14T06:45:41.000Z</published>
    <updated>2018-07-15T06:30:06.374Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文CWE：Joint Learning of Character and Word Embeddings中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Joint Learning of Character and Word Embeddings》是刘知远老师团队2015年在顶会Ijcai上发表的，在词向量生成部分进行了改进，引入了单个汉字的信息（论文主要针对的是中文），提升了词向量生成的质量。因为模型名称叫做“character-enhanced word embeddding model”，故模型简称为CWE。<br>从论文的题目可以看出，这篇paper在进行词向量训练的时候，把组成词语的汉字单独抽取出来，和词语一起进行训练。这样就使那些共享汉字的词语之间产生了联系，因为paper的假设是<strong>“semantically compositional”</strong>的词语中的汉字对词语的意思具有一定的表征作用，比方说词语“智能”。但是在汉语中并不是所有的词语都是semantically compositional，比方说一些<strong>音译词</strong>“巧克力”，“沙发”，再比方说一些实体的名称，比方说一些人名、地名和国家名。在这些词语中，单个汉字的意思可能和本来这个词语要表达的意思是完全没有关系的。在本篇paper中，作者做了大量的工作去把这些没有semantically compositional性质的词语全部人工的挑选出来，对于这些词语不去进行单个字的拆分处理。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/CWE1.png" alt="此处输入图片的描述"><br>这篇文章提出的模型是在word2vec的CBOW模型基础上改进而来，模型的优化函数：</p><script type="math/tex; mode=display">\frac{1}{M}\sum_{i=k}^{N-k}logPr(x_i|x_{i-k},....,x_{i+k})\tag{1}</script><p>其中，</p><script type="math/tex; mode=display">Pr(x_i|x_{i-k},....,x_{i+k})=\frac{exp(x_o\cdot x_i)}{\sum_{x_j'\in{Dictionary}}exp(x_o\cdot x_j')}\tag{2}</script><p>在CBOW模型中，context的表示是$w_i$前后窗口内的词向量的相加求均值。</p><script type="math/tex; mode=display">x_o=\frac{1}{2k}\sum_{j=i-k,....i+k}x_j\tag{3}</script><p>而在CWE模型中，对于context中的词语的表征，一方面来自于词向量，还有一部分在自于这些词语中的字的向量，具体的计算方式如下：</p><script type="math/tex; mode=display">x_j=w_j\oplus\frac{1}{N_j}\sum_{k=1}^{N_j}c_k\tag{4}</script><p>其中，其中，$N_j$是单词$w_j$中的汉字个数，$c_k$是字向量。$\oplus$对应的操作有拼接和相加两种方式，paper里说拼接方式虽然增加了模型的复杂度，但是对于效果的提升并不明显，因此后面的模型中直接就采用了相加的方式，公式如下所示： </p><script type="math/tex; mode=display">x_j=\frac{1}{2}(w_j+\frac{1}{N_j}\sum_{k=1}^{N_j}c_k)\tag{5}</script><p>注意上述公式中的$\frac{1}{2}$非常重要，它保证了具有semantically compositional的词语和不具有semantically compositional词语在计算距离时的一致性。同时paper指出，为了简化起见只对context的生成考虑字向量信息，target部分不予考虑。其中对于$\sum_{k=1}^{N_j}c_k$计算部分只是把一个词语中的汉字向量进行<strong>等权</strong>相加，如果利用<strong>attention机制</strong>，可能效果更好。</p><h2 id="单字不同语义的解决办法"><a href="#单字不同语义的解决办法" class="headerlink" title="单字不同语义的解决办法"></a>单字不同语义的解决办法</h2><p>同一个汉字，在不同的词语中可能具有完全不同的语义，如果使用一个向量来表征一个字，那么很可能会无法标识出这些差异性，故使用多个向量来表征同一个汉字，有下面几种方式：</p><h3 id="Position-based-Character-Embedding"><a href="#Position-based-Character-Embedding" class="headerlink" title="Position-based Character Embedding"></a>Position-based Character Embedding</h3><p>从名字可以看出，在该模型中同一个汉字根据其在词语中出现的位置不同，对应不同位置的向量表示形式。分析可知，汉字在词语中出现的位置有：Begin,Middle,End这三种情况，故每一个汉字都有三种向量表示形式，在进行$x<em>j=\frac{1}{2}(w_j+\frac{1}{N_j}\sum</em>{k=1}^{N_j}c_k)$ 生成向量操作的时候，对于$c_k$按照其在词语中出现的位置进行合理的筛选。这种方式比较简单，但是缺点也是比较明显的，它假设的前提是同一个汉字只要位于不同单词的同一个位置就具有相同的语义，这显然在一些情况下是不成立的。</p><h3 id="Cluster-based-Character-Embedding"><a href="#Cluster-based-Character-Embedding" class="headerlink" title="Cluster_based Character Embedding"></a>Cluster_based Character Embedding</h3><p>这种方法看的不是很懂，简单来讲就是：对于每一个汉字提前分配x个字向量，称之为模式向量。利用该词对应的词语的context信息，从一个汉字的所有模式向量中选择一个和context语义计算上最相似的作为该汉字对应的向量。</p><h3 id="Nonparametric-Cluster-based-Character-Embeddings"><a href="#Nonparametric-Cluster-based-Character-Embeddings" class="headerlink" title="Nonparametric Cluster-based Character Embeddings"></a>Nonparametric Cluster-based Character Embeddings</h3><p>该模型和Cluster_based Character Embedding模型是很相似的，唯一不同的是，Cluster_based Character Embedding中的每一个汉字对应的模型向量的数量是一个预先设定的固定值，也就是作为模型的超参数。而在Nonparametric Cluster-based Character Embeddings模型中，该值是一个模型自动学习的值。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>1 本文提出的汉字和词语结合的方式就是简单的<strong>向量相加</strong>操作，也许应用复杂一点的操作（比如矩阵变换）等方式可以把二者更加合理地结合在一起；<br>2 在相加的时候，需要给以每一个汉字不同权重，这也和我之前说的一致，可以利用attention机制，只不过在2015年的时候还没有attention的概念。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文CWE：Joint Learning of Character and Word Embeddings中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>word2vec公式推导</title>
    <link href="http://yoursite.com/2018/07/10/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://yoursite.com/2018/07/10/word2vec公式推导/</id>
    <published>2018-07-10T08:47:30.000Z</published>
    <updated>2018-07-15T06:29:54.606Z</updated>
    
    <content type="html"><![CDATA[<p>本文接着word2vec的那篇概述再推导一下word2vec中的公式，也是《word2vec Parameter Learning Explained》论文学习笔记，有一些细节的推导我写在了论文处。<br>这篇论文详细地推导和解释了word2vec模型的参数更新公式，包括：<strong>CBOW</strong>（continuous bag-of-word）模型和<strong>SG</strong>（skip-gram）模型，以及两种参数优化技术：<strong>hierarchical softmax</strong> 和 <strong>negative sampling</strong>.<br><a id="more"></a></p><h2 id="Continuous-Bag-of-Word-Model"><a href="#Continuous-Bag-of-Word-Model" class="headerlink" title="Continuous Bag-of-Word Model"></a>Continuous Bag-of-Word Model</h2><h3 id="One-word-context"><a href="#One-word-context" class="headerlink" title="One-word context"></a>One-word context</h3><p>我们从CBOW模型的最简单版本开始介绍——One-word context。即我们假定context（预测目标单词的上下文信息）只有一个单词，也就是说One-word context 模型是在只要一个上下文单词（one context word）的情况下来预测一个目标单词（one target word）的。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC1.png" alt="此处输入图片的描述"><br>如图1描述的就是One-word context定义之下的神经网络模型。这里我们假设文本词汇量的大小为V,隐藏层的大小为N，相邻层的神经元是全连接的。输入层是一个用one-hot方式编码的单词向量$x=(x_1,…,x_V)$，其中只有一个$x_i$为1，其余均为0。<br>从输入层到隐藏层的权重值可以用一个$V×N$维的矩阵$W$来表示，即 </p><script type="math/tex; mode=display">W= \begin{pmatrix} \omega_{11}&\omega_{12}&...&\omega_{1N}\\ \omega_{21}&\omega_{22}&...&\omega_{2N}\\ ...&...&...&...\\ \omega_{V1}&\omega_{V2}&...&\omega_{VN} \end{pmatrix}</script><p> 其中$W$矩阵的每一行代表的是一个与输入层相关的单词的N维向量表示形式$v<em>ω$。那么假设我们给定了一个输入单词（a context）,其单词向量的第k个元素$x_k=1$，其余均为0，则有<br> $\mathbf h= \mathbf W^Tx=\mathbf W</em>{(k,\bullet)}^T x<em>k=\mathbf v</em>{\omega<em>I}^T\tag{1}$<br> 从（1）式我们可以看出，$h$向量完全是从$W$矩阵第k行复制过来的（同$\mathbf v</em>{\omega<em>I}$均为N维向量）。$\mathbf v</em>{\omega<em>I}$即为输入单词$ω_I$的一种向量表示（其实就是<strong>输入向量</strong>，我们后面会提到）。<br> 分析完输入层到隐藏层之后，我们再看隐藏层到输出层，同样连接权重用一个新的$N × V$矩阵$\mathbf W’={\omega</em>{ij}’ }$来表示如下：</p><script type="math/tex; mode=display"> \mathbf W'= \begin{pmatrix} \omega_{11}'&\omega_{12}'&...&\omega_{1V}'\\ \omega_{21}'&\omega_{22}'&...&\omega_{2V}'\\ ...&...&...&...\\ \omega_{N1}'&\omega_{N2}'&...&\omega_{NV}' \end{pmatrix}</script><p> 通过这些权重，我们可以为词表中的每一个单词都计算出一个得分$μ_j$</p><script type="math/tex; mode=display">\mu_j=\mathbf {v_{\omega_j}'}^T\mathbf h\tag{2}</script><p> 其中，${v_{\omega_j}’}$即为矩阵$W′$的第j列向量（也是N维向量，其实就是单词w的输出向量，我们后面会提到）。<br> 经过以上讨论之后，我们可以使用一种对数-线性分类模型softmax函数来计算单词的后验分布（是多项式分布）</p><script type="math/tex; mode=display">p(\omega_j|\omega_I)=y_j=\frac{\exp(\mu_j)}{\sum_{j'=1}^V\exp(\mu_{j'})}\tag{3}</script><p> 其中， $y_j$表示输出层第j个神经单元的输出值。将（1）式和（2）式代入（3）式我们可以得到：</p><script type="math/tex; mode=display">p(\omega_j|\omega_I)=\frac{\exp({\mathbf v_{\omega_j}'}^T \mathbf v_{\omega_I})}{\sum_{j'=1}^V\exp({\mathbf v_{\omega_j}'}^T \mathbf v_{\omega_I})}\tag{4}</script><p> 注意：正如前文所述，$v<em>ω$和$v′</em>ω$是单词的两种向量表示形式。其中$v<em>ω$实际上是权重矩阵$W$（input-&gt;hidden）的某一行向量，$v′</em>ω$则是权重矩阵$W′$（hidden-&gt;output）的某一列向量。我们将$v<em>ω$和$v′</em>ω$分别称为“输入向量（input vector）”和“输出向量（output vector）”（二者均为N维向量）。</p><h4 id="Update-equation-for-hidden→output-weights"><a href="#Update-equation-for-hidden→output-weights" class="headerlink" title="Update equation for hidden→output weights"></a>Update equation for hidden→output weights</h4><p> 在我们推导hidden→output权重的更新公式的过程中，需要用到哦神经网络的反向传播算法，对这部分内容不熟悉的读者可以参考附录A的内容。<br>由以上描述可知，该模型训练的目标就是求公式（4）的最大值。公式（4）代表的就是给定上下文信息（这里为一个单词$ω_I$）以及其权重矩阵的情况下，预测其实际输出单词（即上下文信息的中心词$ω_O$）的条件概率。</p><script type="math/tex; mode=display">\begin{align}& \max p(\omega_O|\omega_I)=\max y_{j^*}\tag{5}\\&=\max \log y_{j^*}\tag{6}\\&=\mu_{j^*} - \log \sum_{j'=1}^V \exp(\mu_{j'}):=-E\tag{7}\end{align}</script><p>其中，$E=-\log p(\omega_O|\omega_I)$ 为该模型的损失函数（我们需要找出它的最小值），$j^<em>$则为实际输出单词的索引下标。我们注意到该损失函数可以理解为一种特殊情形下的<em>*交叉熵</em></em>计算。<br>现在我们开始推导从隐藏层到输出层的权重矩阵在模型训练过程中的参数更新公式。首先我们对损失函数$E=-\log p(\omega_O|\omega_I)$求关于得分$μ_j$的偏导数，得结果为：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial\mu_j}=y_j-t_j:=e_j\tag{8}</script><p> 其中，$t<em>j=1(j=j^∗)$ ,即当且仅当输出层的第j个神经单元为真实的输出单词时 $t_j$的取值为1。接下来我们根据链式法则求出损失函数$E$关于矩阵$W′$元素 $\omega</em>{ij}’$的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial \omega_{ij}'}=\frac{\partial E}{\partial \mu_j}\cdot \frac{\partial \mu_j}{\partial \omega_{ij}'}=e_j\cdot h_i \tag{9}</script><p> 因此，采用随机梯度下降算法（SGD）,我们最终得到了隐藏层到输出层（hidden→output）权重的更新公式如下：</p><script type="math/tex; mode=display">\begin{align}{\omega_{ij}'}^{(new)}={\omega_{ij}'}^{(old)}-\eta \cdot e_j \cdot h_i\tag{10}\end{align}</script><p>or</p><script type="math/tex; mode=display">\begin{align}{\mathbf v_{\omega_j}'}^{(new)}= {\mathbf v_{\omega_j}'}^{(old)} - \eta \cdot e_j \cdot \mathbf h \space \space for\space j=1,2,...V.\tag{11}\end{align}</script><p>其中， $η&gt;0$为参数更新的学习速率；$e<em>j=y_j−t_j$；$h_i$ 为隐藏层的第i个神经单元；$\mathbf v</em>{\omega_j}’$为$ω_j$的输出向量。<br>由公式（11）我们可以看出：在更新权重参数的过程中，我们需要检查词汇表中的每一个单词，计算出它的输出概率$y_j$，并与期望输出$t_j$（取值只能为0或者1）进行比较。比较过程如下：</p><ul><li>如果$y<em>j&gt;t_j$(“overestimating”)，那么就从向量$\mathbf v</em>{\omega<em>j}’$中减去隐藏向量$h$的一部分（例如$\mathbf v</em>{\omega<em>I}$），这样向量$\mathbf v</em>{\omega<em>j}’$就会与向量$\mathbf v</em>{\omega_I}$相差更远。</li><li>如果$y<em>j&lt;t_j$“underestimating”，这种情况只有在$t_j=1$时，才会发生，此时$ω_j=ω_O$），则将隐藏向量$h$的一部分加入$\mathbf v</em>{\omega<em>O}’$，使得$\mathbf v</em>{\omega<em>O}’$与$\mathbf v</em>{\omega_I}$更接近。</li><li>如果$y<em>j$与$t_j$非常接近，则此时$e_j=y_j−t_j$由于（公式（8））非常接近于0，故更新参数基本上没什么变化。<br>这里需要再次提醒的是：$v</em>ω$和$v′_ω$是单词$ω$的两种不同的向量表示形式。<h4 id="Update-equation-for-input→hidden-weights"><a href="#Update-equation-for-input→hidden-weights" class="headerlink" title="Update equation for input→hidden weights"></a>Update equation for input→hidden weights</h4>在介绍完hidden→output的权重矩阵更新公式之后，我们接着介绍input→hidden的权重矩阵$W$的更新过程。我们继续对损失函数$E$求关于隐藏层$h_i$的偏导数，得： <script type="math/tex; mode=display">\frac{\partial E}{\partial h_i}=\sum_{j=1}^V \frac{\partial E}{\partial \mu_j} \cdot \frac{\partial \mu_j}{\partial h_i}=\sum_{j=1}^V e_j \cdot \omega_{ij}':=EH_i \tag{12}</script>其中$h<em>i$为隐藏层第i个神经单元的输出；$μ_j$在公式（2）中已经定义，表示输出层第j个神经单元的输出；$e_j=y_j−t_j$为输出层第j个单词的预测误差。因此$EH$应该是一个N维向量，它的每一个元素代表的是词汇表中的每个单词的预测误差$e_j$与$ω′</em>{ij}$在j=1到V上的乘积之和。<br>接下来，我们需要求出损失函数$E$关于权重矩阵$W$的偏导数。首先，分解公式（1），我们知道隐藏层激活单元的输出$h_i$是输入层$x$与权重的线性组合，即<script type="math/tex; mode=display">h_i=\sum_{k=1}^V x_k \cdot \omega_{ki} \tag{13}</script>因此对于权重矩阵$W$的每一个元素，我们求关于$E$的偏导数，得到： $$\frac{\partial E}{\partial \omega_{ki}}=\frac{\partial E}{\partial h_i} \cdot \frac{\partial h_i}{\partial \omega_{ki}}=EH_i \cdot x_k \tag{14}$$因此我们利用张量乘积的方式，便可得到：<script type="math/tex; mode=display">\frac{\partial E}{\partial W}=\mathbf x \otimes EH = \mathbf xEH^T \tag{15}</script>我们再次得到了一个$N×V$的矩阵。由于$x$向量只有一个非0元素，因此$\frac{\partial E}{\partial W}$只有一行是N维非0向量$EH^T$，因此矩阵$W$的更新公式为：  $${\mathbf v_{\omega_I}}^{(new)}={\mathbf v_{\omega_I}}^{(old)}-\eta \cdot EH^T \tag{16}$$其中$\mathbf v<em>{\omega_I}$是矩阵$W$的其中一行，是唯一的上下文单词（context word）的“输入向量”,也是矩阵$W$唯一的导数非0的行向量。 除了$\mathbf v</em>{\omega_I}$以外，矩阵$W$的其他行向量在参数更新迭代过程中都会保持不变（因为其导数为0）。<br>与矩阵$W′$的更新过程相似，对于公式（16），我们分析如下：</li><li>如果过高地估计了某个单词$ω_j$作为最终输出单词的概率（即：$y_j&gt;t_j$），则上下文单词$ω_I$（context word ）的输入向量与单词$ω_j$的输出向量在更新的过程中会相差越来越大。</li><li>如果相反，某个单词$ω_j$作为最终输出单词的概率被低估（即：$y_j&lt;t_j$），则单词$ω_I$的输入向量与单词$ω_j$的输出向量在更新过程中会越来越接近。</li><li>如果对于单词$ω_I$的概率预测是准确的，则对于单词的输入向量在更新过程中几乎保持不变。</li></ul><p>因此，上下文单词$ω_I$（context word ）的输入向量的更新取决于词汇表中所有单词的预测误差。预测误差越大，则该单词对于上下文单词的输入向量的更新过程影响越大。</p><p>在介绍完One-word context的CBOW模型之后，我们接着介绍multi-word context下的CBOW模型。</p><h3 id="Multi-word-context"><a href="#Multi-word-context" class="headerlink" title="Multi-word context"></a>Multi-word context</h3><p>根据字面意思我们就可以看出，基于multi-word context的CBOW模型就是利用多个上下文单词来推测中心单词target word的一种模型。其结构如图2所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC2.png" alt="此处输入图片的描述"></p><p>其隐藏层的输出值的计算过程为：首先将输入的上下文单词（context words）的向量叠加起来并取其平均值，接着与input→hidden的权重矩阵相乘，作为最终的结果，公式如下：</p><script type="math/tex; mode=display">\begin{align}& \mathbf h = \frac{1}{C} \mathbf W^T(\mathbf x_1 + \mathbf x_2 + \cdots +\mathbf x_C)\tag{17}\\& = \frac{1}{C}(\mathbf v_{\omega_1}+\mathbf v_{\omega_2} + \cdots+\mathbf v_{\omega_C})^T\tag{18}\end{align}</script><p>其中$C$为上下文单词的个数，$ω<em>1,…,ω_C$为上下文单词，$v</em>ω$为单词$ω$的输入向量。损失函数为：</p><script type="math/tex; mode=display">\begin{align}& E = - \log p(\omega_O|\omega_{I,1},...,\omega_{I,C})\tag{19}\\& =- \mu_{j^*} + \log \sum_{j'=1}^{V} exp(\mu_{j'})\tag{20}\\& = - {\mathbf v_{\omega_O}'}^T \cdot \mathbf h + \log \sum_{j'=1}^{V} \exp({\mathbf v_{\omega_j}'}^T \cdot \mathbf h)\tag{21}\end{align}</script><p>同样，由hidden→output的权重更新公式与one-word-context模型下的一模一样，即类似于公式（11），我们直接写在下面：</p><script type="math/tex; mode=display">{\mathbf v_{\omega_j}'}^{(new)}={\mathbf v_{\omega_j}'}^{(old)}-\eta \cdot e_j \cdot \mathbf h \space  \space  \space for \space \space  j=1,2,...,V\tag{22}</script><p> 由input→hidden 的权重矩阵更新公式与公式（16）类似，只不过现在我们需要对每一个上下文单词$ω_{I,c}$都执行如下更新公式：<br> $${\mathbf v_{\omega_{I,c}}}^{(new)}={\mathbf v_{\omega_{I,c}}}^{(old)} - \frac{1}{C}\cdot \eta \cdot EH^T \space \space for \space \space c=1,2,...,C.\tag{23}$$<br> 其中 ${\mathbf v_{\omega_{I,c}}}$为上下文context中第c 个单词的输入向量；$η$为正学习速率；$EH=\frac{\partial E}{\partial h_i}$由公式（12）给出。</p><h2 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h2><p>与CBOW模型正好相反，Skip-Gram模型是根据中心单词（target word）来预测其上下文信息（context words）。如图3所示，为Skip-Gram模型的结构示意图。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC3.png" alt="此处输入图片的描述"><br>我们仍然使用$\mathbf v_{\omega_I}$来表示输入层上唯一的那个单词的<strong>输入向量</strong>，因此，我们对于隐藏层的输出值$h$的计算公式与第一节公式（1）相同，表示如下：</p><script type="math/tex; mode=display">\mathbf h = {\mathbf W}_{(k,\bullet)}^T := \mathbf v_{\omega_I}\tag {24}</script><p> 公式（24）显示：$h$向量其实就是input-&gt;hidden权重矩阵$W$的某一行结合输入单词$ω_I$的向量拷贝。在输出层，与CBOW模型的输出为单个多项式分布不同的是，SG模型在输出层输出了C个多项式分布。每个输出都使用相同的hidden-&gt;output矩阵计算：</p><script type="math/tex; mode=display">p\left( \omega _{c,j}=\omega _{O,c}|\omega _I \right) =y_{c,j}=\frac{\exp \left( \mu _{c,j} \right)}{\sum_{j'-1}^V{\exp \left( \mu '_j \right)}} \tag{25}</script><p>   其中，$\omega<em>{c,j}$表示输出层的第c个panel的第j个单词（何为panel?就是输出层的表示每个上下文单词的神经元的组合，图中一种有C个context words，所以总共有C个panel）；$\omega</em>{O,c}$实际上表示的是输出上下文单词（output context words）的第c个单词；$ω<em>I$是唯一的输入单词；$y</em>{c,j}$为输出层的第c个panel上的第j个神经单元的概率输出值；$\mu_{c,j}$表示的是输出层第c个panel的第j个神经元的输入值；<strong>由于输出层的所有panels共享同一权重矩阵</strong>$W′$,因此：</p><script type="math/tex; mode=display">\mu_{c,j}=\mu_j={\mathbf v_{\omega_j}'}^T\cdot \mathbf h, \space for \space c=1,2,...,C\tag{26}</script><p> 其中，$\mathbf v_{\omega_j}’$为词汇表第j个单词$ω_j$的输出向量；同样，它也是取自于hidden→output权重矩阵$W′$的一列。<br> SG模型参数更新公式的推导过程与one-word-context 模型的推导过程大体上一样。这里我们将损失函数变为：</p><script type="math/tex; mode=display">\begin{align}&E=-\log p(\omega_{O,1},\omega_{O,2},...,\omega_{O,C}|\omega_I)\tag{27}\\&=-\log \prod_{c=1}^C \frac{\exp(\mu_{c,j_c^*})}{\sum_{j'=1}^V \exp(\mu_{j'})}\tag{28}\\&=-\sum_{c=1}^C \mu_{j_c^* }+C\cdot\log\sum_{j'=1}^V\exp(\mu_{j'})\tag{29}\end{align}</script><p>其中，$j_c^*$为第c个输出层输出的上下文单词在词汇表中的真实索引。</p><p>在得到损失函数$E$之后，我们对输出层的每一个panel上的所有激活单元的输入值$\mu<em>{c,j}$,均求其关于$E$的偏导数，得：<br> $$\frac{\partial E}{\partial \mu_{c,j}}=y_{c,j}-t_{c,j}:=e_{c,j}\tag {30}$$<br> 其中$e</em>{c,j}$为输出层神经元的预测误差，与公式（8）类似。为了简化符号，我们定义一个$V$维的向量$EI={\{EI_1,...,EI_V\}}$作为所有上下文单词的预测误差之和，$EI_j$用公式定义如下：</p><script type="math/tex; mode=display">EI_j=\sum_{c=1}^C e_{c,j}\tag{31}</script><p> 接下来，我们计算hidden-&gt;output权重矩阵$W′$关于$E$的偏导数为：<br> $$\frac{\partial E}{\partial \omega_{ij}'}=\sum_{c=1}^C\frac{\partial E}{\partial \mu_{c,j}}\cdot\frac{\partial \mu_{c,j}}{\partial \omega_{ij}'}=EI_j\cdot h_i\tag{32}$$<br> 这样，我们就得到了hidden→output权重矩阵$W′$的参数更新公式为：<br> $${\omega_{ij}^{'}}^{(new)}={\omega_{ij}^{'}}^{(old)}-\eta\cdot EI_j\cdot h_i\tag{33}$$<br> 或者</p><script type="math/tex; mode=display">{\mathbf v_{\omega_j}'}^{(new)}={\mathbf v_{\omega_j}'}^{(old)}-\eta \cdot EI_j \cdot \mathbf h \space\space\space for \space j=1,2,...,V.\tag{34}</script><p> 上述参数更新公式的直观概念理解与上文公式（11）无二，除了一点就是：输出层的预测误差的计算是基于多个上下文单词context words,而不是单个目标单词 target word;需注意的是对于每一个训练样本，我们都要利用该参数更新公式来更新hidden→output权重矩阵$W′$的每个元素。</p><p>同样，对于input→hidden权重矩阵$W$的参数更新公式的推导过程，除了考虑要将预测误差$e_j$替换为$EI_j$外，其他也与上文公式（12）到公式（16）类似。这里我们直接给出更新公式：<br> $${\mathbf v_{\omega_I}}^{(new)}={\mathbf v_{\omega_I}}^{(old)}-\eta \cdot EH^T\tag{35}$$<br> 其中，$EH$是一个$N$维向量，组成该向量的每一个元素可以用如下公式表示：</p><script type="math/tex; mode=display">EH_i=\sum_{j=1}^V EI_j\cdot\omega_{ij}'\tag{36}</script><p> 公式（36）的直观理解与公式（16）类似，这里不作描述。</p><h2 id="Optimizing-Computational-Efficiency"><a href="#Optimizing-Computational-Efficiency" class="headerlink" title="Optimizing Computational Efficiency"></a>Optimizing Computational Efficiency</h2><p>总结以上的模型介绍，我们发现所有模型的词汇表中的每个单词都存在两个向量表示形式：输入向量$v<em>ω$与输出向量$v′</em>ω$.对于输入向量的参数学习成本并不高，但对于输出向量的学习成本代价是非常昂贵的。根据更新公式（22）和（23），我们可以发现，为了更新输出向量$v′<em>ω$，对于每一个训练样例，我们必须迭代遍历词汇表中所有的单词$ω_j$，计算出它们的输入值$μ_j$、概率预测值$y_j$（或者SG模型中的$y</em>{c,j}$），预测误差$e<em>j$（或者SG模型的$EI_j$）。最终使用预测误差更新它们的输出向量$v′_j$.<br>显然，对于每一个训练样例都要对所有单词计算上述各值，其成本是昂贵的。特别是对于大型的词汇表，这种计算方式是不切实际的。因此为了解决这个问题，直观的方式是限制必须要更新的训练样例的输出向量的数目。一种有效的实现方式就是：hierarchical softmax（分层softmax），另一种实现通过采样的方式解决，我们在下个章节来讨论。<br>这两种方法都是通过只优化输出向量更新的计算过程来实现的。在我们的公式推导过程中，我们关心的有三个值：（1）$E$，新的目标函数；（2）$\frac{\partial E}{\partial \mathbf v</em>\omega’}$，新的关于输出向量的更新公式；（3）$\frac{\partial E}{\partial \mathbf h}$，为了更新输入向量反向传播的预测误差的加权和。</p><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>Hierarchical softmax 是一种有效的计算 softmax 的方式。该模型使用一棵二叉树来表示词汇表中的所有单词。所有的$V$个单词都在二叉树的叶节点上。非叶子节点一共有$V−1$个。对于每个叶子节点，从根节点root到该叶子节点只有一条路径；这条路径用来评估用该叶子节点代表该叶子节点上单词的概率值。二叉树的结构如图4所示：</p><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC4.png" alt="此处输入图片的描述"><br> 其中白色的树节点代表的是词汇表中的单词，灰色节点为内部节点。图中高亮显示的是一条从根节点到$ω<em>2$的路径。该条路径的长度为$L(\omega_2)=4$。$n(ω,j)$表示从根节点到单词$ω$的路径上的第j个节点。<br> 在hierarchical softmax模型中，所有的词汇单词没有输出向量表示形式。不同的是，二叉树的每一个内部节点都有一个输出向量${\mathbf v</em>{n(\omega,j)}’}$。因此一个单词作为输出单词的概率计算公式定义如下：</p><script type="math/tex; mode=display">p(\omega = \omega_O)=\prod_{j=1}^{L(\omega)-1}\sigma \bigg(\Big[\Big[n\big(\omega,j+1\big)=ch\big(n\small(\omega,j\small)\big)\Big]\Big]\cdot{\mathbf v_{n(w,j)}'}^T\mathbf h\bigg)\tag{37}</script><p> 其中，$ch(n)$为节点$n$的左孩子节点；$\mathbf v<em>{n(\omega,j)}’$是内部节点$n(\omega,j)$的向量表示（输出向量）；$h$是隐藏层的输出值（在SG模型中，$h=\mathbf v</em>{\omega<em>I}$;而在CBOW模型中，$\mathbf h=\frac{1}{C}\sum</em>{c=1}^C \mathbf v_{\omega_c}$;$[[x]]$是一种特殊的函数定义如下：</p><script type="math/tex; mode=display">[[x]]=\begin{cases}1  & \text{if $x$ is true} \\-1, & \text{otherwise}\end{cases}\tag{38}</script><p>接下来，我们通过一个直观地例子来理解公式（37）。如图4所示，假定我们需要计算单词$ω_2$作为输出单词的概率。我们将这个概率定义为从根节点开始随机游走到叶节点$ω_2$的概率。则在每一个内部节点（包括根节点），我们都需要确定其路径指向左孩子节点还是右孩子节点的概率。我们将经过内部节点的路径指向左孩子的概率定义为：</p><script type="math/tex; mode=display">p(n,left)=\sigma({\mathbf v_n'}^T\cdot\mathbf h)\tag{39}</script><p> 我们可以看出，公式（39）的值取决于内部节点的向量表示$v′_n$和隐藏层的输出值$h$($h$的值取决于输入单词的向量表示)。显然，内部节点的路径指向右孩子的概率则可以表示为：</p><script type="math/tex; mode=display">p(n,right)=1-\sigma({\mathbf v_n'}^T\cdot\mathbf h)=\sigma(-{\mathbf v_n'}^T\cdot \mathbf h)\tag{40}</script><p> 顺着图4中从根节点到单词$ω_2$点的路径，我们可以计算出$ω_2$作为输出单词的概率为：</p><script type="math/tex; mode=display">\begin{align}& p(\omega_2=\omega_O)=p\Big(n(\omega_2,1),left\Big)\cdot p\Big(n(\omega_2,2),left\Big)\cdot p\Big(n(\omega_2,3),right\Big)\tag{41}\\& =\sigma \Big({\mathbf v_{n(\omega_2,1)}'}^T\mathbf h\Big)\cdot\sigma \Big({\mathbf v_{n(\omega_2,2)}'}^T\mathbf h\Big)\cdot\sigma \Big(-{\mathbf v_{n(\omega_2,3)}'}^T\mathbf h\Big)\cdot \tag{42}\end{align}</script><p>不难证明</p><script type="math/tex; mode=display">\sum_{i=1}^{V}p(\omega_i=\omega_O)=1\tag{43}</script><p> 现在我们开始推导内部节点的向量表示形式的参数更新公式。为了简化步骤，我们首先考虑单个上下文单词（one-word context）的模型。<br>为了简化公式，我们定义子公式的简化符号如下：</p><script type="math/tex; mode=display">[[\cdot]]:=[[n(\omega,j+1)=ch(n(\omega,j))]]\tag{44}</script> $$\mathbf v_j':=\mathbf v_{n_{\omega,j}}'\tag{45}$$<p> 则，给定一个训练样例，其误差函数我们可以定义如下：</p><script type="math/tex; mode=display">E=-\log p(\omega = \omega_O|\omega_I)=-\sum_{j=1}^{L(\omega)-1}\log\sigma([[\cdot]]{\mathbf v_j'}^T\mathbf h)\tag{46}</script><p> 对于误差函数$E$，我们取其关于$\mathbf v_j’\mathbf h$的偏导数，得：</p><script type="math/tex; mode=display">E=-\log p(\omega = \omega_O|\omega_I)=-\sum_{j=1}^{L(\omega)-1}\log\sigma([[\cdot]]{\mathbf v_j'}^T\mathbf h)\tag{46}</script><script type="math/tex; mode=display">\begin{align}&\frac{\partial E}{\partial \mathbf v_j'\mathbf h}=\Big(\sigma([[\cdot]]{\mathbf v_j'}^T\mathbf h)-1\Big)[[\cdot]]\tag{47}\\&=\begin{cases}\sigma({\mathbf v_j'}^T\mathbf h)-1 ,&\text{[[.]]=1} \\\sigma({\mathbf v_j'}^T\mathbf h),&\text {[[.]]=-1}\end{cases}\tag{48}\\&=\sigma({\mathbf v_j'}^T\mathbf h)-t_j\tag{49}\end{align}</script><p>其中$t_j=1$（如果$[[⋅]]=1$）或者$t_j=0$（如果$[[⋅]]=−1$）。<br>紧接着我们计算内部节点$n(ω,j)$的向量表示$\mathbf v_j’$关于函数$E$的偏导数，得：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial \mathbf v_j'}=\frac{\partial E}{\partial \mathbf v_j'\mathbf h}\cdot \frac{\partial \mathbf v_j'\mathbf h}{\partial \mathbf v_j'}=\Big(\sigma({\mathbf v_j'}^T\mathbf h)-t_j\Big)\cdot \mathbf h\tag{50}</script><p> 因此，更新公式为：</p><script type="math/tex; mode=display">{\mathbf v_j'}^{(new)}={\mathbf v_j'}^{(old)}-\eta\Big(\sigma({\mathbf v_j'}^T\mathbf h)-t_j\Big)\cdot \mathbf h\space,\space for \space j=1,2,...,L(\omega)-1\tag{51}</script><p> 我们可以将$\sigma({\mathbf v_j’}^T\mathbf h)-t_j$理解为内部节点$n(ω,j)$的预测误差。每一个内部节点的“任务”就是预测其随机游走路径是指向左孩子节点还是指向右孩子节点。$t_j=1$意味着节点$n(ω,j)$的路径指向左孩子节点；$t_j=0$则表示指向右孩子节点。$\sigma({\mathbf v_j’}^T\mathbf h)$是预测结果。对于一个训练实例，如果内部节点的预测值非常接近于真实值，则它的向量表示$\mathbf v_j’$的更新变化很小；否则$\mathbf v_j’$向量指向一个适当的方向是的该实例的预测误差逐渐减小。以上更新公式既能应用于CBOW模型，又能应用于SG模型。当在SG模型中使用该更新公式时，我们需要对C个output context words的每一个单词都重复此更新过程。</p><p>为了使用反向传播该预测误差来学习训练input→hidden的权重，我们对误差函数$E$求关于隐藏层输出值的偏导数，如下：</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial E}{\partial \mathbf h}=\sum_{j=1}^{L(\omega)-1}\frac{\partial E}{\partial \mathbf v_j'\mathbf h}\cdot\frac{\partial \mathbf v_j'\mathbf h}{\partial \mathbf h}\tag{52}\\&=\sum_{j=1}^{L(\omega)-1}\Big(\sigma({\mathbf v_j'}^T\mathbf h)-t_j\Big)\cdot \mathbf v_j'\tag{53}\\&:=EH\tag{54}\end{align}</script><p>接下来我们根据公式（23）便可以获得CBOW模型输入向量的更新公式。对于SG模型，我们需要计算上下文信息中的每个单词的$EH$值,并将$EH$值的和带入公式（35）,就能够得到输入向量的更新公式。<br>从以上更新公式我们可以看出：经过改进的模型Hierarchical softmax的每个训练样例的每个上下文单词的计算复杂度从$O(V)$降为$O(log(V))$级别。但是模型的参数几乎没有什么改变（内部节点对应V-1维向量，而原始模型的单词的输出向量维数为V）。</p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>Negative Sampling模型的思想比hierarchical softmax模型更直接了当,即：在每次迭代的过程中，有大量的输出向量需要更新，为了解决这一困难，negative sampling提出了只更新其中一部分输出向量的解决方案。<br>显然，最终需要输出的上下文单词（正样本）在采样的过程中应该保留下来并更新，同时我们需要采集一些单词作为负样本（因此称为“negative sampling”）。在采样的过程中，我们可以任意选择一种概率分布。我们将这种概率分布称为“噪声分布”（the noise distribution），用$P_n(\omega)$来表示。我们可以根据经验选择一种较好的分布。</p><p>在 word2vec中，我们无需使用一种能够产生良好定义的后验多项式分布的负采样形式，本文作者证明了使用下面简单的训练目标函数能够产生可靠的、高质量的 word embeddings:<br> $$E=-\log \sigma({\mathbf v_{\omega_O}'}^T\mathbf h)-\sum_{\omega_j\in W_{neg}} \log \sigma({-\mathbf v_{\omega_j}'}^T\mathbf h)\tag{55}$$<br> 其中$ω<em>O$是输出单词（the positive sample），$\mathbf v</em>{\omega<em>O}’$是输出向量；$h$是隐藏层的输出值：在CBOW模型中$\mathbf h=\frac{1}{C}\sum</em>{c=1}^{C} \mathbf v<em>{\omega_c}$，在SG模型中$\mathbf h=\mathbf v</em>{\omega<em>I}$；$W_{neg}={\{\omega_j|j=1,...,K\}}$是基于分布$P_n(\omega)$采样的一系列单词。<br>为了获得negative sampling模型的词向量更新公式，我们首先计算$E$关于输出单元$ω_j$的输入${\mathbf v</em>{\omega_j}’}^T\mathbf h$的偏导数：</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial E}{\partial{ \mathbf v_{\omega_j}'}^T\mathbf h}=\begin{cases}\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-1 ,&\text{if }\space \omega_j=\omega_O \\\sigma({\mathbf v_{\omega_j}'}^T\mathbf h),&\text {if}\space\omega_j\in W_{neg}\end{cases}\tag{56}\\&\space\space\space\space\space\space\space\space\space\space\space\space\space\space=\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-t_j\tag{57}\end{align}</script><p>其中，当$ω_j$是一个正样本时，$t_j=1$;否则$t_j=0$。接下来我们计算$E$关于单词$ω_j$的输出向量的偏导数：<br> $$\frac{\partial E}{\partial \mathbf v_{\omega_j}'}=\frac{\partial E}{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}\cdot \frac{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}{\partial {\mathbf v_{\omega_j}'}}=\Big(\sigma({\mathbf v_{\omega_j}'}^T \mathbf h)-t_j\Big)\mathbf h \tag{58}$$<br> 因此输出向量的更新公式为：</p><script type="math/tex; mode=display">{\mathbf v_{\omega_j}'}^{(new)}={\mathbf v_{\omega_j}'}^{(old)}-\eta\Big(\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-t_j\Big)\mathbf h\tag{59}</script><p> negative sampling的关键就是公式（59）的更新过程只应用于词汇表的子集${\omega<em>j|\omega_j\in {\omega_O}\bigcup W</em>{neg}}$,而并非应用于整个词汇表。<br>以上更新公式（59）的直观理解与公式（11）类似。公式（59）对两种应用模型CBOW和SG都适用。对于SG模型，我们每次更新一个上下文单词。<br>接着利用反向传播机制，计算E关于隐藏层输出$h$的偏导数：<br> $$\begin{align}&\frac{\partial E}{\partial \mathbf h}=\sum_{\omega_j \in\{\omega_O\}\bigcup W_{neg}}\frac{\partial E}{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}\cdot \frac{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}{\partial \mathbf h}\tag{60}\\&=\sum_{\omega_j \in\{\omega_O\}\bigcup W_{neg}}\Big(\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-t_j\Big)\mathbf v_{\omega_j}':=EH\tag{61}\end{align}$$<br>将$EH$代入公式（23），我们就可以得到CBOW模型关于输入向量的更新公式；对于SG模型，我们需要计算出每个上下文单词的$EH$值，将$EH$值的和代入公式（35）就能够得到其输入向量的更新公式。</p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>一开始一直不懂word2vec最后得到的应该是输入向量$v<em>ω$还是输出向量和$v′</em>ω$，我在<a href="https://discuss.gluon.ai/t/topic/4180/3" target="_blank" rel="noopener">这个课程</a>中找到了答案，应该是输入向量$v_ω$。课程中还给出了一个例子：<br>以skipgram为例，考虑window_size=1，给定序列abcd。<br>我们需要最大化：$P(b|a)P(a|b)P(c|b)P(b|c)P(d|c)P(c|d)$<br>你会发现上面有三对相互生成。例如下面这对</p><script type="math/tex; mode=display">P(b|a) P(a|b) =  \frac{ \exp(\mathbf{u}_b^\top \mathbf{v}_a)}{ \sum_i \exp(\mathbf{u}_i^\top \mathbf{v}_a)} \frac{ \exp(\mathbf{u}_a^\top \mathbf{v}_b)}{ \sum_i \exp(\mathbf{u}_i^\top \mathbf{v}_b)}</script><p>$u$和$v$在分子等价（$uv$互换不影响全概率的分子大小），但分母上稍有差别。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文接着word2vec的那篇概述再推导一下word2vec中的公式，也是《word2vec Parameter Learning Explained》论文学习笔记，有一些细节的推导我写在了论文处。&lt;br&gt;这篇论文详细地推导和解释了word2vec模型的参数更新公式，包括：&lt;strong&gt;CBOW&lt;/strong&gt;（continuous bag-of-word）模型和&lt;strong&gt;SG&lt;/strong&gt;（skip-gram）模型，以及两种参数优化技术：&lt;strong&gt;hierarchical softmax&lt;/strong&gt; 和 &lt;strong&gt;negative sampling&lt;/strong&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>cw2vec</title>
    <link href="http://yoursite.com/2018/07/09/cw2vec/"/>
    <id>http://yoursite.com/2018/07/09/cw2vec/</id>
    <published>2018-07-09T02:03:28.000Z</published>
    <updated>2018-07-15T06:29:28.786Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information中的核心思想。<br><a id="more"></a></p><h2 id="导语："><a href="#导语：" class="headerlink" title="导语："></a>导语：</h2><p><strong>词向量</strong>算法是自然语言处理领域的基础算法，在序列标注、问答系统和机器翻译等诸多任务中都发挥了重要作用。词向量算法最早由谷歌在2013年提出的 <strong>word2vec</strong>，在接下来的几年里，该算法也经历不断的改进，但大多是仅适用于拉丁字符构成的单词（比如英文、西班牙语、德语等），结合中文语言特性的词向量研究相对较少。<br>中文经过几千年的发展和演变，是一种<strong>强表意</strong>文字，对于我们而言，即使某个字不认识，都或许可以猜到其含义，机器却很难理解这些。比如，“蘒”这个字我们很可能不认识，但里面有“艹”字头，和“禾”木旁，那它也许就是长得像该字右下角部分的某种植物吧。我们可以总结出中文词语一般包含很少的中文字符（我理解为<strong>笔画</strong>），但是中文字符内部包含了很强的语义信息。<br>本文介绍了蚂蚁金服人工智能部与新加坡科技大学一项最新的合作成果：cw2vec——基于汉字笔画信息的中文词向量算法研究，用科学的方法揭示隐藏在一笔一划之间的秘密。此论文在第32届AAAI大会上被高分录用（其中一位审稿人给出了满分，剩下两位也给出了接近满分的评价）。</p><h2 id="相关工作："><a href="#相关工作：" class="headerlink" title="相关工作："></a>相关工作：</h2><p>早在1954年，语言学家Harris提出“Distributional Hypothesis[‘1’] <strong>（分布式假设</strong>）”：语义相似的单词往往会出现在相似的上下文中。这一假设奠定了后续各种词向量的语言学基础，即用数学模型去刻画单词和其上下文的语义相似度。<br>Bengio et al., 2003 [‘2’] 提出了NNLM（基于神经网络的语言模型），由于每次softmax的计算量很大（分母项的计算时间复杂度$O(V)$，$V$是全词表），相继出现了很多快速近似计算策略。<br>为了解决上述问题，谷歌提出了word2vec [‘3,4’] 算法，其中包含了两种策略，一种叫做Negative Sampling（<strong>负采样</strong>），另一种是hierarchical softmax（<strong>层次softmax</strong>）。Negative Sampling的核心思想：每次softmax计算所有单词太慢，那就随机的选几个算一算好了，当然，训练语料中出现次数越多的单词，也就越容易被选中；而Hierarchical Softmax，简单来说，就是建一棵树状的结构，每次自上而下的从根计算到叶子节点，那么就只有对数时间复杂度了！如何构建这棵树可以使得让树的高度尽量小呢？哈夫曼树。<br>词向量模型的核心是构造单词与其上下文的相似度函数，word2vec工具包里面有两种实现方式，分别是<strong>skipgram</strong>（由当前词预测上下文词）和<strong>cbow</strong>（由上下文词预测当前词）。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec1.jpg" alt="此处输入图片的描述"><br>假设当前单词w是“cat”，而上下文单词c是“sat”，算法的目标是给定w最大化c出现概率(skipgram)。在这个算法中，每个单词都被当作一个整体，利用外部的上下文结构信息去学习得到词向量。<br>那么是否可以充分结合单词内部结构的（<strong>亚词</strong>）信息，将其拆分成更细粒度的结构去增强词向量？英文中每个单词所包含的character（字母）较多，每个字母并没有实际的语义表达能力。对于中文词语而言，中文词语可以拆解成character（汉字）。<br>Chen et al., 2015 [‘5’] 提出了CWE模型，思路是把一个中文词语拆分成若干汉字，然后把原词语的向量表示和其中的每一个汉字的向量表示做平均，然后作为新的词语向量。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec2.jpg" alt="此处输入图片的描述"><br>在该算法中，“智能”是一个上下文词语，先拆解成两个汉字“智”和“能”，然后计算出新的词语向量表示；同理，上下文词语“到来”也得到重新计算。CWE保持当前词语不拆分，这里“时代”保持不变。<br>不难想到，将汉字拆分成<strong>偏旁</strong>或许是一种不错的方式，Sun et al., 2014 [‘6’]和Li et al., 2015 [‘7’] 做过相关的研究。<br>然而偏旁只是汉字的一部分，Yu et al., 2017 [‘8’] 提出了更加细化的拆分（JWE），根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起进行联合学习：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec3.jpg" alt="此处输入图片的描述"><br>其中，$w$,$c$和$s$分别表示词语、汉字和字件模块。字件粒度的拆分也取得了超过仅仅利用偏旁信息的方法。<br>此外，Su and Lee, 2017 [‘9’] 提出了GWE模型，尝试从汉字的图片中利用<strong>卷积</strong>自动编码器来提取特征：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec4.jpg" alt="此处输入图片的描述"><br>从汉字图片提取出特征之后，再结合上下文结构信息学习中文词向量。很遗憾的是，根据其原文的描述，这种方式得到的特征基本没有提升，不过这确实是非常有意思的一次试探。</p><h2 id="问题场景："><a href="#问题场景：" class="headerlink" title="问题场景："></a>问题场景：</h2><p>在中文词向量场景下，将中文词语拆解到汉字粒度，虽然会一定程度上提高中文词向量的质量，但是否仍然存在汉字粒度不能刻画的情况？<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec5.jpg" alt="此处输入图片的描述"><br>可以看出，“木材”和“森林”是两个语义很相关的词语，但是当我们拆解到汉字粒度的时候，“木”和“材”这两个字对比“森”和“材”没有一个是相同的（一般会用一个下标去存储一个词语或汉字），因此对于这个例子而言，汉字粒度拆解是不够的。我们所希望得到的是：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec6.jpg" alt="此处输入图片的描述"><br>“木”和“材”可以分别拆解出“木”和“木”（来源于“材”的左半边）结构，而“森”和“林”分别拆解得到多个“木”的相同结构。此外，可以进一步将汉字拆解成偏旁、字件，对于以上例子可以有效提取出语义结构信息，不过我们也分析到：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec7.jpg" alt="此处输入图片的描述"><br>可以看出，“智”的偏旁恰好是“日”，而“日”不能表达出“智”的语义信息。实际上，偏旁的设计是为了方便在字典中查询汉字，因此结构简单、出现频率高变成了首要原则，并不一定恰好能够表达出该汉字的语义信息。此外，将“智”拆分到字件粒度，将会得到“失”，“口”和“日”三个，很不巧的是，这三个字件也均不能表达其汉字语义。我们需要设计出一种新的方法，来重新定义出词语（或汉字）具有语义的结构：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec8.jpg" alt="此处输入图片的描述"><br>这里，“知”是可以表达出“智”语义的模块，如何得到这样的亚词结构，并结合句子上下文设计模型的优化目标，生成出更好的中文词向量，将是后文要探索的内容。</p><h2 id="模型设计："><a href="#模型设计：" class="headerlink" title="模型设计："></a>模型设计：</h2><pre><code>此论文提出了“n元笔画”的概念。所谓“n元笔画”，即就是中文词语（或汉字）连续的n个笔画构成的语义结构。</code></pre><p><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec9.jpg" alt="此处输入图片的描述"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec10.jpg" alt="此处输入图片的描述"><br>如上图，n元笔画的生成共有四个步骤。比如说，“大人”这个词语，可以拆开为两个汉字“大”和“人”，然后将这两个汉字拆分成笔画，再将笔画映射到数字编号，进而利用窗口滑动产生n元笔画。其中，n是一个范围，在上述例子中，我们将n取值为3, 4和5。<br>    在论文中提出了一种基于n元笔画的新型的损失函数：</p><script type="math/tex; mode=display">\mathbb{L}=\sum_{w\in D}{\sum_{c\in T\left( w \right)}{\log \sigma \left( sim\left( w,c \right) \right) +\lambda \mathbb{E}_{c'~P}\left[ \log \sigma \left( -sim\left( w,c \right) \right) \right]}}</script><p>其中，$w$和$c$分别为当前词语和上下文词语，$ \sigma  $ 是sigmoid函数， $ T\left( w \right)  $ 是当前词语划窗内的所有词语集合， $D$是训练语料的全部文本。为了避免传统softmax带来的巨大计算量，我们也采用了负采样的方式。 $ c’ $ 为随机选取的词语，称为“负样例”， $ \lambda  $ 是负样例的个数，而$  \mathbb{E}_{c’~P\left( D \right)}\left[ \cdot \right]  $ 则表示负样例 按照词频分布进行的采样，其中语料中出现次数越多的词语越容易被采样到。相似性函数$ sim\left( \cdot ,\cdot \right)  $  被按照如下构造：</p><script type="math/tex; mode=display">sim\left( w,c \right) =\sum_{q\in S\left( w \right)}{\vec{q}\cdot \vec{c}}</script><p>其中，$ \vec{q} $ 为当前词语对应的一个n元笔画向量，而 是其对应的上下文词语的词向量。我们将当前词语拆解为其对应的n元笔画，但保留每一个上下文词语不进行拆解。$ S\left( w \right)  $ 为词语$w$ 所对应的n元笔画的集合。在算法执行前，我们先扫描每一个词语，生成n元笔画集合，针对每一个n元笔画，都有对应的一个n元笔画向量，在算法开始之前做随机初始化，其向量维度和词向量的维度相同。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec11.jpg" alt="此处输入图片的描述"><br>如上图所示，对于“治理 雾霾 刻不容缓”这句话，假设此刻当前词语恰好是“雾霾”，上下文词语是“治理”和“刻不容缓”。首先我们将当前词语“雾霾”拆解成n元笔画并映射成数字编码，然后划窗得到所有的n元笔画，根据我们设计的损失函数，计算每一个n元笔画和上下文词语的相似度，进而根据损失函数求梯度并对上下文词向量和n元笔画向量进行更新。</p><h2 id="实验分析："><a href="#实验分析：" class="headerlink" title="实验分析："></a>实验分析：</h2><p>为了验证cw2vec算法的效果，我们在公开数据集上，与业界最优的几个词向量算法做了对比:<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec12.jpg" alt="此处输入图片的描述"><br>上图中包括2013年谷歌提出的word2vec [‘2,3’] 的两个模型skipgram和cbow，2014年斯坦福提出的GloVe算法 <a href="http://p5vuwy2ht.bkt.clouddn.com/cw2vec10.jpg" target="_blank" rel="noopener">10</a>，2015年清华大学提出的基于汉字的CWE模型 <a href="http://p5vuwy2ht.bkt.clouddn.com/cw2vec5.jpg" target="_blank" rel="noopener">5</a>，以及2017年最新发表的基于像素和字件的中文词向量算法 [‘8,9’]，可以看出cw2vec在word similarity，word analogy，以及文本分类和命名实体识别的任务中均取得了一致性的提升。同时，我们也展示了不同词向量维度下的实验效果：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec13.jpg" alt="此处输入图片的描述"><br>上图为不同维度下在word analogy测试集上的实验结果，左侧为3cosadd，右侧为3cosmul的测试方法。可以看出我们的算法在不同维度的设置下均取得了不错的效果。此外，我们也在小规模语料上进行了测试：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec14.jpg" alt="此处输入图片的描述"><br>上图是仅选取20%中文维基百科训练语料，在word similarity下测试的结果，skipgram, cbow和GloVe算法由于没有利用中文的特性信息进行加强，所以在小语料上表现较差，而其余四个算法取得了不错的效果，其中我们的算法在两个数据集上均取得的了最优效果。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec15.jpg" alt="此处输入图片的描述"><br>为了更好的探究不同算法的实际效果，我们专门选取了两个词语做案例分析。第一个是环境相关的“水污染”，然后根据词向量利用向量夹角余弦找到与其语义最接近的词语。GWE找到了一些和“污”字相关的词语，比如“污泥”，“污渍”和“污垢”，而JWE则更加强调后两个字“污染”GloVe找到了一些奇怪的相近词语，比如“循环系统”，“神经系统”。CWE找到的相近词语均包含“水”和“污”这两个字，我们猜测是由于其利用汉字信息直接进行词向量加强的原因。此外，只有cw2vec找到了“水质”这个相关词语，我们认为是由于n元笔画和上下文信息对词向量共同作用的结果。第二个例子，我们特别选择了“孙悟空”这个词语，该角色出现在中国的名著《西游记》和知名日本动漫《七龙珠》中，cw2vec找到的均为相关的角色或著作名称。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><ol><li>Harris, Zellig S. “Distributional structure.” Word 1954.</li><li>Bengio, Yoshua, et al. “A neural probabilistic language model.” JMLR 2003.</li><li>Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 (2013).</li><li>Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” NIPS 2013.</li><li>Chen, Xinxiong, et al. “Joint Learning of Character and Word Embeddings.” IJCAI 2015.</li><li>Sun, Yaming, et al. “Radical-enhanced Chinese character embedding.” ICNIP 2014.</li><li>Li, Yanran, et al. “Component-enhanced Chinese character embeddings.” arXiv preprint arXiv:1508.06669 (2015).</li><li>Yu, Jinxing, et al. “Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components.” EMNLP 2017.</li><li>Su, Tzu-Ray, and Hung-Yi Lee. “Learning Chinese Word Representations From Glyphs Of Characters.” EMNLP 2017.</li><li>Pennington, Jeffrey, et al. “Glove: Global vectors for word representation.” EMNLP 2014.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读</title>
    <link href="http://yoursite.com/2018/07/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2018/07/09/论文阅读/</id>
    <published>2018-07-09T01:58:47.000Z</published>
    <updated>2018-07-10T08:47:03.887Z</updated>
    
    <content type="html"><![CDATA[<p>整理了一下读过的论文，会继续更新。<br><a id="more"></a></p><h2 id="词向量："><a href="#词向量：" class="headerlink" title="词向量："></a>词向量：</h2><h3 id="已读："><a href="#已读：" class="headerlink" title="已读："></a>已读：</h3><p>cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information<br><a href="https://zkq1314.github.io/2018/07/09/cw2vec/" target="_blank" rel="noopener">cw2vec</a><br>word2vec：Distributed Representations of Words and Phrases and their Compositionality<br>word2vec：Efficient Estimation of Word Representations in Vector Space<br>word2vec Parameter Learning Explained<br><a href="https://zkq1314.github.io/2018/05/03/word2vec/#more" target="_blank" rel="noopener">word2vec</a></p><h3 id="未读："><a href="#未读：" class="headerlink" title="未读："></a>未读：</h3><p>NNLM：A Neural Probabilistic Language Model<br>Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components<br>Learning Chinese Word Representations From Glyphs Of Characters</p><h2 id="文本分类："><a href="#文本分类：" class="headerlink" title="文本分类："></a>文本分类：</h2><h3 id="已读：-1"><a href="#已读：-1" class="headerlink" title="已读："></a>已读：</h3><p>TextCNN：Convolutional neural networks for sentence classification<br>Char-CNN：Character-level convolutional networks for text classification<br>Text-RNN：Recurrent neural network for text classification with multi-task learning<br>RCNN：Recurrent Convolutional Neural Networks for Text Classification</p><h3 id="未读：-1"><a href="#未读：-1" class="headerlink" title="未读："></a>未读：</h3><p>fasttext：Bag of Tricks for Efficient Text Classification<br>TextRNN+Attention：Hierarchical Attention Networks for Document Classification</p><h2 id="机器翻译："><a href="#机器翻译：" class="headerlink" title="机器翻译："></a>机器翻译：</h2><h3 id="未读：-2"><a href="#未读：-2" class="headerlink" title="未读："></a>未读：</h3><p>Attention：NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</p><h2 id="关系抽取："><a href="#关系抽取：" class="headerlink" title="关系抽取："></a>关系抽取：</h2><h3 id="已读：-2"><a href="#已读：-2" class="headerlink" title="已读："></a>已读：</h3><p>Large Scaled Relation Extraction with Reinforcement Learning</p><h2 id="图像分类："><a href="#图像分类：" class="headerlink" title="图像分类："></a>图像分类：</h2><h3 id="已读：-3"><a href="#已读：-3" class="headerlink" title="已读："></a>已读：</h3><p>AlexNet：ImageNet Classification with Deep Convolutional Neural Networks<br>GoogLeNet：Going deeper with convolutions<br>ResNet：Deep Residual Learning for Image Recognition<br>VGG：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整理了一下读过的论文，会继续更新。&lt;br&gt;
    
    </summary>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://yoursite.com/2018/07/04/MWeb%20init%20Library/docs/15294769775635/"/>
    <id>http://yoursite.com/2018/07/04/MWeb init Library/docs/15294769775635/</id>
    <published>2018-07-04T04:13:24.000Z</published>
    <updated>2018-07-04T04:13:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="欢迎使用-MWeb"><a href="#欢迎使用-MWeb" class="headerlink" title="欢迎使用 MWeb"></a>欢迎使用 MWeb</h1><p>MWeb 是专业的 Markdown 写作、记笔记、静态博客生成软件，目前已支持 Mac，iPad 和 iPhone。MWeb 有以下特色：</p><h2 id="软件本身："><a href="#软件本身：" class="headerlink" title="软件本身："></a>软件本身：</h2><ul><li>使用原生的 macOS 技术打造，追求与系统的完美结合。</li><li>原则上，首先是追求界面简洁和高性能，然后才是强大易用，功能全面。</li></ul><h2 id="Markdown-语法："><a href="#Markdown-语法：" class="headerlink" title="Markdown 语法："></a>Markdown 语法：</h2><ul><li>使用 Github Flavored Markdown 语法，简称 GFM 语法。</li><li>支持表格、TOC、LaTeX、代码块、任务列表、脚注等。</li><li>画图库支持 mermaid, viz, echarts, plantuml, sequence, flow。</li></ul><h2 id="Markdown-辅助："><a href="#Markdown-辅助：" class="headerlink" title="Markdown 辅助："></a>Markdown 辅助：</h2><ul><li>支持截图并粘贴、复制并粘贴、拖拽等方式插入图片并直接显示在编辑器内。</li><li>在兼容 Markdown 语法的情况下支持设置图片宽度。</li><li>好用的表格插入和 LaTeX 书写辅助。</li></ul><h2 id="Markdown-输出："><a href="#Markdown-输出：" class="headerlink" title="Markdown 输出："></a>Markdown 输出：</h2><ul><li>支持导出为图片、HTML、Epub、PDF、RTF、Docx。</li><li>支持发布到 Wordrpess、支持 Metaweblog API 协议的服务、Wordpress.com、印象笔记（Evernote）、Blogger、Medium、Tumblr。</li><li>图片上传服务（图床）支持 Google Photos、Imgur、七牛云、又拍云和自定义的图床服务。</li></ul><h2 id="Markdown-笔记："><a href="#Markdown-笔记：" class="headerlink" title="Markdown 笔记："></a>Markdown 笔记：</h2><ul><li>强大的文档库支持分类树和标签管理文档，文档可归类于多个分类，可以把分类整个导出为 Epub、PDF 和生成静态网站。非常合适用于笔记、个人知识收集、管理和输出。</li><li>快速笔记：随意增加笔记及图片等素材，支持以天为单位把增加的素材组合在一个文档内，方便整理及记录历史收集情况。</li><li>快速搜索：目前已支持全局快捷键调出搜索。</li></ul><h2 id="外部-Markdown-文档："><a href="#外部-Markdown-文档：" class="headerlink" title="外部 Markdown 文档："></a>外部 Markdown 文档：</h2><ul><li>外部 Markdown 文档使用外部模式管理。外部模式使用目录树的方式编辑和管理你的 markdown 文档，还能很好的支持 gitbook、JekyII、hexo 等等编辑和图片插入。</li></ul><h2 id="MWeb-官网："><a href="#MWeb-官网：" class="headerlink" title="MWeb 官网："></a>MWeb 官网：</h2><p>如果要更详细了解 MWeb，建议你一定要去 <a href="https://zh.mweb.im/" target="_blank" rel="noopener">MWeb 官网首页</a> 看一下介绍视频，MWeb 官网也做了比较详细的帮助，建议你也看一下大概内容，帮助的网址为：<a href="https://zh.mweb.im/help.html" target="_blank" rel="noopener">https://zh.mweb.im/help.html</a>。</p><h2 id="帮助我们改进-MWeb"><a href="#帮助我们改进-MWeb" class="headerlink" title="帮助我们改进 MWeb"></a>帮助我们改进 MWeb</h2><p>如果你喜欢 MWeb，想让它变得更好，你可以：</p><ol><li>推荐 MWeb，让更多的人知道。</li><li>给我们发反馈和建议：<a href="&#109;&#x61;&#105;&#108;&#x74;&#x6f;&#58;&#x63;&#x6f;&#x64;&#x65;&#x72;&#x66;&#x6f;&#x72;&#x61;&#114;&#116;&#43;&#50;&#51;&#x33;&#x33;&#x40;&#x67;&#109;&#x61;&#105;&#108;&#46;&#x63;&#111;&#109;">&#x63;&#x6f;&#x64;&#x65;&#x72;&#x66;&#x6f;&#x72;&#x61;&#114;&#116;&#43;&#50;&#51;&#x33;&#x33;&#x40;&#x67;&#109;&#x61;&#105;&#108;&#46;&#x63;&#111;&#109;</a></li><li>在 Mac App Store 上评价 （如果是在 MAS 上购买的话）。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;欢迎使用-MWeb&quot;&gt;&lt;a href=&quot;#欢迎使用-MWeb&quot; class=&quot;headerlink&quot; title=&quot;欢迎使用 MWeb&quot;&gt;&lt;/a&gt;欢迎使用 MWeb&lt;/h1&gt;&lt;p&gt;MWeb 是专业的 Markdown 写作、记笔记、静态博客生成软件，目前已支持 M
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>BP推导</title>
    <link href="http://yoursite.com/2018/06/07/BP%E6%8E%A8%E5%AF%BC/"/>
    <id>http://yoursite.com/2018/06/07/BP推导/</id>
    <published>2018-06-07T01:32:39.000Z</published>
    <updated>2018-06-07T01:37:07.025Z</updated>
    
    <content type="html"><![CDATA[<p>整理一下BP推导过程，以备查验。<br><a id="more"></a><br><img src="http://p5vuwy2ht.bkt.clouddn.com/bp%E6%8E%A8%E5%AF%BC.png" alt="BP推导"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整理一下BP推导过程，以备查验。&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="BP" scheme="http://yoursite.com/tags/BP/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
