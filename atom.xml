<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-12-19T11:50:49.294Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>KaiQiang Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Boosting学习笔记</title>
    <link href="http://yoursite.com/2018/12/19/Boosting%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/12/19/Boosting学习笔记/</id>
    <published>2018-12-19T11:49:02.000Z</published>
    <updated>2018-12-19T11:50:49.294Z</updated>
    
    <content type="html"><![CDATA[<p>本文为学习boosting时整理的笔记，全文主要包括以下几个部分：<br>对集成学习进行了简要的说明<br>给出了一个Adboost的具体实例<br>对Adboost的原理与学习过程进行了推导<br>针对GBDT的学习过程进行了简要介绍<br>针对Xgboost的损失函数进行了简要介绍<br><a id="more"></a><br><img src="/2018/12/19/Boosting学习笔记/Boosting学习笔记.jpg" alt="Boosting1"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文为学习boosting时整理的笔记，全文主要包括以下几个部分：&lt;br&gt;对集成学习进行了简要的说明&lt;br&gt;给出了一个Adboost的具体实例&lt;br&gt;对Adboost的原理与学习过程进行了推导&lt;br&gt;针对GBDT的学习过程进行了简要介绍&lt;br&gt;针对Xgboost的损失函数进行了简要介绍&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title> 从决策树到GBDT</title>
    <link href="http://yoursite.com/2018/12/12/%E4%BB%8E%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%B0GBDT/"/>
    <id>http://yoursite.com/2018/12/12/从决策树到GBDT/</id>
    <published>2018-12-12T08:12:39.000Z</published>
    <updated>2018-12-12T08:57:52.727Z</updated>
    
    <content type="html"><![CDATA[<p>梯度提升决策树GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。<br>  GBDT中的树是<strong>回归树</strong>（不是分类树），GBDT用来做回归预测，调整后也可以用于分类。<br><a id="more"></a></p><h1 id="回归树Regression-Decision-Tree"><a href="#回归树Regression-Decision-Tree" class="headerlink" title="回归树Regression Decision Tree"></a>回归树Regression Decision Tree</h1><p>回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是<strong>最小化平方误差</strong>。也就是被预测出错的人数越多，错的越离谱，平方误差就越大，通过最小化平方误差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。<br><img src="/2018/12/12/从决策树到GBDT/GBDT1.png" alt="GBDT1"><br>回归树算法如下图（截图来自《统计学习方法》5.5.1 CART生成）：<br><img src="/2018/12/12/从决策树到GBDT/GBDT2.png" alt="GBDT2"></p><h1 id="提升树Boosting-Decision-Tree"><a href="#提升树Boosting-Decision-Tree" class="headerlink" title="提升树Boosting Decision Tree"></a>提升树Boosting Decision Tree</h1><p>提升树是迭代多棵回归树来共同决策。当采用<strong>平方误差损失函数</strong>时，每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树，残差的意义如公式：<strong>残差 = 真实值 - 预测值</strong> 。提升树即是整个迭代过程生成的回归树的累加。<br>  举个例子，较直观地展现出多棵决策树线性求和过程以及残差的意义。<br>  训练一个提升树模型来预测年龄：<br>  训练集是4个人，A，B，C，D年龄分别是14，16，24，26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下：<br><img src="/2018/12/12/从决策树到GBDT/GBDT3.png" alt="GBDT3"><br>该例子很直观的能看到，预测值等于所有树值得累加，如A的预测值 = 树1左节点 值 15 + 树2左节点 -1 = 14。<br>  因此，给定当前模型 $f_{m-1}(x)$，只需要简单的拟合当前模型的残差。现将回归问题的提升树算法叙述如下<br><img src="/2018/12/12/从决策树到GBDT/GBDT4.png" alt="GBDT4"></p><h1 id="梯度提升决策树Gradient-Boosting-Decision-Tree"><a href="#梯度提升决策树Gradient-Boosting-Decision-Tree" class="headerlink" title="梯度提升决策树Gradient Boosting Decision Tree"></a>梯度提升决策树Gradient Boosting Decision Tree</h1><p>提升树利用加法模型和前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。<br><img src="/2018/12/12/从决策树到GBDT/GBDT5.png" alt="GBDT5"></p><p>但对于一般的损失函数，往往每一步优化没那么容易，如上图中的绝对值损失函数和Huber损失函数。针对这一问题，Freidman提出了梯度提升算法：利用最速下降的近似方法，即<strong>利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值</strong>，拟合一个回归树。</p><p>算法步骤解释：<br>参考：<a href="https://www.cnblogs.com/pinard/p/6140514.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6140514.html</a></p><p>1、初始化，估计使损失函数极小化的常数值，它是只有一个根节点的树。<br>2、<br>（a）计算损失函数的负梯度在当前模型的值，将它作为残差的估计，第t轮的第i个样本的损失函数的负梯度表示为$r<em>{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]</em>{f(x) = f<em>{t-1}\;\; (x)}$<br>（b）利用$(x_i,r</em>{ti})\;\; (i=1,2,..m)$,我们可以拟合一颗CART回归树，得到了第t颗回归树，其对应的叶节点区域$R<em>{tj}, j =1,2,…, J$。其中J为叶子节点的个数。<br>（c）利用线性搜索估计叶节点区域的值，使损失函数极小化$$c</em>{tj} = \underbrace{arg\; min}<em>{c}\sum\limits</em>{x<em>i \in R</em>{tj}} L(y<em>i,f</em>{t-1}(x<em>i) +c)<script type="math/tex">（d）更新回归树</script>h_t(x) = \sum\limits</em>{j=1}^{J}c<em>{tj}I(x \in R</em>{tj})$$</p><script type="math/tex; mode=display">f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})</script><p>3、得到输出的最终模型 f(x)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度提升决策树GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，该算法由多棵决策树组成，所有树的结论累加起来做最终答案。它在被提出之初就和SVM一起被认为是泛化能力较强的算法。&lt;br&gt;  GBDT中的树是&lt;strong&gt;回归树&lt;/strong&gt;（不是分类树），GBDT用来做回归预测，调整后也可以用于分类。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>stacking原理</title>
    <link href="http://yoursite.com/2018/12/12/stacking%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2018/12/12/stacking原理/</id>
    <published>2018-12-12T06:43:58.000Z</published>
    <updated>2018-12-12T08:13:20.779Z</updated>
    
    <content type="html"><![CDATA[<p>先由所有基模型对训练集进行预测，使用交叉验证或者留一法生成新的训练集，再由次级学习器（多使用线性回归）训练。<br><a id="more"></a><br>一个例子：<br><img src="/2018/12/12/stacking原理/stacking.png" alt="stacking 例子"><br>XGB模型，把train分train1~train5,共5份，用其中4份预测剩下的那份,同时预测test数据，这样的过程做5次,生成5份train（原train样本数/5）数据和5份test数据。然后把5份预测的train数据纵向叠起来，把test预测的结果取平均。</p><p>RF模型和XGB模型一样，再来一次。这样就生成了2份train数据和2份test数据（XGB重新表达的数据和RF重新表达的数据），将它们并列合并。然后用LR模型，进一步做融合，得到最终的预测结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;先由所有基模型对训练集进行预测，使用交叉验证或者留一法生成新的训练集，再由次级学习器（多使用线性回归）训练。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>特征离散,特征交叉,连续特征离散化(以广告学为例)</title>
    <link href="http://yoursite.com/2018/12/10/%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3,%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89,%E8%BF%9E%E7%BB%AD%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3%E5%8C%96(%E4%BB%A5%E5%B9%BF%E5%91%8A%E5%AD%A6%E4%B8%BA%E4%BE%8B)/"/>
    <id>http://yoursite.com/2018/12/10/特征离散,特征交叉,连续特征离散化(以广告学为例)/</id>
    <published>2018-12-10T07:37:37.000Z</published>
    <updated>2018-12-12T11:11:59.419Z</updated>
    
    <content type="html"><![CDATA[<p>博文<a href="https://blog.csdn.net/mytestmy/article/details/18987881" target="_blank" rel="noopener">《互联网广告综述之点击率系统》</a>论述了互联网广告的点击率系统，可以看到，其中的logistic regression模型是比较简单而且实用的，其训练方法虽然有多种，但目标是一致的，训练结果对效果的影响是比较大，但是训练方法本身，对效果的影响却不是决定性的，因为训练的是每个特征的权重，权重细微的差别不会引起ctr的巨大变化。<br>在训练方法确定后，<strong>对ctr预估起到决定性作用的是选用的特征</strong>。<br><a id="more"></a></p><h2 id="1-1特征选择与使用"><a href="#1-1特征选择与使用" class="headerlink" title="1.1特征选择与使用"></a>1.1特征选择与使用</h2><p>做点击率预估需要两方面的数据，一方面是<strong>广告的数据</strong>，另一方面是<strong>用户的数据</strong>，现在所有的数据都有，那么工作就是利用这两方面的数据评估用户点击这个广告的可能性（也就是概率）。<br>用户的特征是比较多的，用户的年龄，性别，地域，职业，学校，手机平台等等。广告的特征也很丰富，如广告大小，广告文本，广告所属行业，广告图片。还有反馈特征，如每个广告的实时ctr，广告跟性别交叉的ctr。如何从这么多的特征中选择到能刻画一个人对一个广告的兴趣的特征，是数据挖掘工程师的一个大难题。<br>选中了特征，还需要注意特征的选择方式，例如，如果单独把年龄作为一个特征，最终能训练出来啥吗？因为年龄相加相减是没有意义的，所以只能把每个年龄做为一个特征，但是光这样可以了吗？怎么用特征，是广告算法工程师的一个大课题。</p><h2 id="1-1-1-选择特征"><a href="#1-1-1-选择特征" class="headerlink" title="1.1.1 选择特征"></a>1.1.1 选择特征</h2><p>什么样的特征适合用来预估ctr？这个问题是很多广告算法工程师的需要考虑的。<br>机器学习算法最多会大谈模型，对于特征的讨论很少涉及。真正的应用中，多数数据挖掘工程师的工作都是在想特征，验证特征。<br>想特征是一个脑力加体力的活，需要不少的领域的知识，更让人郁闷的是，工业界并没有一整套想特征的办法，工业界有的只是验证特征的办法。对于互联网广告业，就简单说说通用特征怎么来的吧。<br>首先说<strong>年龄</strong>这个特征，怎么知道它跟点击率有关系？现在直观的解释是，年轻人普遍喜欢运动类的广告，30岁左右的男人喜欢车，房子之类的广告，50岁以上的人喜欢保健品的广告。可以看到，选择年龄作为特征的理由是基于对各个年龄段的人喜欢的不同类型的东西的一个粗略的划分，是一个很主观的东西。<br>再说<strong>性别</strong>这个特征，直观的感觉是，男性普遍喜欢体育类的，车类的，旅游类广告，女性普遍喜欢化妆品，服装类的广告。这也可以看到，选择性别作为特征也是基于相似的理由，就是认为男性和女性大体会喜欢不同的东西。<br>对于<strong>地域</strong>这个特征，这下就学问多了，华南的人在比较喜欢动漫和游戏，华北的人喜欢酒品和烟？<br>在广告方面的特征，广告的图片大小，广告前景色背景色真的能影响人的点击吗？这其实都是一种猜测。图片里面是一个明星还是一个动物之类的因素也可以考虑。</p><p>总结：选特征的流程，就是先猜想，然后统计验证，然后将特征加到模型中，进行验证。<br>总之，想特征的这个事情基本没多大谱，只能天南地北地想象，还要多了解各行各业的知识，以便想到更多的特征，哪怕某个特征跟人关系并不大，也得好好验证一番。<br>想到了特征，就要验证和进行判断。<br>验证特征的办法多，有<strong>直接观察ctr</strong>，<strong>卡方检验</strong>，<strong>单特征AUC</strong>等。直接观察ctr是个很有效的方法，如根据投放记录，化妆品的广告在女性上面的点击率就比在男性上面的点击率高很多，说明性别这个特征在化妆品行业是有预测能力的；又如体育用品的广告在男性上面的点击率也比女性高，说明性别这个特征在体育行业也是有预测能力的，经过多个行业的验证，就认为性别这个特征可以用了。<br>年龄这个特征的评估类型，主要是观察一个广告在不同年龄段的点击率是否有区别，再观察不同广告的点击率在不同年龄段的分布是否不一样，如果都有区别，说明年龄这个特征就可以用了。<br>在实际的使用中发现，性别这个特征比较有效，手机平台这个特征也比较有效，地域和年龄这两个特征有一定效果，但没有前两个那么明显，跟他们的使用方式可能有关，还需要进一步挖掘。<br>同时，实际使用中也发现，<strong>广告反馈ctr这个特征也很有效，这个特征的意思就是当前的广告正在投放，已经投放了一部分了，这部分的点击率基本可以认为是这个广告的点击率了，也可以认为是这个广告的质量的一个体现，用来预估一个流量的ctr是很有效的。</strong></p><h2 id="1-1-2-特征的处理和使用"><a href="#1-1-2-特征的处理和使用" class="headerlink" title="1.1.2 特征的处理和使用"></a>1.1.2 特征的处理和使用</h2><p>选择得到特征，怎么用也是一个问题。<br>先说需求，其实预估ctr要做的事情是下面的图的工作——计算一个用户/广告组合的ctr。<br><img src="/2018/12/10/特征离散,特征交叉,连续特征离散化(以广告学为例)/特征离散,特征交叉,连续特征离散化(以广告学为例" alt="特征离散1">/特征离散1.png)</p><p>上面已经选好了特征，暂定有广告的反馈ctr，用户年龄，性别三个特征。</p><p><strong>一、离散化</strong>，有些特征虽然也是数值型的，但是该特征的取值相加相减是没有实际意义的，那么该数值型特征也要看成离散特征，采用离散化的技术。<br>反馈ctr是一个浮点数，直接作为特征是可以的，假设1号特征就是反馈ctr。对应年龄来说就不是这样了，因为年龄不是浮点数，而且年龄的20岁跟30岁这两个数字20,30大小比较是没有意义的，相加相减都是没有意义的，在优化计算以及实际计算ctr是会涉及这两个数字的大小比较的。如w.x，在w已经确定的情况下，x的某个特征的值是20，或者30，w.x的值相差是很大的，哪怕用逻辑化公式再比较，得到的值也是比较大的，但是往往20岁的人跟30岁的人对同一个广告的兴趣差距不会那么大。解决这样的情况的方法就是，每个年龄一个特征，如总共只有20岁到29岁10种年龄，就把每个年龄做一个特征，编号是从2到11（1号是广告的反馈ctr），如果这个人是20岁，那么在编号为2的特征上的值就是1，3到11的编号上就是0。这样，年龄这一类特征就有了10个特征，而且这10个特征就是互斥的，这样的特征称为离散化特征。</p><p><strong>二、交叉</strong>，交叉从理论上而言是为了引入特征之间的交互，也即为了引入非线性。是有实际意义的。本文对交叉的意义解释得非常nice<br>这样看起来就能解决上面的问题了，但是够了吗？<br>比如一个人是20岁，那么在编号为2的特征上面，它一直都是1，对篮球的广告是1，对化妆品的广告也是1，这样训练的结果得到的编号为2的权重的意义是——20岁的人点击所有的广告的可能性的都是这个权重，这样其实是不合理的。<br>有意义的应该是，这个20岁的人，当广告是跟体育相关的时候，它是一个值；当广告跟保健品相关的时候，它又是一个值。这样看起来才合理。如果这个不够深刻，基于跟上面同样的道理，性别这个特征也是一样的，假如也做了上面的离散化操作，编号是12和13,12是男性，13是女性。这样的话，对于一个男性/体育广告组合来说，编号12的特征值为1，男性/化妆品的组合的编号12的特征值也是1。这样也是不合理的。<br>怎么做到合理呢？以上面的性别的例子来说。编号12的特征值不取1，取值为该广告在男性用户上面的点击率，如对于男性/体育广告的组合，编号12的特征的值为男性在体育广告上面点击率，这样，编号为12的特征就变成了一个浮点数，这个浮点数的相加减是有意义的。<br>这样的做法称为特征的交叉，现在就是性别跟广告的交叉得到的特征值。还有很多其他的方式可以进行交叉，目前工业上的应用最多的就是广告跟用户的交叉特征（编号为1的那个特征）、广告跟性别的交叉特征，广告跟年龄的交叉特征，广告跟手机平台的交叉特征，广告跟地域的交叉特征。如果做得比较多，可能会有广告主（每个广告都是一个广告主提交的一个投放计划，一个广告主可能会提交多个投放计划）跟各个特征的交叉。</p><p><strong>三、连续特征变离散特征</strong>：连续特征离散化的基本假设，是默认连续特征不同区间的取值对结果的贡献是不一样的<br>做到的交叉的特征值就足够了吗？答案还是不一定。<br>如编号为1的那个特征，就是广告本身的ctr，假设互联网广告的点击率符合一个长尾分布，叫做对数正态分布，其概率密度是下图（注意这个是假设，不代表真实的数据，从真实的数据观察是符合这么样的一个形状的，好像还有雅虎的平滑的那个论文说它符合beta分布）。<br><img src="/2018/12/10/特征离散,特征交叉,连续特征离散化(以广告学为例)/特征离散,特征交叉,连续特征离散化(以广告学为例" alt="特征离散2">/特征离散2.png)</p><p>可以看到，大部分广告的点击率都是在某一个不大的区间内的，点击率越高的广告越少，同时这些广告覆盖的流量也少。换句话说，点击率在0.2%左右的时候，如果广告a的点击率是0.2%，广告b的点击率是0.25%，广告b的点击率比广告a高0.05%，其实足以表示广告b比广a好不少,因为有足够多的样本支持这个结论；但是点击率在1.0%左右的的时候，广告a点击率是1.0%，广告b的点击率是1.05%，并没有办法表示广告b比广告a好很多，因为在这0.05%的区间内的广告并不多，两个广告基本可以认为差不多的。也就是点击率在不同的区间，应该考虑是不同的权重系数，因为这个由广告点击率组成的编号为1的特征与这个用户对广告的点击的概率不是完全的正相关性，有可能值越大特征越重要，也有可能值增长到了一定程度，重要性就下降了。比如说，在区间[0.2%,0.3%]区间的系数就要比[0.3%,0.4%]的系数大。故，<strong>我们如果将数值型特征进行区间离散化，就是默认不同区间的权重是不一样的</strong>。<br>对于这样的问题，百度有科学家提出了对连续特征进行离散化。他们认为，特征的连续值在不同的区间的重要性是不一样的，所以希望连续特征在不同的区间有不同的权重，实现的方法就是对特征进行划分区间，每个区间为一个新的特征。常用做法，就是先对特征进行排序，然后再按照等频离散化为N个区间<br>具体实现是使用等频离散化方式：1）对于上面的编号为1的那个特征，先统计历史记录中每条展示记录中编号为1的特征的值的排序，假设有10000条展示记录，每个展示记录的这个特征值是一个不相同的浮点数，对所有的展示记录按照这个浮点数从低到高排序，取最低的1000个展示记录的特征值作为一个区间，排名1001到2000的展示记录的特征值作为一个区间，以此类推，总共划分了10个区间。2）对特征编号重新编排，对于排名从1到1000的1000个展示记录，他们的原来编号为1的特征转变为新的特征编号1，值为1；对于排名是从1001到2000的记录，他们的原来编号为1的特征转变为新的特征编号2，值为1，以此类推，新的特征编号就有了1到10总共10个。对于每个展示记录来说，如果是排名1到1000的，新的特征编号就只有编号1的值为1，2到10的为0，其他的展示记录类似，这样，广告本身的ctr就占用了10个特征编号，就成为离散化成了10个特征。<br>等频离散化需要对原有的每个特征都做，也就是原来的编号为1到13的编号，会离散化成很多的编号，如果每个特征离散化成10个，则最终会有130个特征，训练的结果w就会是一个130维的向量，分别对应着130个特征的权重。<br>实际的应用表名，离散化的特征能拟合数据中的非线性关系，取得比原有的连续特征更好的效果，而且在线上应用时，无需做乘法运算，也加快了计算ctr的速度。</p><h2 id="1-1-3-特征的过滤与修正"><a href="#1-1-3-特征的过滤与修正" class="headerlink" title="1.1.3 特征的过滤与修正"></a>1.1.3 特征的过滤与修正</h2><p>上面提到，很多特征其实是反馈的特征，如广告反馈ctr，广告与性别交叉特征，这些特征本来可以通过历史展示日志的统计得到。但有些广告本来展示量很少，在男性用户上展示就更少，这时要计算广告与性别交叉的ctr是很不准确的，需要对这个特征进行修正。具体的修正方法可以参考博文<a href="https://blog.csdn.net/mytestmy/article/details/19088519" target="_blank" rel="noopener">《广告点击率的贝叶斯平滑》</a>。<br>经过修正后的ctr再做特征，实际线上效果有了比较大的提升。<br>如果使用的特征又更多了，有了学校跟广告交叉特征什么的，离散化后有了上万的特征，这下就会产生特征过多导致的各种问题，如过拟合等。解决这个问题的方法一种是离线的数据评估，如用ctr的区分性。另一种就是<strong>利用正则</strong>，特别是<strong>L1正则</strong>，经过L1正则训练的得到的权重向量，其中某些特征如果对点击率预估预测性不强，权重会变成0，不影响预估。这就是特征过滤，具体的有关L1的一些论述与实现参看博文<a href="https://blog.csdn.net/mytestmy/article/details/18983889" target="_blank" rel="noopener">《从广义线性模型到逻辑回归》</a><a href="https://blog.csdn.net/mytestmy/article/details/18969821" target="_blank" rel="noopener">《OWL-QN算法》</a>和<a href="https://blog.csdn.net/mytestmy/article/details/18980163" target="_blank" rel="noopener">《在线学习算法FTRL》</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;博文&lt;a href=&quot;https://blog.csdn.net/mytestmy/article/details/18987881&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《互联网广告综述之点击率系统》&lt;/a&gt;论述了互联网广告的点击率系统，可以看到，其中的logistic regression模型是比较简单而且实用的，其训练方法虽然有多种，但目标是一致的，训练结果对效果的影响是比较大，但是训练方法本身，对效果的影响却不是决定性的，因为训练的是每个特征的权重，权重细微的差别不会引起ctr的巨大变化。&lt;br&gt;在训练方法确定后，&lt;strong&gt;对ctr预估起到决定性作用的是选用的特征&lt;/strong&gt;。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器学习知识点汇总</title>
    <link href="http://yoursite.com/2018/12/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2018/12/07/机器学习知识点汇总/</id>
    <published>2018-12-07T07:37:37.000Z</published>
    <updated>2018-12-23T07:37:32.647Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h1 id="机器学习模型"><a href="#机器学习模型" class="headerlink" title="机器学习模型"></a>机器学习模型</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><h2 id="k-近邻"><a href="#k-近邻" class="headerlink" title="k 近邻"></a>k 近邻</h2><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>sklearn中有3种不同类型的朴素贝叶斯：<br>1.多项式型：用于<strong>离散</strong>值模型里。比如文本分类问题里面，我们不光看词语是否在文本中出现，也得看出现次数。如果总词数为n，出现词数为m的话，有点像掷骰子n次出现m次这个词的场景。<br>2.高斯分布型：当特征是连续变量的时候，假定属性/特征服从正态分布的。<br>3.伯努利型：每个特征的取值只能是1和0(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0)。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><strong>贝叶斯公式</strong>和<strong>特征条件独立假设</strong><br><strong>条件概率</strong>：<br>$P(A|B)$表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。其基本求解公式为：$P(A|B)=\frac{P(AB)}{P(B)}$<br>贝叶斯定理便是基于条件概率，通过$P(A|B)$来求$P(B|A)$：<br>$P(B|A)=\frac{P(A|B)P(B)}{P(A)}$<br>在分类任务中：<br>$P(类别|特征)=\frac{P(特征|类别)P(类别)}{P(特征)}$</p><h3 id="特征条件独立假设的作用"><a href="#特征条件独立假设的作用" class="headerlink" title="特征条件独立假设的作用"></a>特征条件独立假设的作用</h3><p>由贝叶斯公式，分子中的$P(y<em>k)$是先验概率，根据训练集就可以简单地计算出来。<br>而条件概率$P(x|y_k)=P(x_1,x_2,…,x_n|y_k)$，它的参数规模是<strong>指数</strong>数量级别的，假设第i维特征$x_i$可取值的个数有$S_i$个，类别取值个数为k个，那么参数个数为：$$k\prod</em>{i=1}^{n}S<em>{i}<script type="math/tex">针对这个问题，朴素贝叶斯算法对条件概率分布作出了独立性的假设，通俗地讲就是说假设各个维度的特征$x_1,x_2,...,x_n$互相独立，在这个假设的前提上，条件概率可以转化为：</script>P(x|y</em>{k})=P(x<em>{1},x</em>{2},…,x<em>{n}|y</em>{k})=\prod<em>{i=1}^{n}P(x</em>{i}|y<em>{k})<script type="math/tex">这样，参数规模就降到</script>k\sum</em>{i=1}^{n}S_{i}$$</p><h3 id="平滑"><a href="#平滑" class="headerlink" title="平滑"></a>平滑</h3><p>拉普拉斯平滑 </p><h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p><strong>优点</strong>:<br>(1）算法逻辑简单,易于实现（算法思路很简单，只要使用贝叶斯公式转化即可）<br>(2) 分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）<br><strong>缺点</strong>：<br>朴素贝叶斯假设属性之间相互独立，这种假设在实际过程中往往是不成立的。在属性之间相关性越大，分类误差也就越大。</p><h3 id="场景题"><a href="#场景题" class="headerlink" title="场景题"></a>场景题</h3><p>如预测今天的天气：根据历史数据，可以知道天气类型的<strong>先验分布</strong>，以及每种天气类型下的特征数据（如温度、湿度等）的<strong>条件分布</strong>，然后根据贝叶斯公式求得天气类型的<strong>后验分布</strong>。</p><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h3 id="3种决策树，区别，适用场景"><a href="#3种决策树，区别，适用场景" class="headerlink" title="3种决策树，区别，适用场景"></a>3种决策树，区别，适用场景</h3><p><strong>ID3</strong>:检测所有的属性，选择<strong>信息增益</strong>最大的属性产生决策树结点，由该属性的不同取值建立分支，再对各分支的子集递归调用该方法建立决策树结点的分支，直到所有子集仅包含同一类别的数据为止。最后得到一棵决策树。<br><strong>信息增益</strong>：<br>特征A对训练数据集D的信息增益，定义为集合A的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差：</p><script type="math/tex; mode=display">g(D,A)=H(D)-H(D|A)</script><p>不足之处：当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱，不能够对新样本进行有效的预测。</p><p><strong>C4.5</strong>：用<strong>信息增益比</strong>来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足。<br><strong>信息增益比</strong>：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点8.png" alt="机器学习知识点8"><br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点9.png" alt="机器学习知识点9"><br>但是同样的这个增益率对可取值数目较少的属性有所偏好，因此C4.5决策树<strong>先从候选划分属性中找出信息增益高于平均水平的属性，在从中选择增益率最高的</strong>。<br><strong>CART</strong>:用<strong>基尼系数</strong>来选择属性，选择划分后基尼系数最小的属性。<br><strong>基尼系数</strong>：<br>基尼系数值越小，说明二分之后的子样本的“纯净度”越高，即说明选择该属性（值）作为分裂属性（值）的效果越好。<br>对于样本集S，GINI计算如下：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点10.png" alt="机器学习知识点10"><br>其中，在样本集S中，$P_k$表示分类结果中第k个类别出现的频率。<br>  对于含有N个样本的样本集S，根据属性A的第i个属性值，将数据集S划分成两部分，则划分成两部分之后，GINI计算如下：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点11.png" alt="机器学习知识点11"><br>其中，$n_1$、$n_2$分别为样本子集$S_1$、$S_2$的样本个数。<br>  对于属性A，分别计算任意属性值将数据集划分成两部分之后的GINI，选取其中的最小值，作为属性A得到的最优二分方案：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点12.png" alt="机器学习知识点12"><br>对于样本集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本集S的最优二分方案：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点13.png" alt="机器学习知识点13"><br>所得到的属性A及其第i属性值，即为样本集S的最优分裂属性以及最优分裂属性值。</p><h3 id="连续值处理办法"><a href="#连续值处理办法" class="headerlink" title="连续值处理办法"></a>连续值处理办法</h3><p>参考周志华《机器学习》<br>C4.5采用<strong>二分法</strong>处理连续数值，取每两个相邻属性值的中位点作为候选划分点，计算基于此划分点二分化后的信息增益，选择最大的最为划分点。不同于离散属性，连续属性还可以作为后代结点的划分属性。</p><h3 id="缺失值处理办法"><a href="#缺失值处理办法" class="headerlink" title="缺失值处理办法"></a>缺失值处理办法</h3><p>问题一：如何在属性值缺失的情况下进行划分属性选择？<br>计算某个属性时，取在此属性上不缺失的样本子集，计算子集上的信息增益，最后乘以此子集占所有样本的比重，得到最终的信息增益。</p><p>问题二：给定划分属性，若该样本在此属性上的值缺失，怎么划分？<br>给每个样本附一个权重，若该样本在此属性上有值，则加入对应子结点，权重不变。若该样本在此属性上没有值，则加入所有子结点，并将其权重调整为此子结点中无缺失值占所有无缺失样本的比例*原权重。</p><h3 id="GBDT-和随机森林区别"><a href="#GBDT-和随机森林区别" class="headerlink" title="GBDT 和随机森林区别"></a>GBDT 和随机森林区别</h3><p>哪个具备交叉验证</p><h3 id="GBDT-和-xgboost-区别"><a href="#GBDT-和-xgboost-区别" class="headerlink" title="GBDT 和 xgboost 区别"></a>GBDT 和 xgboost 区别</h3><h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><h3 id="随机森林样本数量是否会影响模型复杂度"><a href="#随机森林样本数量是否会影响模型复杂度" class="headerlink" title="随机森林样本数量是否会影响模型复杂度"></a>随机森林样本数量是否会影响模型复杂度</h3><h3 id="特征选择方法"><a href="#特征选择方法" class="headerlink" title="特征选择方法"></a>特征选择方法</h3><p>信息增益<br>信息增益比<br>基尼系数</p><h3 id="信息熵和基尼系数关系"><a href="#信息熵和基尼系数关系" class="headerlink" title="信息熵和基尼系数关系"></a>信息熵和基尼系数关系</h3><p>信息熵在 x=1处一阶泰勒展开式就是基尼系数，纯度越高，值越小。<br>参考 <a href="https://blog.csdn.net/YE1215172385/article/details/79470926" target="_blank" rel="noopener">https://blog.csdn.net/YE1215172385/article/details/79470926</a></p><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><h3 id="损失函数-交叉熵损失函数"><a href="#损失函数-交叉熵损失函数" class="headerlink" title="损失函数(交叉熵损失函数)"></a>损失函数(交叉熵损失函数)</h3><p>$L=\sum_{i=1}^Ny^{(i)}log\ \hat y^{(i)}+(1-y^{(i)})log\ (1-\hat y^{(i)})$</p><h3 id="LR-怎么处理非线性"><a href="#LR-怎么处理非线性" class="headerlink" title="LR 怎么处理非线性"></a>LR 怎么处理非线性</h3><p>1.特征离散+特征交叉<br>2.核函数，不同于SVM，LR中所有样本都会参与运算，计算量过大。<br>3.LR之前加入神经网络，将神经网络看做特征提取</p><h3 id="能否用核函数"><a href="#能否用核函数" class="headerlink" title="能否用核函数"></a>能否用核函数</h3><p>加L2正则可以引入核函数。<a href="https://www.youtube.com/watch?v=AbaIkcQUQuo&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&amp;index=21" target="_blank" rel="noopener">https://www.youtube.com/watch?v=AbaIkcQUQuo&amp;list=PLXVfgk9fNX2IQOYPmqjqWsNUFl2kpk1U2&amp;index=21</a><br>推导：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点4.jpg" alt="机器学习知识点4"></p><h3 id="和-NB-的不同"><a href="#和-NB-的不同" class="headerlink" title="和 NB 的不同"></a>和 NB 的不同</h3><h3 id="特征离散化的好处"><a href="#特征离散化的好处" class="headerlink" title="特征离散化的好处"></a>特征离散化的好处</h3><p><a href="https://blog.csdn.net/yang090510118/article/details/39478033" target="_blank" rel="noopener">https://blog.csdn.net/yang090510118/article/details/39478033</a><br>在工业界，很少直接将连续值作为特征喂给逻辑回归模型，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p><ol><li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易scalable（扩展）。</li><li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。</li><li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合。</li><li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。</li><li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</li></ol><p>李沐少帅指出，模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p><p>大概的理解：<br>1）计算简单<br>2）简化模型<br>3）增强模型的泛化能力，不易受噪声的影响</p><h3 id="参数估计推导"><a href="#参数估计推导" class="headerlink" title="参数估计推导"></a>参数估计推导</h3><p>伯努利过程 极大似然 损失函数 梯度下降<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点3.jpeg" alt="机器学习知识点3"></p><h3 id="为什么使用Sigmoid"><a href="#为什么使用Sigmoid" class="headerlink" title="为什么使用Sigmoid"></a>为什么使用Sigmoid</h3><p>因为它引入了非线性映射，将线性回归值域映射到0-1之间，有助于直观的做出预测类型的判断：大于等于0.5表示阳性，小于0.5表示阴性。</p><h3 id="做-ctr预估"><a href="#做-ctr预估" class="headerlink" title="做 ctr预估"></a>做 ctr预估</h3><p>把被点击的样本当成正例，把未点击的样本当成负例，那么样本的ctr实际上就是样本为正例的概率，LR可以输出样本为正例的概率，所以可以用来解决这类问题，另外LR相比于其他模型有求解简单、可解释强的优点，这也是工业界所看重的。</p><ol><li>LR是线性模型，具有很好的可解释性，分布式计算迭代速度快。</li><li>LR可以很好的利用正则化解决稀疏性问题，尤其特征维数非常大，大到千亿级别。</li><li>输出是0-1的连续值，取值范围天然契合概率，而点击率也是一个0-1的概率值。<h3 id="归一化或者取对数"><a href="#归一化或者取对数" class="headerlink" title="归一化或者取对数"></a>归一化或者取对数</h3>提高梯度下降法求解最优解的速度<br><a href="https://blog.csdn.net/weixin_38111819/article/details/79729444" target="_blank" rel="noopener">https://blog.csdn.net/weixin_38111819/article/details/79729444</a><h3 id="与NB区别"><a href="#与NB区别" class="headerlink" title="与NB区别"></a>与NB区别</h3><a href="https://blog.csdn.net/cjneo/article/details/45167223" target="_blank" rel="noopener">https://blog.csdn.net/cjneo/article/details/45167223</a><br>1.一个是生成式模型，一个是判别式模型<br>2.Naive Bayes是建立在条件独立假设基础之上的，设特征X含有n个特征属性（X1，X2，…Xn），那么在给定Y的情况下，X1，X2，…Xn是条件独立的。LR的限制则要宽松很多，如果数据满足条件独立假设，LR能够取得非常好的效果；当数据不满度条件独立假设时，LR仍然能够通过调整参数让模型最大化的符合数据的分布，从而训练得到在现有数据集下的一个最优模型。<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2>当你要猜一个概率分布时，如果你对这个分布一无所知，那就猜熵最大的均匀分布，如果你对这个分布知道一些情况，那么，就猜满足这些情况的熵最大的分布。</li></ol><h3 id="与逻辑回归的相似点"><a href="#与逻辑回归的相似点" class="headerlink" title="与逻辑回归的相似点"></a>与逻辑回归的相似点</h3><p>从最大熵的思想出发得出的最大熵模型，最后的最大化求解就是在求$P(y|x)$的<strong>对数似然最大化</strong>。逻辑回归也是在求条件概率分布关于样本数据的对数似然最大化。二者唯一的不同就是条件概率分布的表示形式不同。</p><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>从分类超平面，到求两类间的最大间隔，到转化为求间隔分之一，等优化问题。然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求偏导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题， 最后再利用SMO（序列最小优化）来解决这个对偶问题。</p><h3 id="求解方法和推导"><a href="#求解方法和推导" class="headerlink" title="求解方法和推导"></a>求解方法和推导</h3><p>转化为对偶形式，极大极小求α，w 和 b</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>合页损失+正则化项<br>二分类问题的真正损失函数是0/1损失函数，但是因为其不是连续可导的，所以用合页损失函数代替。合页损失函数是0/1损失函数的上界。</p><h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>SMO 序列最小最优化算法</p><h3 id="怎么防止过拟合"><a href="#怎么防止过拟合" class="headerlink" title="怎么防止过拟合"></a>怎么防止过拟合</h3><p><strong>松弛变量</strong> 引入松弛变量使SVM能够容忍异常点的存在<br>目标函数中的<strong>正则化项</strong></p><h3 id="核函数原理"><a href="#核函数原理" class="headerlink" title="核函数原理"></a>核函数原理</h3><p>将样本从原始空间映射到一个更高维的特征空间，使得样本在这个空间中线性可分。但是计算映射的内积通常是困难的，因此引入核函数。两个样本在特征空间的内积等于他们在原始空间通过核函数计算的结果。</p><h3 id="为什么要引入对偶"><a href="#为什么要引入对偶" class="headerlink" title="为什么要引入对偶"></a>为什么要引入对偶</h3><p>原问题是凸二次规划问题，转换为对偶问题更加高效。对偶问题只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0。</p><h3 id="怎么做回归"><a href="#怎么做回归" class="headerlink" title="怎么做回归"></a>怎么做回归</h3><p>SVR</p><h3 id="怎么做多分类"><a href="#怎么做多分类" class="headerlink" title="怎么做多分类"></a>怎么做多分类</h3><p><a href="https://blog.csdn.net/xfChen2/article/details/79621396" target="_blank" rel="noopener">https://blog.csdn.net/xfChen2/article/details/79621396</a><br>多个超平面同时求解<br>一对多<br>一对一 投票 Libsvm的实现方法</p><h3 id="是否适合大规模数据"><a href="#是否适合大规模数据" class="headerlink" title="是否适合大规模数据"></a>是否适合大规模数据</h3><p>PEGASOS</p><h3 id="与-LR-区别"><a href="#与-LR-区别" class="headerlink" title="与 LR 区别"></a>与 LR 区别</h3><p><a href="https://www.jianshu.com/p/f86de852ee96" target="_blank" rel="noopener">https://www.jianshu.com/p/f86de852ee96</a><br><a href="https://blog.csdn.net/jieming2002/article/details/79317496" target="_blank" rel="noopener">https://blog.csdn.net/jieming2002/article/details/79317496</a></p><ol><li>本质区别是损失函数的不同：<br><strong>LR 的损失函数</strong>：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点1.png" alt="机器学习知识点1"><br>基于概率理论，假设样本为1的概率可以用sigmoid函数来表示，然后通过极大似然估计的方法估计出参数的值。<br><strong>SVM 的目标函数</strong>：<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点2.png" alt="机器学习知识点2"><br>基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面。</li><li>支持向量机只考虑局部的边界线附近的点（支持向量），而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。</li><li>解决非线性问题时，LR 很少使用核函数。若使用核函数，LR 中每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。</li><li>SVM 目标函数中自带正则</li></ol><h3 id="数据不平衡解决办法"><a href="#数据不平衡解决办法" class="headerlink" title="数据不平衡解决办法"></a>数据不平衡解决办法</h3><p>再缩放《西瓜书》P67 实际操作复杂<br>上采样<br>下采样<br>libSVM的解决办法：在惩罚因子上做文章。给样本数量少的类更大的惩罚因子，表示我们重视这部分样本。libSVM就直接使用样本数量的比确定惩罚因子。</p><h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h2><h2 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h2><h3 id="3个-attention-的作用"><a href="#3个-attention-的作用" class="headerlink" title="3个 attention 的作用"></a>3个 attention 的作用</h3><p>编码器和解码器中的attention 学习文本的表示，学习句子内部的词依赖关系，捕获句子的内部结构。<br>编码器和解码器attention用来<strong>翻译对齐</strong>。</p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><h3 id="对比-Attention-和-CNN-RNN"><a href="#对比-Attention-和-CNN-RNN" class="headerlink" title="对比 Attention 和 CNN RNN"></a>对比 Attention 和 CNN RNN</h3><p>RNN的方案很简单，递归式进行：<br>$\boldsymbol{y}<em>t = f(\boldsymbol{y}</em>{t-1},\boldsymbol{x}_t)$<br>无法并行，速度较慢<br>输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示。<br>CNN 则需要通过层叠来扩大感受野</p><h3 id="长度需要填充的文本怎么避免填充值影响后续计算"><a href="#长度需要填充的文本怎么避免填充值影响后续计算" class="headerlink" title="长度需要填充的文本怎么避免填充值影响后续计算"></a>长度需要填充的文本怎么避免填充值影响后续计算</h3><p><a href="https://blog.csdn.net/stupid_3/article/details/83184691" target="_blank" rel="noopener">https://blog.csdn.net/stupid_3/article/details/83184691</a><br><a href="https://blog.csdn.net/mijiaoxiaosan/article/details/74909076" target="_blank" rel="noopener">https://blog.csdn.net/mijiaoxiaosan/article/details/74909076</a><br>给在较短的序列后面填充0。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。<br>具体的做法是，把这些位置的值加上一个非常大的负数(可以是负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！</p><h1 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h1><h2 id="最优化算法"><a href="#最优化算法" class="headerlink" title="最优化算法"></a>最优化算法</h2><p>迭代尺度法<br>梯度下降法<br>牛顿法或拟牛顿法</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><h3 id="过拟合-1"><a href="#过拟合-1" class="headerlink" title="过拟合"></a>过拟合</h3><p>当我们提高在训练数据上的表现时，在测试数据上反而下降。<br>过拟合发生的本质原因，是由于监督学习问题的不适定：在高中数学我们知道，从n个（线性无关）方程可以解n个变量，解n+1个变量就会解不出。在监督学习中，往往数据（对应了方程）远远少于模型空间（对应了变量）。因此过拟合现象的发生，可以分解成以下三点：</p><p>1.<strong>有限的训练数据</strong>不能完全反映出一个模型的好坏，然而我们却不得不在这有限的数据上挑选模型，因此我们完全有可能挑选到在训练数据上表现很好而在测试数据上表现很差的模型，因为我们完全无法知道模型在测试数据上的表现。<br>2.如果模型空间很大，也就是有很多很多模型可以给我们挑选，那么挑到对的模型的机会就会很小。<br>3.与此同时，如果我们要在训练数据上表现良好，最为直接的方法就是要在足够大的模型空间中挑选模型，否则如果模型空间很小，就不存在能够拟合数据很好的模型。</p><p>由上3点可见，要拟合训练数据，就要足够大的模型空间；用了足够大的模型空间，挑选到测试性能好的模型的概率就会下降。因此，就会出现训练数据拟合越好，测试性能越差的过拟合现象。</p><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>在<strong>经验风险</strong>的上加一个正则化项或罚项，是<strong>结构风险</strong>最小化策略的实现。正则化项一般是模型复杂度的单点递增函数，模型越复杂，正则项就越大。<br>正则化的作用是选择经验风险与模型复杂度同时较小的模型。</p><h3 id="L0和L1"><a href="#L0和L1" class="headerlink" title="L0和L1"></a>L0和L1</h3><p>L0范数是指向量中<strong>非零元素</strong>的个数。<br>如果用L0规则化一个参数矩阵W，就是希望W中大部分元素是零，实现稀疏。<br>L0范数的应用：<br>1）特征选择<br>​实现特征的自动选择，去除无用特征。稀疏化可以去掉这些无用特征，将特征对应的权重置为零。<br>2）可解释性（interpretability）​<br>例如判断某种病的患病率时，最初有1000个特征，建模后参数经过稀疏化，最终只有5个特征的参数是非零的，那么就可以说影响患病率的主要就是这5个特征。<br>弊端：<strong>非连续</strong>；<strong>非凸</strong>；<strong>不可导</strong>；</p><p>L1范数是指向量中各个元素的绝对值之和，也叫”系数规则算子（Lasso regularization）“。<br>L1范数也可以实现稀疏，通过将无用特征对应的参数W置为零实现。<br>L0和L1都可以实现稀疏化，不过一般选用L1而不用L0，原因包括：<br>1）L0范数很难优化求解（NP难）；<br>2）L1是L0的最优凸近似，比L0更容易优化求解。</p><h3 id="L2"><a href="#L2" class="headerlink" title="L2"></a>L2</h3><p><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点5.png" alt="机器学习知识点5"><br>L2范数​​是指向量各元素的平方和然后开方，用在回归模型中也称为岭回归（Ridge regression）。<br>L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。</p><h3 id="为什么L1系数可以被压缩为0，L2是接近于0"><a href="#为什么L1系数可以被压缩为0，L2是接近于0" class="headerlink" title="为什么L1系数可以被压缩为0，L2是接近于0"></a>为什么L1系数可以被压缩为0，L2是接近于0</h3><p><a href="https://www.zhihu.com/question/37096933/answer/475278057" target="_blank" rel="noopener">https://www.zhihu.com/question/37096933/answer/475278057</a><br><a href="https://liam.page/2017/03/30/L1-and-L2-regularizer/" target="_blank" rel="noopener">https://liam.page/2017/03/30/L1-and-L2-regularizer/</a><br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点6.jpg" alt="机器学习知识点6"></p><h3 id="L1怎么解决求导困难（不可导）"><a href="#L1怎么解决求导困难（不可导）" class="headerlink" title="L1怎么解决求导困难（不可导）"></a>L1怎么解决求导困难（不可导）</h3><p>近端梯度下降<br><a href="https://www.zhihu.com/question/265426774" target="_blank" rel="noopener">https://www.zhihu.com/question/265426774</a></p><h3 id="怎么防止过拟合-1"><a href="#怎么防止过拟合-1" class="headerlink" title="怎么防止过拟合"></a>怎么防止过拟合</h3><p><a href="https://www.zhihu.com/question/20924039" target="_blank" rel="noopener">https://www.zhihu.com/question/20924039</a> 参考景略集智的回答<br>1.正则化，可以用 L1或 L2范数，选择经验风险和模型复杂度同时较小的模型。<br>2.dropout CNN 和 RNN 不同<br>DropoutWrapper RNN 中Dropout只能是层与层之间（输入层与LSTM1层、LSTM1层与LSTM2层）的Dropout；同一个层里面，T时刻与T+1时刻是不会Dropout的<br>3.数据增强 利用 GAN 生成更多数据<br>4.bagging<br>5.batch normalization CNN<br>6.layer normalization RNN<br>7.Early Stopping</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>训练阶段和测试阶段有何不同？ <a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">https://blog.csdn.net/program_developer/article/details/80737724</a><br>dropout中不同设置的神经元和我们训练几种不同的神经网络很像。因此，dropout处理很像是大量不同网络的平均结果。<br>训练阶段以 p 的概率置隐层神经元输出值为0。测试阶段要缩放激活函数，<em>(1-p)。<br>为了一次性定义模型，tf 等选择测试阶段保持不变，在训练阶段对激活函数进行Inverted Dropout，</em>1/(1-p)</p><h2 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h2><p>Internal Covariate Shift问题，在训练过程中，隐层的输入分布老是变来变去，所以输出的分布也会不断变化，造成梯度需要不断适应新的数据分布。<br>BN(2015):对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，这样输入的小变化使得损失函数变化较大，梯度变大，以此避免梯度消失问题，也加快了收敛速度。<a href="https://blog.csdn.net/leviopku/article/details/83109422" target="_blank" rel="noopener">https://blog.csdn.net/leviopku/article/details/83109422</a> RNN 不适用（也有 paper 实现 rnn 可用的 BN）BN特别依赖Batch Size；当Batch size很小的适合，BN的效果就非常不理想了。</p><h2 id="LN"><a href="#LN" class="headerlink" title="LN"></a>LN</h2><p>BN 是“竖着”来的，对于 batch 内的样本各个维度做归一化，与 batch 有关；<br>LN 是“横着”来的，对于一个样本，同一层的不同神经元间做归一化。</p><h2 id="对偶问题（拉格朗日乘子法）"><a href="#对偶问题（拉格朗日乘子法）" class="headerlink" title="对偶问题（拉格朗日乘子法）"></a>对偶问题（拉格朗日乘子法）</h2><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="boosting-AdaBoost"><a href="#boosting-AdaBoost" class="headerlink" title="boosting(AdaBoost)"></a>boosting(AdaBoost)</h3><p>参考《统计学习方法》</p><ol><li>给定训练样本集S，其中X和Y分别对应于正例样本和负例样本； T为训练的最大循环次数；</li><li>初始化样本权重为1/n ，即为训练样本的初始概率分布；</li><li>第一次迭代：<br>(1) 训练弱分类器；<br>(2) 计算弱分类器的错误率；<br>(3) 计算弱分类器的权重；<br>(4) 更新样本权重；<br>经T次循环后，得到T个弱分类器，按更新的权重叠加，最终得到的强分类器。</li></ol><h3 id="bagging"><a href="#bagging" class="headerlink" title="bagging"></a>bagging</h3><p>1）从原始样本集中抽取训练集.每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）.共进行k轮抽取，得到k个训练集.（k个训练集相互独立）<br>2）每次使用一个训练集得到一个模型，k个训练集共得到k个模型.（注：根据具体问题采用不同的分类或回归方法，如决策树、神经网络等）<br>3）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果.</p><h3 id="两者不同"><a href="#两者不同" class="headerlink" title="两者不同"></a>两者不同</h3><p>Bagging + 决策树 = 随机森林<br>AdaBoost + 决策树 = 提升树<br>Gradient Boosting + 决策树 = GBDT</p><p>1）样本选择上：<br>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的.<br>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整.</p><p>2）样例权重：<br>Bagging：使用均匀取样，每个样例的权重相等<br>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大.</p><p>3）预测函数：<br>Bagging：所有预测函数的权重相等.<br>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.</p><p>4）并行计算：<br>Bagging：各个预测函数可以并行生成<br>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果.</p><h3 id="为什么说bagging是减少方差variance，而boosting是减少偏差bias"><a href="#为什么说bagging是减少方差variance，而boosting是减少偏差bias" class="headerlink" title="为什么说bagging是减少方差variance，而boosting是减少偏差bias"></a>为什么说bagging是减少方差variance，而boosting是减少偏差bias</h3><p><a href="https://www.zhihu.com/question/45487317" target="_blank" rel="noopener">https://www.zhihu.com/question/45487317</a><br>其实就机器学习算法来说，其泛化误差可以分解为两部分，偏差（bias)和方差(variance)。这个可由下图的式子导出（这里用到了概率论公式$D(X)=E(X^2)-[E(X)]^2$。<strong>偏差指的是算法的期望预测与真实预测之间的偏差程度，反应了模型本身的拟合能力；方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响</strong>。</p><script type="math/tex; mode=display">Err(h) = E[(h-f)^2]=E[((h-E[h])-(f-E[h]))^2]$$$$=E[(h-E[h])^2]-2E[(h-E[h])(f-E[h])]+E[(f-E[h])^2]$$$$=var-2E[b(h-E[h]) +b^2=var-2b(E[h]-E[E[h]]) +b^2$$$$=var -2b*0+b^2=var+b^2</script><p>如下图所示，当模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但此时如果换一组数据可能模型的变化就会很大，即模型的方差很大。所以模型过于复杂的时候会导致过拟合。当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，模型的方差很小。还是因为模型简单，所以偏差会很大。<br><img src="/2018/12/07/机器学习知识点汇总/机器学习知识点7.png" alt="机器学习知识点7"><br>也就是说，当我们训练一个模型时，偏差和方差都得照顾到，漏掉一个都不行。对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance)$E[(h-E[h])]$ ,因为采用了相互独立的基分类器多了以后，h的值自然就会靠近$E[h]$.所以对于每个基分类器来说，目标就是如何降低这个偏差（bias),所以我们会采用深度很深甚至不剪枝的决策树。对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差（bias）,所以对于每个基分类器来说，问题就在于如何选择variance更小的分类器，即更简单的分类器，所以我们选择了深度很浅的决策树。</p><h3 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h3><p>随机森林的集成学习方法是bagging ，但是和bagging 不同的是bagging只使用bootstrap有放回的采样样本，但随机森林既随机采样样本，也<strong>随机选择特征</strong>，因此防止过拟合能力更强，降低方差。<br>如果有M个输入变量，每个节点都将随机选择m($m&lt;M$)个特定的变量，然后运用这m个变量来确定最佳的分裂点。在决策树的生成过程中，m的值是保持不变的。m一般取$log_2 M$</p><h3 id="stacking"><a href="#stacking" class="headerlink" title="stacking"></a>stacking</h3><p><a href="https://blog.csdn.net/qq_18916311/article/details/78557722" target="_blank" rel="noopener">https://blog.csdn.net/qq_18916311/article/details/78557722</a></p><h3 id="gbdt"><a href="#gbdt" class="headerlink" title="gbdt"></a>gbdt</h3><p>用损失函数的负梯度来拟合本轮损失的近似值，进而拟合出一个CART回归树。</p><h3 id="xgboost"><a href="#xgboost" class="headerlink" title="xgboost"></a>xgboost</h3><h3 id="lgbm"><a href="#lgbm" class="headerlink" title="lgbm"></a>lgbm</h3><h3 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h3><h3 id="xgboost和gbdt区别"><a href="#xgboost和gbdt区别" class="headerlink" title="xgboost和gbdt区别"></a>xgboost和gbdt区别</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;机器学习模型&quot;&gt;&lt;a href=&quot;#机器学习模型&quot; class=&quot;headerlink&quot; title=&quot;机器学习模型&quot;&gt;&lt;/a&gt;机器学习模型&lt;/h1&gt;&lt;h2 id=&quot;感知机&quot;&gt;&lt;a href=&quot;#感知机&quot; class=&quot;head
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2017-360-人机大战比赛记录</title>
    <link href="http://yoursite.com/2018/10/15/2017-360-%E4%BA%BA%E6%9C%BA%E5%A4%A7%E6%88%98%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/"/>
    <id>http://yoursite.com/2018/10/15/2017-360-人机大战比赛记录/</id>
    <published>2018-10-15T01:50:06.000Z</published>
    <updated>2018-11-29T12:11:02.443Z</updated>
    
    <content type="html"><![CDATA[<p>CCF 大数据与计算智能大赛（BDCI）<br>【赛题四】360搜索-AlphaGo之后“人机大战”Round 2 ——机器写作与人类写作的巅峰对决<br>比赛时间2017.10-2017.12<br><a id="more"></a></p><h1 id="赛事背景"><a href="#赛事背景" class="headerlink" title="赛事背景"></a>赛事背景</h1><p>如果说AlphaGo和人类棋手的对决拉响了“人机大战”的序曲，在人类更为通识的写作领域，即将上演更为精彩的机器写作和人类写作的对决。人类拥有数万年的书写历史，人类写作蕴藏无穷的信息、情感和思想。但随着深度学习、自然语言处理等人工智能技术发展，机器写作在语言组织、语法和逻辑处理方面几乎可以接近人类水平。360搜索智能写作助手也在此背景下应运而生。<br>本次CCF大数据和人工智能大赛上，360搜索智能写作助手（机器写作）和人类写作将狭路相逢，如何辨别出一篇文章是通过庞大数据算法训练出来的机器写作的，还是浸染漫长书写历史的人类创作的？我们拭目以待！<br>本次赛题任务：挑战者能够设计出优良的算法模型从海量的文章中区分出文章是机器写作还是人类写作。</p><h1 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h1><h2 id="传统文本分类"><a href="#传统文本分类" class="headerlink" title="传统文本分类"></a>传统文本分类</h2><p>文本分类问题算是自然语言处理领域中一个非常经典的问题，目前形成了一套经典算法，这个阶段的主要是人工特征工程+浅层分类模型。训练文本分类器过程见下图：<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战1.png" alt="人机大战1"></p><h2 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h2><h3 id="CNN-方法"><a href="#CNN-方法" class="headerlink" title="CNN 方法"></a>CNN 方法</h3><p>我们首先设计了基于词项和字符的卷积神经网络模型 (Char-CNN、Word-CNN、WordChar-CNN) 来提取局部感受野的特征。根据观察，词序级别的错误或颠倒是负样本中出现次数最多的问题，而卷积神经网络中的卷积+池化操作能够很好的捕获这种 N-Gram 的错误。Char-CNN 首先对单个字进行向量表示，随后输入 三通道的卷积神经网络通过池化层提取特征，最后进行二分类的 softmax 得到每一类的概率。Word-CNN 和 WordChar-CNN 的模型与 Char-CNN 类似，只是在输入数据上，Word-CNN 读取的是分词后的词向量表示而 WordChar-CNN 综合考虑了词项和字符的特征，相当于前两个模型的集成。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战3.png" alt="人机大战3"></p><h3 id="RNN-Attention方法"><a href="#RNN-Attention方法" class="headerlink" title="RNN + Attention方法"></a>RNN + Attention方法</h3><p>除了上述词序问题，负样本还有很多语句重复或语句不连贯的问题。而带有 Attention 机制的循环神经网络模型十分适合提取这样的特征，因此我们针对此类样本设计了加入注意力机 制的循环神经网络模型 (Word-HAN) 模型，首先对分词后的词项进行向量表示，随后通过双向 GRU 来提取句子每个时序的特征，使用 Attention 机制来提取更大视野窗又的不连贯特征以及缓解长距离依赖问题，这样就可以得到句子的向量表示，最后再使用相同的方法获得文章的向量表示并进行二分类操作。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战4.png" alt="人机大战4"></p><h3 id="CNN-RNN-Attention-方法"><a href="#CNN-RNN-Attention-方法" class="headerlink" title="CNN + RNN + Attention 方法"></a>CNN + RNN + Attention 方法</h3><p>我们还思考了如何在不过分牺牲训练时间的同时尽可能地提高循环神经网络的表现性能。我们认为，传统的 LSTM、GRU 模型对句子语义较为敏感，能够很好的保留句子的语义。但是对感知相邻句子之间的错误并不敏感，如果能让它结合卷积神经网络的优点，就可以增强模型的特征提取能力。基于这种想法，我们设计了 Word-HCNN 模型，结构如图所示。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战5.png" alt="人机大战5"></p><p>Word-HCNN 综合了 Word-HANN 的和 Word-CNN 的网络结构。首先通过双向 GRU+Attention 的方法将每个句子表示成为向量，再通过多通道卷积提取相邻句子之间特征，这是句子级别的”N-Gram”，模型的输出层依旧是一个 2 分类的 softmax。</p><h1 id="样本数据分析"><a href="#样本数据分析" class="headerlink" title="样本数据分析"></a>样本数据分析</h1><p>因为分类问题中类别平衡与否对最终的分类结果会产生很大的影响，所以我们首先统计了类别分布情况。两类样本大致平衡。<br>与此同时，我们还对文章长度分布进行了统计，总体来看，几乎所有文章的长度都在 1500 个词以内，这符合我们的正常认知。<br><img src="/2018/10/15/2017-360-人机大战比赛记录/人机大战2.png" alt="人机大战2"></p><h1 id="数据预处理与模型初始参数"><a href="#数据预处理与模型初始参数" class="headerlink" title="数据预处理与模型初始参数"></a>数据预处理与模型初始参数</h1><p>考虑到卷积神经网络只能处理固定维度的输入数据，所以我们对于长度为 $n$ 的文章，首先对文章进行截断或者补齐到固定长度 $T$，如果 $n &gt; T$，则只保留前 $T$ 个 单词，如果 $n &lt; T$，则在文章末尾添加 padding 来补齐长度。预处理之后，每篇文章都包含 T 个词。</p><p>尽管 RNN 能够处理变长的时序数据，但是考虑到矩阵操作的便利，我们依然需要固定输入的维度对数据进行预处理，与卷积神经网络不同的是，循环神经网络是以句子作为处理单元，而不是把文章中的所有词都拼在一起。对于包含 $s$ 个句子的文章，首先进行句子级别的截断或者补齐到固定数目 $S$ ，如果 $s &gt; S$ ，则只保留前 $S$ 个句子，如果 s &lt; S，则添加 padding 补齐句子数目。同样的操作也作用于每个句子，对于长度为 $n$ 的句子，首先对句子进行截断或者补齐到固定长度 $T$ ， 如果$n&gt;T$，则只保留前 $T$ 个单词，如果$n&lt;T$，则在句子末尾添加 padding 补齐长度。预处理之后，每篇文章都被包含 $S$ 个句子，每个句子都包含 $T$ 个词。</p><p>将数据集按照 6:2:2 的比例拆分成训练集，验证集和测试集三部分。文章截断 补齐句子数为 45，句子截断补齐词个数为 48，分词工具使用 jieba，输入层使用 wiki 语料通过 word2vec 预训练得到的 100 维向量，只保留频率最高的前 10000 个单词，其他单词均映射为 <unk>，Batch size 为 64。三通道卷积层使用 1 维卷积，卷积核尺寸分别为 3、4、5，每种卷积核的滤波器数目均为 256，在最大池化层之后加入 0.2 的 Dropout，GRU 的隐藏层维数为 128，Attention 的权重矩阵 $W$ 的维度是 128 × 128。输出层使用了包含 256 个神经元的单隐层全连接神经网络，并且进行了 Batch Normalization，模型所有参数的初始化都从 $[−0.1, 0.1]$ 中均匀采样，使用 adam 优化模型，损失函数为多类交叉熵损失，迭代 15 轮。</unk></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CCF 大数据与计算智能大赛（BDCI）&lt;br&gt;【赛题四】360搜索-AlphaGo之后“人机大战”Round 2 ——机器写作与人类写作的巅峰对决&lt;br&gt;比赛时间2017.10-2017.12&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP中的attention和self-attention</title>
    <link href="http://yoursite.com/2018/10/11/NLP%E4%B8%AD%E7%9A%84attention%E5%92%8Cself-attention/"/>
    <id>http://yoursite.com/2018/10/11/NLP中的attention和self-attention/</id>
    <published>2018-10-11T07:08:14.000Z</published>
    <updated>2018-11-29T15:53:53.675Z</updated>
    
    <content type="html"><![CDATA[<p>本文参考张俊林博士的《深度学习中的注意力机制》和苏剑林的《Attention is All You Need》浅读（简介+代码）》，简单总结一下 NLP 中用到的 attention 和 self-attention。<br><a id="more"></a></p><h1 id="attention-的发展趋势"><a href="#attention-的发展趋势" class="headerlink" title="attention 的发展趋势"></a>attention 的发展趋势</h1><p>attention 机制的发展如下图所示：<br><img src="/2018/10/11/NLP中的attention和self-attention/self-attention1.png" alt="self-attention1"><br>Attention机制最早是在视觉图像领域提出来的，但是真正火起来应该算是2014年google mind团队的论文《Recurrent Models of Visual Attention》，他们在RNN模型上使用了attention机制来进行图像分类。随后，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，使用类似attention的机制在机器翻译任务上将翻译和对齐同时进行，他们的工作算是第一个将attention机制应用到NLP领域中。接着attention机制被广泛应用在基于RNN/CNN等神经网络模型的各种NLP任务中。2017年，google机器翻译团队发表的《Attention is all you need》中大量使用了自注意力（self-attention）机制来学习文本表示。</p><h1 id="NLP-中的序列编码"><a href="#NLP-中的序列编码" class="headerlink" title="NLP 中的序列编码"></a>NLP 中的序列编码</h1><p>深度学习做NLP的方法，基本上都是先将句子分词，然后每个词转化为对应的词向量序列。这样一来，每个句子都对应的是一个矩阵$X=(x_1,x_2,…,x_t)$，其中$x_i$都代表着第$i$个词的词向量（行向量），维度为$d$维，故$X\in\mathbb{R}^{n×d}$。这样的话，问题就变成了编码这些序列了。</p><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>第一个基本的思路是RNN层，RNN的方案很简单，递归式进行：</p><script type="math/tex; mode=display">y_t=f(y_{t−1},x_t)</script><p>不管是已经被广泛使用的LSTM、GRU还是最近的SRU，都并未脱离这个递归框架。RNN结构本身比较简单，也很适合序列建模，但RNN的明显缺点之一就是<strong>无法并行</strong>，因此速度较慢，这是递归的天然缺陷。另外苏剑林提到：</p><blockquote><p>RNN无法很好地学习到全局的结构信息，因为它本质是一个马尔科夫决策过程。</p></blockquote><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>第二个思路是CNN层，其实CNN的方案也是很自然的，窗口式遍历，比如尺寸为3的卷积，就是</p><script type="math/tex; mode=display">y_t=f(x_{t−1},x_t,x_{t+1})</script><p>在FaceBook的论文中，纯粹使用卷积也完成了Seq2Seq的学习，是卷积的一个精致且极致的使用案例。CNN方便并行，而且容易捕捉到一些全局的结构信息。</p><h2 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h2><p>Google的大作提供了第三个思路：纯Attention！单靠注意力就可以！RNN要逐步递归才能获得全局信息，因此一般要双向RNN才比较好；CNN事实上只能获取局部信息，是通过层叠来增大感受野；attention的思路最为粗暴，它一步到位获取了全局信息！它的解决方案是：</p><script type="math/tex; mode=display">y_t=f(x_t,A,B)</script><p>其中$A$,$B$是另外一个序列（矩阵）。如果都取$A=B=X$，那么就称为Self-Attention，它的意思是直接将$x_t$与原来的每个词进行比较，最后算出$y_t$！</p><h1 id="attention-1"><a href="#attention-1" class="headerlink" title="attention"></a>attention</h1><p><img src="/2018/10/11/NLP中的attention和self-attention/self-attention2.png" alt="self-attention2"><br>将Source中的构成元素想象成是由一系列的&lt; Key,Value &gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><script type="math/tex; mode=display">Attention(Query,Source)=\sum_{i=1}^{L_x}{Similarity(Query,Key_i)*Value_i}</script><p>其中， $L_x=||Source||$代表Source的长度。在机器翻译中，Source中的Key和Value指向的是同一个东西（输入句子中每个单词对应的语义编码）。</p><h2 id="attention计算过程"><a href="#attention计算过程" class="headerlink" title="attention计算过程"></a>attention计算过程</h2><ol><li>根据Query和Key计算两者的相似性或者相关性；</li><li>对第一阶段的原始分值进行归一化处理，得到权重系数；</li><li>根据权重系数对Value进行加权求和。</li></ol><p><img src="/2018/10/11/NLP中的attention和self-attention/self-attention3.png" alt="self-attention3"><br>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个$Key_i$，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：<br><img src="/2018/10/11/NLP中的attention和self-attention/self-attention4.png" alt="self-attention4"><br>第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><script type="math/tex; mode=display">a_i=Softmax(Sim_i)=\frac{e^{Sim_i}}{\sum_{j=1}^{L_x}e^{Sim_j}}</script><p>第二阶段的计算结果$a_i$即为$Value_i$对应的权重系数，然后进行加权求和即可得到Attention数值：</p><script type="math/tex; mode=display">Attention(Query,Source)=\sum_{i=1}^{L_x}{a_i*Value_i}</script><h1 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h1><p>Google的论文模型的整体结构如下图，还是由编码器和解码器组成，在编码器的一个网络块中，由一个多头attention子层和一个前馈神经网络子层组成，整个编码器栈式搭建了N个块。类似于编码器，只是解码器的一个网络块中多了一个多头attention层。为了更好的优化深度网络，整个网络使用了残差连接和对层进行了规范化（Add&amp;Norm）。<br><img src="/2018/10/11/NLP中的attention和self-attention/self-attention6.png" alt="self-attention6"></p><h2 id="scaled-dot-Product-attention"><a href="#scaled-dot-Product-attention" class="headerlink" title="scaled dot-Product attention"></a>scaled dot-Product attention</h2><p>对比我在前面背景知识里提到的attention的一般形式，其实scaled dot-Product attention就是我们常用的使用点积进行相似度计算的attention，只是多了一个$\sqrt{d_k}$，$\sqrt{d_k}$为$K$的维度,起到调节作用，使得内积不至于太大。</p><script type="math/tex; mode=display">Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax\left(\frac{\boldsymbol{Q}\boldsymbol{K}^{\top}}{\sqrt{d_k}}\right)\boldsymbol{V}</script><p><img src="/2018/10/11/NLP中的attention和self-attention/self-attention5.png" alt="self-attention5"></p><h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><p><img src="/2018/10/11/NLP中的attention和self-attention/self-attention7.png" alt="self-attention7"></p><p>多头attention（Multi-head attention）结构如上图，Query，Key，Value首先经过一个线性变换，然后输入到scaled dot-Product attention，注意这里要做h次，其实也就是所谓的多头，每一次算一个头。而且每次Q，K，V进行线性变换的参数W是不一样的。</p><script type="math/tex; mode=display">head_i = Attention(\boldsymbol{Q}\boldsymbol{W}_i^Q,\boldsymbol{K}\boldsymbol{W}_i^K,\boldsymbol{V}\boldsymbol{W}_i^V)</script><p>然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。</p><script type="math/tex; mode=display">MultiHead(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = Concat(head_1,...,head_h)</script><p>可以看到，google提出来的多头attention的不同之处在于进行了h次计算而不仅仅算一次，论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息。这里的设计类似于 CNN 的多个卷积核。</p><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h2><p>所谓Self Attention，其实就是$Attention(X,X,X)$，$X$就是前面说的输入序列。也就是说，在序列内部做Attention，寻找序列内部的联系。更准确来说，Google所用的是Self Multi-Head Attention：</p><script type="math/tex; mode=display">\boldsymbol{Y}=MultiHead(\boldsymbol{X},\boldsymbol{X},\boldsymbol{X})</script><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>但是这样的模型并不能捕捉<strong>序列的顺序</strong>！换句话说，如果将$K,V$按行打乱顺序（相当于句子中的词序打乱），那么Attention的结果还是一样的。这就表明了，到目前为止，Attention模型顶多是一个非常精妙的“词袋模型”而已。</p><p>于是Google再又提出了Position Embedding，也就是“位置向量”，将每个位置编号，然后每个编号对应一个向量，通过结合位置向量和词向量，就给每个词都引入了一定的位置信息，这样Attention就可以分辨出不同位置的词了。</p><p>在以往的Position Embedding中，基本都是根据任务训练出来的向量。而Google直接给出了一个构造Position Embedding的公式:</p><script type="math/tex; mode=display">\left\{\begin{aligned}&PE_{2i}(p)=\sin\Big(p/10000^{2i/{d_{pos}}}\Big)\\ &PE_{2i+1}(p)=\cos\Big(p/10000^{2i/{d_{pos}}}\Big) \end{aligned}\right.</script><p>将id为$p$的位置映射为一个$d_{pos}$维的位置向量，这个向量的第$i$个元素的数值就是$PE_i(p)$。</p><p>结合位置向量和词向量有几个可选方案，可以把它们拼接起来作为一个新向量，也可以把位置向量定义为跟词向量一样大小，然后两者加起来。FaceBook的论文和Google论文中用的都是后者。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="attention-2"><a href="#attention-2" class="headerlink" title="attention:"></a>attention:</h2><h3 id="Keras"><a href="#Keras" class="headerlink" title="Keras:"></a>Keras:</h3><p><a href="https://github.com/fuliucansheng/360/blob/master/models/deepzoo.py" target="_blank" rel="noopener">https://github.com/fuliucansheng/360/blob/master/models/deepzoo.py</a></p><h2 id="self-attention-1"><a href="#self-attention-1" class="headerlink" title="self-attention:"></a>self-attention:</h2><h3 id="tf的实现："><a href="#tf的实现：" class="headerlink" title="tf的实现："></a>tf的实现：</h3><p><a href="https://github.com/bojone/attention/blob/master/attention_tf.py" target="_blank" rel="noopener">https://github.com/bojone/attention/blob/master/attention_tf.py</a></p><h3 id="Keras版"><a href="#Keras版" class="headerlink" title="Keras版:"></a>Keras版:</h3><p><a href="https://github.com/bojone/attention/blob/master/attention_keras.py" target="_blank" rel="noopener">https://github.com/bojone/attention/blob/master/attention_keras.py</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文参考张俊林博士的《深度学习中的注意力机制》和苏剑林的《Attention is All You Need》浅读（简介+代码）》，简单总结一下 NLP 中用到的 attention 和 self-attention。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://yoursite.com/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>创建第一个Fabric应用中各文件作用</title>
    <link href="http://yoursite.com/2018/10/10/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFabric%E5%BA%94%E7%94%A8%E4%B8%AD%E5%90%84%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8/"/>
    <id>http://yoursite.com/2018/10/10/创建第一个Fabric应用中各文件作用/</id>
    <published>2018-10-10T09:09:12.000Z</published>
    <updated>2018-10-11T07:10:26.476Z</updated>
    
    <content type="html"><![CDATA[<p>本文接着上次创建第一个 Fabric 应用,简单介绍一下流程中各个文件的作用.<br><a id="more"></a></p><h2 id="startFabric-sh脚本中做的事情"><a href="#startFabric-sh脚本中做的事情" class="headerlink" title="./startFabric.sh脚本中做的事情:"></a>./startFabric.sh脚本中做的事情:</h2><h3 id="1-清除keystore"><a href="#1-清除keystore" class="headerlink" title="1.清除keystore"></a>1.清除keystore</h3><pre><code>`rm -rf ./hfc-key-store`</code></pre><h3 id="2-启动-Fabric-创建一个channel-并把peer添加到channel"><a href="#2-启动-Fabric-创建一个channel-并把peer添加到channel" class="headerlink" title="2.启动 Fabric; 创建一个channel,并把peer添加到channel"></a>2.启动 Fabric; 创建一个channel,并把peer添加到channel</h3><ul><li>在 ../basic-network中执行./start.sh</li><li>启动 Fabric: 在start.sh中</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.yml up -d ca.example.com orderer.example.com peer0.org1.example.com couchdb</span><br></pre></td></tr></table></figure><ul><li>创建 channel:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp&quot; peer0.org1.example.com peer channel create -o orderer.example.com:7050 -c mychannel -f /etc/hyperledger/configtx/channel.tx</span><br></pre></td></tr></table></figure><ul><li>把 peer 加入到刚刚创建的 channel 中:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/msp/users/Admin@org1.example.com/msp&quot; peer0.org1.example.com peer channel join -b mychannel.block</span><br></pre></td></tr></table></figure><h3 id="3-启动CLI-container-安装并实例化-chaincode"><a href="#3-启动CLI-container-安装并实例化-chaincode" class="headerlink" title="3.启动CLI container,安装并实例化 chaincode"></a>3.启动CLI container,安装并实例化 chaincode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp&quot; cli peer chaincode install -n fabcar -v 1.0 -p &quot;$CC_SRC_PATH&quot; -l &quot;$LANGUAGE&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp&quot; cli peer chaincode instantiate -o orderer.example.com:7050 -C mychannel -n fabcar -l &quot;$LANGUAGE&quot; -v 1.0 -c &apos;&#123;&quot;Args&quot;:[&quot;&quot;]&#125;&apos; -P &quot;OR (&apos;Org1MSP.member&apos;,&apos;Org2MSP.member&apos;)&quot;</span><br></pre></td></tr></table></figure><h3 id="4-调用initLedger函数在-ledger-上生成10个cars"><a href="#4-调用initLedger函数在-ledger-上生成10个cars" class="headerlink" title="4.调用initLedger函数在 ledger 上生成10个cars"></a>4.调用initLedger函数在 ledger 上生成10个cars</h3><p>initLedger 函数定义在fabric-samples/chaincode/fabcar/go/fabcar.go 中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -e &quot;CORE_PEER_LOCALMSPID=Org1MSP&quot; -e &quot;CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp&quot; cli peer chaincode invoke -o orderer.example.com:7050 -C mychannel -n fabcar -c &apos;&#123;&quot;function&quot;:&quot;initLedger&quot;,&quot;Args&quot;:[&quot;&quot;]&#125;&apos;</span><br></pre></td></tr></table></figure><h2 id="enrollAdmin-js做的事情"><a href="#enrollAdmin-js做的事情" class="headerlink" title="enrollAdmin.js做的事情:"></a>enrollAdmin.js做的事情:</h2><p>注册 admin 用户</p><h2 id="registerUser-js做的事情"><a href="#registerUser-js做的事情" class="headerlink" title="registerUser.js做的事情:"></a>registerUser.js做的事情:</h2><p>使用 admin 用户注册和登记用户 user1</p><h2 id="query-js-做的事情"><a href="#query-js-做的事情" class="headerlink" title="query.js 做的事情:"></a>query.js 做的事情:</h2><p> 查询 ledger</p><h2 id="invoke-js做的事情"><a href="#invoke-js做的事情" class="headerlink" title="invoke.js做的事情:"></a>invoke.js做的事情:</h2><p>更新 ledger</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文接着上次创建第一个 Fabric 应用,简单介绍一下流程中各个文件的作用.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
      <category term="Fabric" scheme="http://yoursite.com/tags/Fabric/"/>
    
  </entry>
  
  <entry>
    <title>创建第一个Fabric应用</title>
    <link href="http://yoursite.com/2018/10/08/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFabric%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2018/10/08/创建第一个Fabric应用/</id>
    <published>2018-10-08T06:15:06.000Z</published>
    <updated>2018-11-29T12:42:18.469Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上创建了第一个Fabric应用fabcar。<br><a id="more"></a></p><h2 id="设置开发环境"><a href="#设置开发环境" class="headerlink" title="设置开发环境"></a>设置开发环境</h2><p>首先配置开发环境，下载fabric-samples代码以及镜像文件（与搭建网络操作一样）。<br>进入facar目录，有以下文件：<br><img src="/2018/10/08/创建第一个Fabric应用/facar1.png" alt="facar1"><br>在开始之前，我们先做些清理工作。执行以下命令以关闭旧的或者启动着的容器：<br><code>$ docker rm -f $(docker ps -aq)</code><br>清除网络中的缓存：<br><code>$ docker network prune</code></p><h2 id="安装客户端-amp-启动网络"><a href="#安装客户端-amp-启动网络" class="headerlink" title="安装客户端&amp;启动网络"></a>安装客户端&amp;启动网络</h2><p><strong>接下来的操作都在facar目录下</strong><br>执行以下命令为应用安装Fabric依赖。fabric-ca-client允许我们的app连接CA server并且检索身份材料。fabric-client允许我们获取身份材料，并且与节点和排序服务对话。<br><code>$ npm install</code><br>这一步在此处会有延迟，耐心等待一下：<br><img src="/2018/10/08/创建第一个Fabric应用/facar2.png" alt="facar2"><br>使用startFabric.sh脚本启动网络。这个命令会初始化各种Fabric实体，还会启动一个使用Golang编写的智能合约容器。<br><code>$ ./startFabric.sh</code><br>这个脚本做了以下事情(详细介绍在<a href="https://zkq1314.github.io/2018/10/10/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFabric%E5%BA%94%E7%94%A8%E4%B8%AD%E5%90%84%E6%96%87%E4%BB%B6%E4%BD%9C%E7%94%A8/#more" target="_blank" rel="noopener">这里</a>)：</p><ul><li>启动一个peer节点，order节点，证书和CLI容器</li><li>创建一个channel，并把peer添加到channel</li><li>安装智能合约（例如chaincode）到peer的文件系统，然后初始化chaincode到channel，实例化一个chaincode容器</li><li>调用initLedger方法来初始化10辆汽车到channel账本</li></ul><p>可以通过<code>$ docker ps</code>命令来查看当前脚本运行后的进程情况.</p><h2 id="登记管理员用户"><a href="#登记管理员用户" class="headerlink" title="登记管理员用户"></a>登记管理员用户</h2><p>当我们启动网络时，我们通过Certificate Authority注册了一个管理员用户admin。现在我们需要向CA服务器发送一个登记请求，然后为其取回一个登记证书。这里我们不深入登记的细节，只要知道这个证书是构成管理员用户的必要条件。我们随后会使用这个管理员来注册和登记新的用户。现在向CA服务器发送管理员登记请求：<br><code>$ node enrollAdmin.js</code></p><p><img src="/2018/10/08/创建第一个Fabric应用/facar3.png" alt="facar3"><br>这行代码会调用一个证书签名请求(CSR)，最后会在项目根目录生成一个新的文件夹<strong>hfc-key-stor</strong>e，里面包含了证书和密钥材料。当我们的app需要创建和读取不同身份用户时，需要定位到此文件夹。</p><h2 id="User1的注册和登记-Register-and-Enroll-user1"><a href="#User1的注册和登记-Register-and-Enroll-user1" class="headerlink" title="User1的注册和登记(Register and Enroll user1)"></a>User1的注册和登记(Register and Enroll user1)</h2><p>使用刚刚生成的管理员证书，我们再一次联通CA服务器来注册和登记一个新用户。user1是我们用来查询和更新账本的用户。这里着重说明的是，admin发起了新用户的注册和登记工作(就好像admin扮演了登记员的角色)。现在为admin发起登记和注册请求：<br><code>$ node registerUser.js</code></p><p><img src="/2018/10/08/创建第一个Fabric应用/facar4.png" alt="facar4"><br>和管理员登记一样，上面的指令会调用CSR然后将证书和密钥放入<strong>hfc-key-store</strong>文件夹中。现在我们有了两个用户的身份材料。</p><h2 id="查询账本-Querying-the-Ledger"><a href="#查询账本-Querying-the-Ledger" class="headerlink" title="查询账本(Querying the Ledger)"></a>查询账本(Querying the Ledger)</h2><p>查询是指如何从账本中读取数据。您可以查询单个或者多个键的值，如果账本是以类似于JSON这样的数据存储格式写入的，则可以执行更复杂的搜索（如查找包含某些关键字的所有资产）。</p><p>下图是一个查询流程的示意图：<br><img src="/2018/10/08/创建第一个Fabric应用/facar5.png" alt="facar5"><br>首先，运行query.js 程序<code>$ node query.js</code>，返回账本上所有汽车列表。我们使用user1作为签名实体。我们的程序中已经指定了user1为签名实体：<br><code>fabric_client.getUserContext(&#39;user1&#39;, true);</code></p><p>user1的登记材料已经放在了hfc-key-store文件夹中，我们只需要简单的告诉程序去获取它就行了。在定义了用户对象后，我们继续读取账本的流程。queryAllCars这个方法已经被提前定义在了app中，它可以查询所有的cars.返回应如下：<br><img src="/2018/10/08/创建第一个Fabric应用/facar6.png" alt="facar6"><br>这里有10辆车，一辆属于Adriana的黑色Tesla Model S、一辆属于Brad的红色Ford Mustang、一辆属于Pari的紫罗兰色Fiat Punto等等。账本是基于Key/Value 的，在这里，关键字是从CAR0到CAR9。</p><p>query.js中包括：</p><ol><li>初始部分定义了变量，如链码，通道名称和网络端点。在我们的app中，这些变量已经定义好了，但是在真实的开发中，这些变量应该又开发者指定。<br><img src="/2018/10/08/创建第一个Fabric应用/facar7.png" alt="facar7"></li><li>构建查询的代码块<br><img src="/2018/10/08/创建第一个Fabric应用/facar8.png" alt="facar8"><br>当程序运行时，它会调用节点上的fabcar链码，执行queryAllCars函数，不传任何参数。</li></ol><p>我们再转至到fabric-samples子目录chaincode/fabcar/go，并在编辑器中打开fabcar.go，查看queryAllCars函数是如何与账本进行交互的。<br><img src="/2018/10/08/创建第一个Fabric应用/facar9.png" alt="facar9"><br>这里定义了queryAllCars的范围。在CAR0和CAR999的每辆车。因此，我们理论上可以创建1,000辆汽车，queryAllCars函数将会显示出每一辆汽车的信息。</p><p>现在我们返回query.js程序并编辑请求构造函数以查询特定的车辆。为达此目的，我们将函数queryAllCars更改为queryCar并将特定的“Key” 传递给args参数。在这里，我们使用CAR4。 所以我们编辑后的query.js程序现在应该包含以下内容：<br><img src="/2018/10/08/创建第一个Fabric应用/facar10.png" alt="facar10"><br>再次执行程序得到一辆车的结果：<br><img src="/2018/10/08/创建第一个Fabric应用/facar11.png" alt="facar11"><br>这样，我们就从查询所有车变成了只查询一辆车：Adriana的黑色Tesla Model S。使用queryCar函数，我们可以查询任意关键字（例如CAR0），并获得与该车相对应的制造厂商、型号、颜色和所有者。</p><h2 id="更新账本-Updating-the-Ledger"><a href="#更新账本-Updating-the-Ledger" class="headerlink" title="更新账本(Updating the Ledger)"></a>更新账本(Updating the Ledger)</h2><p><img src="/2018/10/08/创建第一个Fabric应用/facar12.png" alt="facar12"></p><ol><li>账本更新是从生成交易提案的应用程序开始的。就像查询一样，我们将会构造一个请求，用来识别要进行交易的通道ID、函数以及智能合约。该程序然后调用channel.SendTransactionProposalAPI将交易建议发送给peer(s)进行认证。</li><li>网络（即endorsing peer）返回一个提案答复，应用程序以此来创建和签署交易请求。该请求通过调用channel.sendTransaction API发送到排序服务器。排序服务器将把交易打包进区块，然后将区块“发送”到通道上的所有peers进行认证。（在我们的例子中，我们只有一个endorsing peer。）</li><li>最后，应用程序使用eh.setPeerAddr API连接到peer的事务监听端口，并调用eh.registerTxEvent注册与特定交易ID相关联的事务。该API使得应用程序获得事务的结果（即成功提交或不成功）。把它当作一个通知机制。<br>我们初始调用的目标是简单地创建一个新的汽车。我们有一个独立的用于这些交易的JavaScript程序 - invoke.js。就像查询一样，使用编辑器打开程序并转到构建调用的代码块：<br><img src="/2018/10/08/创建第一个Fabric应用/facar13.png" alt="facar13"><br>我们可以调用函数createCar或者changeCarOwner。首先我们创建一个红色的Chevy Volt，并把它归属于Nick。在账本中我们的Key值已经用到了CAR9 ，所以这里我们将使用CAR10。更新代码块如下：<br><img src="/2018/10/08/创建第一个Fabric应用/facar14.png" alt="facar14"><br>保存并运行程序，得到输出：<br><img src="/2018/10/08/创建第一个Fabric应用/facar15.png" alt="facar15"><br>回到query.js，然后修改参数CAR4为CAR10。<br><img src="/2018/10/08/创建第一个Fabric应用/facar16.png" alt="facar16"><br>得到以上输出说明我们CAR10创建成功。<br>最后，我们来调用最后一个函数changeCarOwner。Nick很慷慨，他想把他的Chevy Volt送给Dave。所以，我们简单编辑invoke.js 如下：<br><img src="/2018/10/08/创建第一个Fabric应用/facar17.png" alt="facar17"><br>第一个参数定义了哪辆车被变更主人。第二个参数定义了新主人姓名。<br>保存并执行，得到输出结果：<br><img src="/2018/10/08/创建第一个Fabric应用/facar18.png" alt="facar18"><br><img src="/2018/10/08/创建第一个Fabric应用/facar19.png" alt="facar19"><br>真实情况下，链码需要权限控制。例如只有某些具有权限的人才能创造新车，也应该只有车主才能转让汽车所有权。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上创建了第一个Fabric应用fabcar。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
      <category term="Fabric" scheme="http://yoursite.com/tags/Fabric/"/>
    
  </entry>
  
  <entry>
    <title>Hyperledger fabric Building Your First Network on MacOS</title>
    <link href="http://yoursite.com/2018/09/30/Hyperledger-fabric-Building-Your-First-Network-on-MacOS/"/>
    <id>http://yoursite.com/2018/09/30/Hyperledger-fabric-Building-Your-First-Network-on-MacOS/</id>
    <published>2018-09-30T03:26:04.000Z</published>
    <updated>2018-11-29T12:37:54.095Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上实现了Building Your First Network。<br><a id="more"></a></p><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><p>按照官网的要求<a href="https://hyperledger-fabric.readthedocs.io/en/latest/prereqs.html" target="_blank" rel="noopener">Prerequisites</a>，需要安装docker，Go语言以及Node等。本机配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker --version</span><br><span class="line">Docker version 18.06.1-ce, build e68fc7a</span><br><span class="line">$ docker-compose --version</span><br><span class="line">docker-compose version 1.22.0, build f46880f</span><br><span class="line">$ go version</span><br><span class="line">go version go1.11 darwin/amd64</span><br><span class="line">$ node --version</span><br><span class="line">v10.8.0</span><br><span class="line">$ npm --version</span><br><span class="line">6.2.0</span><br></pre></td></tr></table></figure><p>其中，</p><ul><li>docker我下载的安装包安装的，安装过程中也安装了docker-compose.</li><li><p>go是通过brew install go安装的，记得添加环境变量。<br><code>vim ~/.bash_profile</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export GOPATH=$HOME/go</span><br><span class="line">export PATH=$PATH:$GOPATH/bin</span><br></pre></td></tr></table></figure></li><li><p>node是下载的pkg安装的。</p></li></ul><h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>工具都准备好之后，接下来开始下载镜像，把这个<a href="https://raw.githubusercontent.com/hyperledger/fabric/v1.0.5/scripts/bootstrap.sh" target="_blank" rel="noopener">页面</a>下的内容全部保存到新建的images.sh文件中。</p><p>因为我们使用的是1.1.0版本，把其中的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export VERSION=$&#123;1:-1.0.5&#125;</span><br></pre></td></tr></table></figure><p>改为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export VERSION=$&#123;1:-1.1.0-preview&#125;</span><br></pre></td></tr></table></figure><p>然后执行，下载镜像（用华严的网一直无法下载，vpn也开了，晚上回到学校用的教育网下载没问题）：</p><p><code>sh images.sh</code></p><p>下载好之后，用以下命令来查看镜像是否都下载成功：</p><p><code>docker images</code><br><img src="/2018/09/30/Hyperledger-fabric-Building-Your-First-Network-on-MacOS/byfn1.png" alt="byfn1"></p><h2 id="下载Fabric-Samples源码"><a href="#下载Fabric-Samples源码" class="headerlink" title="下载Fabric-Samples源码"></a>下载Fabric-Samples源码</h2><p>从<a href="https://github.com/hyperledger/fabric-samples" target="_blank" rel="noopener">此处</a>下载。</p><h2 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h2><p><code>cd first-network</code><br><code>./byfn.sh generate</code><br><code>./byfn.sh up</code><br><img src="/2018/09/30/Hyperledger-fabric-Building-Your-First-Network-on-MacOS/byfn2.png" alt="byfn2"><br>出现以下画面说明示例运行成功。<br><code>./byfn.sh down</code></p><h2 id="examples-e2e-cli"><a href="#examples-e2e-cli" class="headerlink" title="examples/e2e_cli"></a>examples/e2e_cli</h2><p>参考 <a href="https://www.jianshu.com/p/e108cf655c0f" target="_blank" rel="noopener">https://www.jianshu.com/p/e108cf655c0f</a><br>从git上拉取Hyperledger Fabric:<br><code>git clone git@github.com:hyperledger/fabric.git</code><br>进入项目文件夹，查看 tag：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">git tag</span><br><span class="line">baseimage-v0.0.11</span><br><span class="line">v0.6.0-preview</span><br><span class="line">v0.6.1-preview</span><br><span class="line">v1.0.0</span><br><span class="line">v1.0.0-alpha</span><br><span class="line">v1.0.0-alpha2</span><br><span class="line">v1.0.0-beta</span><br><span class="line">v1.0.0-rc1</span><br><span class="line">v1.0.1</span><br><span class="line">v1.0.2</span><br><span class="line">v1.0.3</span><br><span class="line">v1.0.4</span><br><span class="line">v1.0.5</span><br><span class="line">v1.0.6</span><br><span class="line">v1.1.0</span><br><span class="line">v1.1.0-alpha</span><br><span class="line">v1.1.0-preview</span><br><span class="line">v1.1.0-rc1</span><br></pre></td></tr></table></figure><p>上面的tag表示相应的fabric项目的版本， fabric项目现在还处于早期发展阶段， 修改频繁且不一定能向下兼容， 所以在继续之前请先确定一个版本，避免后面踩坑。 笔者在这里使用v1.0.0。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git checkout v1.0.0</span><br><span class="line">git branch</span><br><span class="line">* (HEAD detached at v1.0.0)</span><br><span class="line">release-1.1</span><br></pre></td></tr></table></figure><h2 id="启动项目"><a href="#启动项目" class="headerlink" title="启动项目"></a>启动项目</h2><p>进入fabric文件夹下的examples/e2e_cli, 下面我们要测试e2e_cli这个demo。<br>执行download-dockerimage.sh，程序将会通过docker拉取项目所需镜像, 为了统一版本，请指定拉取镜像的版本号:</p><p><code>chmod +x download-dockerimages.sh</code><br><code>./download-dockerimages.sh -c x86_64-1.0.0 -f x86_64-1.0.0</code></p><p>现在执行完整脚本:</p><p><code>./network_setup.sh up &lt;channel-ID&gt;</code></p><p>执行成功后使用以下命令终止网络：</p><p><code>./network_setup.sh down</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于 Hyperledger fabric 的官方文档和网上较新的博客，在一台MacOS上实现了Building Your First Network。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>Hyperledger fabric v1.0.0 环境部署过程</title>
    <link href="http://yoursite.com/2018/09/27/Hyperledger-fabric-v1-0-0-%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/</id>
    <published>2018-09-27T02:23:36.000Z</published>
    <updated>2018-11-29T14:51:07.298Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要基于 Hyperledger fabric 的官方文档来搭建其实验环境，但官方文档对于很多步骤都有省略，结合网上比较新的博客，在一台 Ubuntu 14.04 机器(没用通过测试，换成了16.04的虚拟机)上来安装部署 fabric 的环境。<br><a id="more"></a><br>查看系统版本<br><code>$ cat /proc/version</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署1.png" alt="Fabric环境部署1"><br>切换至root用户<br><code>sudo -i</code></p><h2 id="更换-apt-源"><a href="#更换-apt-源" class="headerlink" title="更换 apt 源"></a>更换 apt 源</h2><p>先备份 sources.list 文件：<br><code>$ sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</code><br>再修改 sources.list 文件(全部替换即可)，换成阿里云的国内源：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/apt/sources.list</span><br><span class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class="line">deb http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</span><br><span class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</span><br><span class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</span><br></pre></td></tr></table></figure></p><p>最后更新一下源：<br><code>$ sudo apt-get update</code></p><h2 id="安装-curl"><a href="#安装-curl" class="headerlink" title="安装 curl"></a>安装 curl</h2><p>Ubuntu一般默认是安装了 curl 的，可以通过以下命令验证：</p><p><code>$ curl -V</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署2.png" alt="Fabric环境部署2"></p><p>如果没有安装，则通过 apt-get 安装：</p><p><code>$ sudo apt-get install curl</code></p><h2 id="安装-Docker"><a href="#安装-Docker" class="headerlink" title="安装 Docker"></a>安装 Docker</h2><ul><li><p>由于 apt 源使用HTTPS以确保软件下载过程中不被篡改。因此，我们首先需要添加使用HTTPS传输的软件包以及CA证书。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    software-properties-common</span><br></pre></td></tr></table></figure></li><li><p>为了确认所下载软件包的合法性，需要添加软件源的 GPG 秘钥<br><code>$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</code></p></li><li><p>然后，我们需要向 sources.list 中添加 Docker 软件源</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo add-apt-repository \</span><br><span class="line">    &quot;deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \</span><br><span class="line">    $(lsb_release -cs) \</span><br><span class="line">    stable&quot;</span><br></pre></td></tr></table></figure><p>  以上命令会添加稳定版本的Docker CE apt 镜像源。</p></li><li><p>更新 apt 软件包缓存，并安装 docker-ce：</p><p>  <code>$ sudo apt-get update</code><br>  <code>$ sudo apt-get install docker-ce</code></p></li><li><p>查看 Docker 版本：</p><p>  <code>$ docker -v</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署3.png" alt="Fabric环境部署3"></p><p>  满足官方文档中 Docker version 17.06.2-ce or greater is required 的要求。</p></li><li><p>启动 Docker CE<br>Ubuntu 16.04 使用：<br><code>$ sudo systemctl enable docker</code><br><code>$ sudo systemctl start docker</code></p><p>  Ubuntu 14.04 使用：<br><code>$service docker start</code></p></li><li><p>建立 docker 用户组</p><p>  默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。出于安全考虑，一般 Linux 系统上不会直接使用 root 用户。因此，更好地做法是将需要使用 docker 的用户加入 docker 用户组。<br>  <code>$ sudo groupadd docker</code><br>其实一般按照上面的方法安装 Docker 后就已经创建好 docker 用户组了，可以使用 <code>$ cat /etc/group | grep docker</code> 命令来验证，所以就不需要再建立 docker 用户组了，再建立也会报错提示用户组已存在的。</p></li><li><p>将当前用户加入 docker 用户组：</p><p>  <code>$ sudo usermod -aG docker $USER</code><br>下次登录时即可方便的使用 docker 命令。</p></li><li><p>测试 Docker 是否安装正确</p><p>  <code>$ docker run hello-world</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署4.png" alt="Fabric环境部署4"></p></li><li><p>配置镜像加速器</p><p>  国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker 官方和国内很多云服务商都提供了国内加速器服务，例如：</p><ol><li>Docker 官方提供的中国 registry mirror <a href="https://registry.docker-cn.com" target="_blank" rel="noopener">https://registry.docker-cn.com</a></li><li>七牛云加速器 <a href="https://reg-mirror.qiniu.com/" target="_blank" rel="noopener">https://reg-mirror.qiniu.com/</a></li></ol></li></ul><pre><code>当配置某一个加速器地址之后，若发现拉取不到镜像，请切换到另一个加速器地址。国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务。我们以 Docker 官方加速器 https://registry.docker-cn.com 为例进行介绍。在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在则创建）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    &#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [</span><br><span class="line">    &quot;https://registry.docker-cn.com&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>之后重新启动服务</code></pre><p>Ubuntu16.04:<br><code>$ sudo systemctl daemon-reload</code><br><code>$ sudo systemctl restart docker</code><br>Ubuntu14.04：<br><code>$ service docker restart</code></p><h2 id="安装-Docker-Compose"><a href="#安装-Docker-Compose" class="headerlink" title="安装 Docker Compose"></a>安装 Docker Compose</h2><p>通过二进制包来安装，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。</p><pre><code>`$ sudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose``$ sudo chmod +x /usr/local/bin/docker-compose`</code></pre><ul><li><p>查看 Docker compose 版本</p><p>  <code>$ docker-compose --version</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署5.png" alt="Fabric环境部署5"></p><p>  满足官方文档中 Docker Compose version 1.14.0 or greater 的要求。</p></li></ul><h2 id="安装-Go-语言环境"><a href="#安装-Go-语言环境" class="headerlink" title="安装 Go 语言环境"></a>安装 Go 语言环境</h2><p>Hyperledger Fabric 在很多组件中使用了 Go 语言，并且 Hyperledger fabric 1.2.0 要求使用的是 GO version 1.10.x ，所以需要在我们的环境中安装对应的 Go 语言。</p><ul><li><p>从官网下载 1.10.x 版本的 Linux 平台的源码包</p><p>  <code>$ wget https://dl.google.com/go/go1.10.3.linux-amd64.tar.gz</code></p></li><li><p>解压到指定目录</p><p>  <code>$ sudo tar zxvf go1.10.3.linux-amd64.tar.gz -C /usr/local/</code></p></li><li><p>先创建 Go 的工作目录</p><p>  <code>$ mkdir ~/go</code></p></li><li><p>配置环境变量</p><p>  <code>$ vi ~/.bashrc</code><br>添加</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export GOROOT=/usr/local/go</span><br><span class="line">export GOPATH=/home/zkq/go</span><br><span class="line">export PATH=$PATH:$GOROOT/bin:$GOPATH/bin</span><br></pre></td></tr></table></figure><p>  保存并使生效：</p><p>  <code>$ source ~/.bashrc</code></p></li><li><p>测试 Go 的 demo 程序</p><p>  <code>$ cd ~/go</code></p><p>  <code>$ vi hello.go</code></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import &quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">fmt.Printf(&quot;hello world\n&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  <code>$ go build hello.go</code></p><p>  <code>$ ./hello</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署6.png" alt="Fabric环境部署6"></p><p>  在这里遇到一个问题（在虚拟机上没遇到这个问题）：<br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署7.png" alt="Fabric环境部署7"><br>在网上查到的解决办法是：<a href="https://stackoverflow.com/questions/21510714/go-cannot-find-package-fmt-error" target="_blank" rel="noopener">解决办法</a><br><code>$ unset GOROOT</code></p></li></ul><h2 id="Fabric-源码下载"><a href="#Fabric-源码下载" class="headerlink" title="Fabric 源码下载"></a>Fabric 源码下载</h2><ul><li><p>首先创建存放源码的文件夹：</p><p>  <code>$ mkdir -p ~/go/src/github.com/hyperledger</code></p></li><li>进入刚创建的目录：<br><code>$cd ~/go/src/github.com/hyperledger</code></li><li><p>使用 Git 下载完整源码（有点慢）：</p><p>  <code>$ git clone https://github.com/hyperledger/fabric.git</code></p></li><li><p>进入 fabric 目录查看版本分支并切换分支：</p><p>  <code>$ cd fabric</code><br><code>$ git branch</code></p></li><li>release-1.2<br><code>$ git checkout v1.0.0</code><br>由于在 release-1.2 版本中碰到没有解决的问题，所以先切换到 v1.0.0 来完成搭建并测试的过程。</li></ul><p><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署8.png" alt="Fabric环境部署8"></p><h2 id="Fabric-Docker-镜像下载"><a href="#Fabric-Docker-镜像下载" class="headerlink" title="Fabric Docker 镜像下载"></a>Fabric Docker 镜像下载</h2><p>进入 ~/go/src/github.com/hyperledger/fabrci/examples/e2e_cli/ 目录，完成镜像下载，执行命令：</p><p><code>$ cd ~/go/src/github.com/hyperledger/fabrci/examples/e2e_cli/</code><br><code>$ ls</code><br><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署9.png" alt="Fabric环境部署9"></p><p><code>$ source download-dockerimages.sh -c x86_64-1.0.0 -f x86_64-1.0.0</code>  (时间较久，耐心等待)<br><code>$ docker image list</code></p><p><img src="/2018/09/27/Hyperledger-fabric-v1-0-0-环境部署过程/Fabric环境部署10.png" alt="Fabric环境部署10"></p><h2 id="启动-fabric-网络并完成-chaincode-测试"><a href="#启动-fabric-网络并完成-chaincode-测试" class="headerlink" title="启动 fabric 网络并完成 chaincode 测试"></a>启动 fabric 网络并完成 chaincode 测试</h2><p>还是在刚刚的 e2e_cli 文件加下，执行：</p><p><code>$ ./network_setup.sh up</code></p><p>最后出现上面字符说明 fabric 网络已经启动并完成了 chaincode 的测试。这一步没有成功，所以上面图片暂时没有。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要基于 Hyperledger fabric 的官方文档来搭建其实验环境，但官方文档对于很多步骤都有省略，结合网上比较新的博客，在一台 Ubuntu 14.04 机器(没用通过测试，换成了16.04的虚拟机)上来安装部署 fabric 的环境。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>HyperLedger Fabric架构篇</title>
    <link href="http://yoursite.com/2018/09/26/HyperLedger-Fabric%E6%9E%B6%E6%9E%84%E7%AF%87/"/>
    <id>http://yoursite.com/2018/09/26/HyperLedger-Fabric架构篇/</id>
    <published>2018-09-26T01:41:09.000Z</published>
    <updated>2018-11-29T14:53:01.825Z</updated>
    
    <content type="html"><![CDATA[<p>调研了一下HyperLedger Fabric的架构，因为它的官网文档并没有及时更新，所以有的内容参考的还是以前版本的文档。<br><a id="more"></a></p><h2 id="chaincode-可以用Go-java开发"><a href="#chaincode-可以用Go-java开发" class="headerlink" title="chaincode(可以用Go,java开发)"></a>chaincode(可以用Go,java开发)</h2><p><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric1.png" alt="Fabric1"></p><h2 id="Ledger"><a href="#Ledger" class="headerlink" title="Ledger"></a>Ledger</h2><p><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric2.png" alt="Fabric2"><br>整个区块结构分为文件系统存储的<strong>Block结构</strong>和数据库维护的<strong>State状态</strong>，其中state的存储结构是可以替换的，可选的实现包括各种KV数据库（LevelDB，CouchDB等）</p><h2 id="node有三种："><a href="#node有三种：" class="headerlink" title="node有三种："></a>node有三种：</h2><ul><li>client或submitting-client：负责向背书人（endorsers）提交实际的事务调用，并将交易提议广播到订阅服务</li><li>peer：负责提交交易，维持state，有的peer还担任endorser（审查交易合法性，合法则签名）</li><li>Ordering-service-node or orderer: 负责通信服务</li></ul><h2 id="交易的基本工作流程"><a href="#交易的基本工作流程" class="headerlink" title="交易的基本工作流程"></a>交易的基本工作流程</h2><ol><li>client创建一个transaction，并根据endorse policy发送给某些endorser peers</li><li>peers模拟transaction，并产生认可签名</li><li>client收集到足够的peers对transaction的认可，并交给ordering service，orderer会汇总各client递交过来的transaction交易，排序、打包</li><li>orderer将交易打包成区块block，然后通知所有commit peer，各peer各自验证结果，最后将区块block记录到自己的ledger账本中</li></ol><p>如以下例子所示：<br><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric8.png" alt="Fabric8"></p><ol><li>客户端向其中1个Peer提交Transaction。注意，这里的Client是真正的客户端。而这里的submiting peer，你可以认为是fabric网络的客户端（对客户端来说，这个submiting peer是服务器）</li><li><p>submiting peer把请求给多个endoser<strong>模拟执行</strong>，然后从每个endoser收集返回结果。也就是图中的步骤 圆圈2,3。在Fabric里面，把这个“模拟执行”的过程，称为“背书”，endoser。 至所以这么叫，是因为如果模拟执行不通过的话，这个Transaction就直接被拒绝了。模拟执行通过，才有机会进入下面的过程，才有机会被区块链网络接受。 所以，“背书”，通俗点讲，就是“告诉整个区块链网络，这个Transaction被我模拟执行过了，有效的，你们要不也试试看？”<br>关键点：<br>发给几个endoser取决于你的endose poclicy怎么配置的，你配置成只有1个endoser也可以。 注意这里只是“模拟执行”。endoser并不会把结果直接写到worldState里面。</p></li><li><p>submitting peer 把这个Transaction + 模拟执行的结果发给Ordering Service。也就是图中的步骤 圆圈 4。<br>关键点：因为有很多个Client往不同的submit peer发送请求，所以这个Ordering Service会收到多笔的Transaction。Ordering Service会对这些Transaction进行打包，形成Block。</p></li><li>Ordering Service再把这个Block广播给每个Peer(此时，Peer充当了另外1个角色，不是endorser，而是Committer)。所有Committer接受到这个Block，真正写入：把交易加入区块链，同时更新WorldState，也就是分布式账本(Ledger)</li><li>给Client发送Event，通知其交易已被执行。</li></ol><h2 id="构建企业级区块链的要素"><a href="#构建企业级区块链的要素" class="headerlink" title="构建企业级区块链的要素"></a>构建企业级区块链的要素</h2><p><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric4.png" alt="Fabric4"></p><h2 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h2><p><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric5.png" alt="Fabric5"><br>竖着来看，分为3大块： </p><ol><li>最左边的，Memship，就是联盟链特有的身份认证相关的内容。比如组织注册，公钥证书发放，交易签名，验证等等。 </li><li>中间1块：区块链的基础东西，和比特币、以太坊类似，BlockChain、Transaction、Ledger(分布式账本）、P2P协议。</li><li>最右边1块：智能合约。在以太坊里面，称为Smart Contract，这里换了个名字，叫做ChainCode而已。</li></ol><p>0.6运行时架构与1.0运行时架构：<br><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric6.png" alt="Fabric6"><br><img src="/2018/09/26/HyperLedger-Fabric架构篇/Fabric7.png" alt="Fabric7"><br>下面这张图，展示了Fabric运行时的架构是什么样子。Fabric采用Go语言开发，通常运行在Docker容器里面，每1个Node对应1个Docker容器。</p><ol><li>这里的application/SDK，并不是指我们通常的APP的客户端。而是整个Fabric网络的调用端，通常是我们的Web应用服务器。 </li><li>membership服务，就是上图中的注册登记、身份认证。</li><li>peer就是区块链网络中说的物理上的node。peer有2个角色（对应2个模块），1个叫Endorser，1个叫Committer。peer上面存的是Ledger(区块链 + WorldState)，存的智能合约ChainCode，然后peer之间、peer和客户端之间，用event通信。 </li><li>Order-Service 排序服务。</li></ol><p>Fabric相对于<strong>以太坊</strong>，大部分其实很类似（比如peer，peer上面的账本、worldState）。多了2个东西出来，1个是membership（用于注册、身份认证），1个是order service，用于共识算法。</p><p>0.6版本和1.0版本运行时架构对比：</p><ul><li>分拆Peer的功能，将Blockchain的数据维护和共识服务进行分离，共识服务从Peer节点中完全分离出来，独立为Orderer节点提供共识服务；</li><li>基于新的架构，实现多通道（channel）的结构，实现了更为灵活的业务适应性（业务隔离、安全性等方面）</li><li>支持更强的配置功能和策略管理功能，进一步增强系统的灵活性和适应性；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;调研了一下HyperLedger Fabric的架构，因为它的官网文档并没有及时更新，所以有的内容参考的还是以前版本的文档。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>2018.08.22收获与总结</title>
    <link href="http://yoursite.com/2018/08/22/2018-08-22%E6%94%B6%E8%8E%B7%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/22/2018-08-22收获与总结/</id>
    <published>2018-08-22T13:52:05.000Z</published>
    <updated>2018-08-22T14:06:55.047Z</updated>
    
    <content type="html"><![CDATA[<p>今天处理了一下THUC的新闻数据集，具体代码在下面。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>sklearn里的shuffle可以将数据打乱，我在之前处理南开数据集的时候忽视了这一点。同样，pandas中的sample()也是同样的作用，numpy库中的方法不推荐，会导致内存溢出。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>离开实验室的时候跑了TextCNN的模型，结果到家看了一下结果，从第九个epoch开始准确率都为1（一共15个epochs）。一开始我纳闷为啥准确率这么高，因为在复旦数据集上也就0.86左右的准确率。后来看了一下下面的代码。最后两行是我生成训练集和测试集的方法，仔细看知道了测试集就是训练集的一个子集！怪不得准确率这么高，因为已经告诉你label了啊！明天重新生成一下训练集和测试集。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_to_mini</span><span class="params">(dirname, targetname, per_class_max_docs=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理清华大学语料库，将类别和文档处理成1个txt</span></span><br><span class="line"><span class="string">    dirname:原始路径</span></span><br><span class="line"><span class="string">    targetname:保存路径</span></span><br><span class="line"><span class="string">    per_class_max_docs:每类文档保留的文档数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># f_train = io.open(targetname, 'w', encoding='utf-8')</span></span><br><span class="line">    labels = []</span><br><span class="line">    contents = []</span><br><span class="line">    ids = []</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(dirname):</span><br><span class="line">        print(<span class="string">'path error'</span>)</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> os.listdir(dirname):  <span class="comment"># 分类目录</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        cat_dir = os.path.join(dirname, category)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(cat_dir):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        files = os.listdir(cat_dir)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> cur_file <span class="keyword">in</span> files: <span class="comment">#具体文件</span></span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count &gt; per_class_max_docs:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            filename = os.path.join(cat_dir, cur_file)</span><br><span class="line">            <span class="keyword">with</span> io.open(filename, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                content = f.read().replace(<span class="string">'\n'</span>, <span class="string">''</span>).replace(<span class="string">'\t'</span>, <span class="string">''</span>).replace(<span class="string">'\u3000'</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="comment"># content = is_ustr(content)</span></span><br><span class="line">                <span class="comment"># line_content = category + '\t' + content + '\t' _ str(count) + '\n'</span></span><br><span class="line">                <span class="comment"># f_train.write(category + '\t' + content + '\t' + str(count) + '\n')</span></span><br><span class="line">            labels.append(label_dict[category])</span><br><span class="line">            contents.append(content.replace(<span class="string">'\n'</span>, <span class="string">''</span>))</span><br><span class="line">            ids.append(str(count))</span><br><span class="line">        print(<span class="string">'Finished:'</span>, category)</span><br><span class="line">    <span class="comment"># train_len=int(len(ids)*0.8)</span></span><br><span class="line">    <span class="comment"># train_df = pd.DataFrame(&#123;'label': labels[:train_len], 'content': contents[:train_len], 'id': ids[:train_len]&#125;)</span></span><br><span class="line">        val_df = pd.DataFrame(&#123;<span class="string">'label'</span>: labels[:], <span class="string">'content'</span>: contents[:], <span class="string">'id'</span>: ids[:]&#125;)</span><br><span class="line">    <span class="comment"># train_df.to_csv("/home/zkq/data/small_train.csv", index=False, sep='\t')</span></span><br><span class="line">        val_df = shuffle(val_df) <span class="comment">#打乱每行顺序</span></span><br><span class="line">        val_df.to_csv(targetname , index=<span class="keyword">False</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">'Finished Tran'</span>)</span><br><span class="line">    <span class="comment"># f_train.close()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    label_dict = &#123;<span class="string">'星座'</span>: <span class="string">'0'</span>, <span class="string">'股票'</span>: <span class="string">'1'</span>, <span class="string">'房产'</span>: <span class="string">'2'</span>, <span class="string">'时尚'</span>: <span class="string">'3'</span>, <span class="string">'体育'</span>: <span class="string">'4'</span> , <span class="string">'社会'</span>:<span class="string">'5'</span>,<span class="string">'家居'</span>:<span class="string">'6'</span>,<span class="string">'游戏'</span>:<span class="string">'7'</span>,<span class="string">'彩票'</span>:<span class="string">'8'</span>,<span class="string">'科技'</span>:<span class="string">'9'</span>,<span class="string">'教育'</span>:<span class="string">'10'</span>,<span class="string">'时政'</span>:<span class="string">'11'</span>,<span class="string">'娱乐'</span>:<span class="string">'12'</span>,<span class="string">'财经'</span>:<span class="string">'13'</span>&#125;</span><br><span class="line">    <span class="comment"># category = ['星座', '股票', '房产', '时尚', '体育', '社会', '家居', '游戏', '彩票', '科技', '教育', '时政', '娱乐', '财经']</span></span><br><span class="line">    <span class="comment">#合并为一个文件</span></span><br><span class="line">    <span class="comment"># corpus = load_data_to_mini('/home/zkq/data/THUCNews', '/home/zkq/data/thuc_train.csv', 1000)</span></span><br><span class="line">    corpus = load_data_to_mini(<span class="string">'/home/zkq/data/THUCNews'</span>, <span class="string">'/home/zkq/data/thuc_val.csv'</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天处理了一下THUC的新闻数据集，具体代码在下面。&lt;/p&gt;
&lt;h2 id=&quot;收获&quot;&gt;&lt;a href=&quot;#收获&quot; class=&quot;headerlink&quot; title=&quot;收获&quot;&gt;&lt;/a&gt;收获&lt;/h2&gt;&lt;p&gt;sklearn里的shuffle可以将数据打乱，我在之前处理南开数据集的时候忽视了这一点。同样，pandas中的sample()也是同样的作用，numpy库中的方法不推荐，会导致内存溢出。&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h2&gt;&lt;p&gt;离开实验室的时候跑了TextCNN的模型，结果到家看了一下结果，从第九个epoch开始准确率都为1（一共15个epochs）。一开始我纳闷为啥准确率这么高，因为在复旦数据集上也就0.86左右的准确率。后来看了一下下面的代码。最后两行是我生成训练集和测试集的方法，仔细看知道了测试集就是训练集的一个子集！怪不得准确率这么高，因为已经告诉你label了啊！明天重新生成一下训练集和测试集。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>scipy.stats.spearmanr用法</title>
    <link href="http://yoursite.com/2018/08/13/scipy-stats-spearmanr%E7%94%A8%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/13/scipy-stats-spearmanr用法/</id>
    <published>2018-08-13T08:26:00.000Z</published>
    <updated>2018-08-13T08:28:06.862Z</updated>
    
    <content type="html"><![CDATA[<p>计算<strong>Spearman秩相关系数</strong>和<strong>P值</strong>（非相关性检验）。<br>在计算word similarity时用到的。具体用法是数据集中每行有一对词和人工标注的相关性，如(李白 诗 9.2)。程序先从词向量中读取两个词的向量，求得两个向量的余弦相似性，再用spearmanr求得相关系数和P值。最后的实验分析用的是相关系数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;计算&lt;strong&gt;Spearman秩相关系数&lt;/strong&gt;和&lt;strong&gt;P值&lt;/strong&gt;（非相关性检验）。&lt;br&gt;在计算word similarity时用到的。具体用法是数据集中每行有一对词和人工标注的相关性，如(李白 诗 9.2)。程序先从词向量中读取两个
      
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>keras中的embedding层</title>
    <link href="http://yoursite.com/2018/08/08/keras%E4%B8%AD%E7%9A%84embedding%E5%B1%82/"/>
    <id>http://yoursite.com/2018/08/08/keras中的embedding层/</id>
    <published>2018-08-08T01:23:20.000Z</published>
    <updated>2018-08-08T02:04:32.804Z</updated>
    
    <content type="html"><![CDATA[<h2 id="keras中的Embedding层"><a href="#keras中的Embedding层" class="headerlink" title="keras中的Embedding层"></a>keras中的Embedding层</h2><p>将索引映射为固定维度的稠密向量，如[[4],[20]]-&gt;[[0.25,0.1],[0.6,-0.2]]。<br>Embedding层只能作为模型的第一层。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><ol><li>从头训练<br>就像word2vec一样, 这一层是可学习的, 用随机数initialize , 通过BP去调整.</li><li>pre-trained + fine tuning<br>用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 然后继续学习.</li><li>pre-trained + static<br>用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 并且这些参数保持固定, 不参与网络的学习.<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer=<span class="string">'uniform'</span>, embeddings_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, embeddings_constraint=<span class="keyword">None</span>, mask_zero=<span class="keyword">False</span>, input_length=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></li></ol><p>input_dim：大或等于0的整数，字典长度（词汇量），即输入数据最大下标+1<br>output_dim：大于0的整数，代表词向量的维度<br>embeddings_initializer: 初始化方法<br>embeddings_regularizer: 嵌入矩阵的正则项，为Regularizer对象<br>embeddings_constraint: 嵌入矩阵的约束项，为Constraints对象<br>mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为$|vocabulary| + 1$。<br>input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;keras中的Embedding层&quot;&gt;&lt;a href=&quot;#keras中的Embedding层&quot; class=&quot;headerlink&quot; title=&quot;keras中的Embedding层&quot;&gt;&lt;/a&gt;keras中的Embedding层&lt;/h2&gt;&lt;p&gt;将索引映射为固定维度
      
    
    </summary>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>文本分类综述</title>
    <link href="http://yoursite.com/2018/08/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/08/03/文本分类综述/</id>
    <published>2018-08-03T03:58:38.609Z</published>
    <updated>2018-11-29T14:59:20.918Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了神经网络在文本分类方面的一些应用，其中多篇论文被广泛引用。如果有最新的相关研究会及时更新。<br>文章将按照这些论文提出年份展开介绍，发展历史如下图所示：<br><img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B015.jpg" alt="此处输入图片的描述"><br><a id="more"></a></p><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>文章提出的方法类似于word2vec的cbow模型，并在此基础上加上bag of n-grams（考虑单词的顺序关系），下图是FastText文本分类的模型，$w$是语句中的词语，词语的向量相加求平均值作为文本表示然后做一个线性分类。<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B02.jpg" alt="此处输入图片的描述"><br>但与cbow不同的是，fasttext不是预测中心词，而是直接预测标签。模型以语句中的词语作为输入，输出语句属于各类别上的概率。<br>    模型比较简单，训练速度很快，但是准确率不高。</p><h2 id="TextCNN2014"><a href="#TextCNN2014" class="headerlink" title="TextCNN2014"></a>TextCNN2014</h2><p> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B03.jpg" alt="此处输入图片的描述"><br>输入层将一个句子所有单词的词向量拼接成一个矩阵，每一行代表一个词。图中有两个channels，原文中用的是static和non-static，即使用的词向量是否随着训练发生变化；也可以使用不同的词向量方法生成的词向量作为不同channel。对于未登录词的向量，使用0或随机正数来填充。<br>    卷积层的每个卷积核的大小为$filter<em>-size\times embedding</em>-size$。$filter<em>-size$代表卷积核纵向上包含的单词个数，即认为相邻几个词之间有词序关系，代码里使用的是[3，4，5]。$embedding</em>-size$就是词向量的维度。每个卷积核计算完成之后我们就得到了1个列向量，代表着该卷积核从句子中提取出来的特征。<br>池化层使用Max-over-time Pooling的方法。这种方法就是简单地从之前的Feature Map中提出最大的值（文中解释最大值代表着最重要的信号）。可以看出，这种Pooling方式可以解决可变长度的句子输入问题（因为不管Feature Map中有多少个值，只需要提取其中的最大值）。最终池化层的输出为各个Feature Map的最大值，即一个一维的向量。<br>全连接层，为了将pooling层输出的向量转化为我们想要的预测结果，加上一个softmax层。文中还提到了过拟合的问题，在倒数第二层的全连接部分上使用<strong>Dropout</strong>技术，即对全连接层上的权值参数给予L2正则化的限制。这样做的好处是防止隐藏层单元自适应（或者对称），从而减轻过拟合的程度。<br><strong>本文使用的词向量是CBOW在Google News上的训练结果。</strong></p><h2 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h2><p>分析了Recursive Neural Network、RNN、CNN做文本分类的优缺点：<br>Recursive Neural Network效果完全依赖于文本树的构建，并且构建文本树所需的时间是$ O\left( n^2 \right)  $  。并且两个句子的关系也不能通过一颗树表现出来。因此不适合于长句子或者文本。<br>    RNN是有偏的模型，后面的词比前面的词更重要。<br>    CNN卷积核的尺寸难以设置。如果选小了容易造成信息的丢失；如果选大了，会造成巨大的参数空间。<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B04.jpg" alt="此处输入图片的描述"><br>卷积层是一个BiRNN（双向LSTM），通过正向和反向循环来构建一个单词的上文和下文，如下式：</p><script type="math/tex; mode=display">c_l\left( w_i \right) =f\left( W^{\left( l \right)}c_l\left( w_{i-1} \right) +W^{\left( sl \right)}e\left( w_{i-1} \right) \right)</script><script type="math/tex; mode=display">c_r\left( w_i \right) =f\left( W^{\left( r \right)}c_r\left( w_{i+1} \right) +W^{\left( sr \right)}e\left( w_{i+1} \right) \right)</script><p>得到上下文表示后，拼接表示当前词：</p><script type="math/tex; mode=display">x_i=\left[ c_l\left( w_i \right) ;e\left( w_i \right) ;c_r\left( w_i \right) \right]</script><p>使用$tanh$函数激活得到：</p><script type="math/tex; mode=display">y_{i}^{\left( 2 \right)}=\tanh \left( W^{\left( 2 \right)}x_i+b^{\left( 2 \right)} \right)</script><p>池化层使用最大池化，使用所有单词在每个维度上的最大值表示文本的信息。最后输出层使用softmax得到分类结果。<br>    <strong>本文使用的词向量是使用Skip-gram训练的中英文Wikipedia。</strong></p><h2 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h2><p>传统的RNN（LSTM）结构：<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B05.jpg" alt="此处输入图片的描述"><br>下面的公式为LSTM中各门的公式：<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B012.jpg" alt="此处输入图片的描述"><br>文章《Recurrent Neural Network for Text Classification with Multi-Task Learning》中介绍了RNN用于文本分类的模型设计，主要提出了以下三种模型：<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B06.jpg" alt="此处输入图片的描述"><br>模型（a）中多任务共享LSTM结构以及公有的词向量；针对于任务$m$，输入$ \hat{x}_t $ 为：</p><script type="math/tex; mode=display">\hat{x}_{t}^{\left( m \right)}=x_{t}^{\left( m \right)}\oplus x_{t}^{\left( s \right)}</script><p>其中，$ x<em>{t}^{\left( m \right)} $ ，$ x</em>{t}^{\left( s \right)} $  分别表示任务私有的词向量和公有的词向量。最后一个时刻的hidden state则作为输入传入softmax。<br>模型（b）每个任务具有自己独立的LSTM层，但是每一时刻所有任务的hidden state则会和下一时刻的character一起作为输入，最后一个时刻的hidden state进行分类。作者修改了候选状态的计算公式：</p><script type="math/tex; mode=display">\tilde{c}_{t}^{\left( m \right)}=\tanh \left( W_{t}^{\left( m \right)}x_t+\sum_{i\in \left\{ m,n \right\}}{g^{\left( i\rightarrow m \right)}U_{c}^{\left( i\rightarrow m \right)}h_{t-1}^{\left( i \right)}} \right)</script><script type="math/tex; mode=display">g^{\left( i\rightarrow m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( i \right)}h_{t-1}^{\left( i \right)} \right)</script><p>模型（c）除了一个共享的BI-LSTM层用于获取共享信息，每个任务有自己独立的LSTM层，LSTM的输入包括每一时刻的character和BI-LSTM的hidden state，同模型2一样，作者修改了候选状态的计算公式：</p><script type="math/tex; mode=display">\tilde{c}_{t}^{\left( m \right)}=\tanh \left( W_{t}^{\left( m \right)}x_t+g^{\left( m \right)}U_{c}^{\left( m \right)}h_{t-1}^{\left( m \right)}+g^{\left( s\rightarrow m \right)}U_{c}^{\left( s \right)}h_{t}^{\left( s \right)} \right)</script><script type="math/tex; mode=display">g^{\left( m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( m \right)}h_{t-1}^{\left( s \right)} \right)</script><script type="math/tex; mode=display">g^{\left( s\rightarrow m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( s\rightarrow m \right)}h_{t}^{\left( s \right)} \right)</script><p><strong>本文使用的词向量是使用Word2Vec训练的Wikipedia语料。</strong></p><h2 id="CharCNN"><a href="#CharCNN" class="headerlink" title="CharCNN"></a>CharCNN</h2><p> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B07.jpg" alt="此处输入图片的描述"><br>首先对文本编码，使用的字母表共有69个字符，对其使用one-hot编码，外加一个全零向量（用于处理不在该字符表中的字符），所以共70个。经过6个卷积层和3个全连接层得到输出。<br>优点是是不需要使用预训练好的词向量和语法句法结构等信息，并且可以很容易的推广到所有语言。针对于汉语，作者使用拼音代替汉字实现的编码。<br>对于几百上千等小规模数据集，可以优先考虑传统方法，对于百万规模的数据集，CharCNN表现不错。CharCNN适用于用户生成数据(user-generated data)（如拼写错误，表情符号等）。</p><h2 id="GRNN（Conv、LSTM）"><a href="#GRNN（Conv、LSTM）" class="headerlink" title="GRNN（Conv、LSTM）"></a>GRNN（Conv、LSTM）</h2><p> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B08.jpg" alt="此处输入图片的描述"><br>首先使用CNN/LSTM来建模句子表示，接下来使用双向GRU模型对句子表示进行编码得到文档表示，得到的文档表示用于Softmax情感分类。<br><strong>在上图中，底层的词向量是由word2vec预训练得到。</strong>使用CNN/LSTM学习得到句子的表示，这里会把变长的句子表示表示成相同维度的向量。卷积模型如下：<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B09.jpg" alt="此处输入图片的描述"><br>filter的宽度分别取1，2，3来编码unigrams，bigrams和trigrams的语义信息。最后使用一个Average层捕获全局信息并转化为输出向量。<br> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B010.jpg" alt="此处输入图片的描述"><br>使用GRU模型，输入是变长的句子向量，输出固定长度的文本向量，这里会对最后每个单元的输出向量进行取平均操作</p><h2 id="HAN（RNN-Attention）"><a href="#HAN（RNN-Attention）" class="headerlink" title="HAN（RNN+Attention）"></a>HAN（RNN+Attention）</h2><p> <img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B011.jpg" alt="此处输入图片的描述"><br>    一个句子中每个单词的重要性不同。一篇文档中每个句子的重要性也不同。因此本文主要思想是，首先考虑文档的分层结构：单词构成句子，句子构成文档，所以建模时也分这两部分进行。其次，不同的单词和句子具有不同的信息量，不能单纯的统一对待所以引入Attention机制。而且引入Attention机制除了提高模型的精确度之外还可以进行单词、句子重要性的分析和可视化，让我们对文本分类的内部有一定了解。模型主要可以分为四个部分，如上图所示：<br>    Word encoder和sentence encoder都是双向GRU，公式如下：<br>Word encoder中公式如下：</p><script type="math/tex; mode=display">x_{it}=W_ew_{it},t\in \left[ \text{1,}T \right]  \\ \vec{h}_{it}=\overrightarrow{GRU}\left( x_{it} \right) ,t\in \left[ \text{1,}T \right]  \\ \vec{h}_{it}=\overleftrightarrow{GRU}\left( x_{it} \right) ,t\in \left[ T,1 \right]</script><p>Word attention中公式如下：</p><script type="math/tex; mode=display">u_{it}=\tanh \left( W_wh_{it}+b_w \right)  \\ \alpha _{it}=\frac{\exp \left( u_{it}^{\top}u_w \right)}{\sum_t^{}{\exp \left( u_{it}^{\top}u_w \right)}} \\ s_i=\sum_t{\alpha _{it}h_{it}}</script><p>Sentence encoder中公式如下：</p><script type="math/tex; mode=display">\vec{h}_i=\overrightarrow{GRU}\left( s_i \right) ,i\in \left[ \text{1,}L \right]  \\ \vec{h}_i=\overleftrightarrow{GRU}\left( s_i \right) ,i\in \left[ L,1 \right]</script><p>Sentence attention中公式如下：</p><script type="math/tex; mode=display">u_i=\tanh \left( W_sh_i+b_s \right)  \\ \alpha _i=\frac{\exp \left( u_{i}^{\top}u_s \right)}{\sum_i^{}{\exp \left( u_{i}^{\top}u_s \right)}} \\ v=\sum_i{\alpha _ih_i}</script><p>每个词语对应的hidden vector的输出经过变换（转置和$tanh$）之后和$u_w$ 相互作用（点积），结果就是每个词语的权重。加权以后就可以产生整个sentence的表示。从高一级的层面来看(hierarchical的由来)，每个document有$L$个句子组成，那么这$L$个句子就可以连接成另一个sequence model, 同样是双向GRU，同样的对输出层进行变换后和 相互作用，产生每个句子的权重，加权以后我们就产生了对整个document的表示。最后用softmax就可以产生对分类的预测。</p><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p><img src="/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B013.jpg" alt="此处输入图片的描述"><br>表征学习是自然语言处理中的一个基本问题。文章《Learning Structured Representation for Text Classification via Reinforcement Learning》研究如何学习文本分类的结构化表示。与大多数既不使用结构也不依赖于预定义结构的现有表示模型不同，作者提出了一种强化学习（RL）方法，通过自动地优化结构来学习句子表示。<br>作者在文章中提出两种结构表示模型（模型第二部分）：Information Distilled LSTM (ID-LSTM) 和 Hierarchically Structured LSTM (HS-LSTM)。其中 ID-LSTM 只选择重要的任务相关的单词，HS-LSTM 则去发现句子中的短语结构。两种表示模型中的结构发现被表述为一个顺序决策问题，结构发现的当前决策影响随后的决策，这可以通过策略梯度 RL 来解决。<br>结果表明，这种方法可以通过识别重要的词或任务相关的结构而无需明确的结构注释来学习任务友好的表示，从而获得有竞争力的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了神经网络在文本分类方面的一些应用，其中多篇论文被广泛引用。如果有最新的相关研究会及时更新。&lt;br&gt;文章将按照这些论文提出年份展开介绍，发展历史如下图所示：&lt;br&gt;&lt;img src=&quot;/2018/08/03/文本分类综述/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B015.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="文本分类" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>词向量综述</title>
    <link href="http://yoursite.com/2018/07/23/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/07/23/词向量综述/</id>
    <published>2018-07-23T08:14:07.000Z</published>
    <updated>2018-11-29T14:56:07.970Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了词向量的多种表示方法，主要为word2vec基础上的多种模型，侧重于中文词向量模型。<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%91%E5%B1%95%E5%9B%BE.jpg" alt="此处输入图片的描述"><br>2018年7月23日更新，在NLP课上宗成庆老师在讲词向量时提到，除了基于文本的词汇语义表示模型，还有基于图像、语音、多模态信息学习词汇语义表示等，这一些有空再补充。<br><a id="more"></a></p><h2 id="什么是词向量"><a href="#什么是词向量" class="headerlink" title="什么是词向量"></a>什么是词向量</h2><p>自然语言是符合语法、具有一定的信息、为人类所能理解并进行沟通的语言。但当我们使用计算机来处理自然语言时，计算机是无法直接理解这些符号的（汉字、字母、标点等）。这些符号需要经过数值化后才能输入计算机进行后续的处理。然而只用单个数字来表示单词是毫无意义的，它只是一个id，无法体现单词的属性，因此需要将单词进行向量化。</p><h2 id="词向量的表示"><a href="#词向量的表示" class="headerlink" title="词向量的表示"></a>词向量的表示</h2><p>词向量的表示主要有两种方式：独热编码（One-hot Representation）和分布式表示（Distributed Representation）。</p><h3 id="One-hot表示"><a href="#One-hot表示" class="headerlink" title="One-hot表示"></a>One-hot表示</h3><p>独热编码是最直观、也最常用的方法。是一种稀疏的表示方式。其思路为将每个词都表示成一个很长的向量，该向量的维度等于词表大小，其中只有一个维度的值为1（维度的位置等于词的id），其余维度都为0。举个例子：<br>假设我们从语料库中为每个词分配一个数字id（从0开始分配），得到“话筒”这个词的id为3，“麦克”为8，那么用独热编码为：</p><blockquote><p>话筒：[0 0 0 1 0 0 0 0 0 0 …]<br>麦克: [0 0 0 0 0 0 0 0 1 0 …]</p></blockquote><p>每个词都是茫茫0海中的一个1。如果要编程实现的话，用 Hash 表给每个词分配一个id就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。<br>如此简洁的方法自然有它的缺点：<br>1、向量维度会随着词表增大而增大：</p><ul><li>存储效率低</li><li>若词表扩容，则每个词维度也必须相应增加</li><li>若某个词出现次数很少的话，则相应的权重会容易被错误估计</li></ul><p>2、“词汇鸿沟”问题：每个维度彼此正交，即所有词彼此孤立，无法表示词与词之间的相关信息，例如余弦相似度。由于任意一对向量 的余弦相似度为</p><script type="math/tex; mode=display">\frac{x^{\top}y}{\lVert x \rVert \lVert y \rVert}\in \left[ -\text{1,}1 \right]</script><p>任何一对词的one-hot向量的余弦相似度都为0。</p><h3 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h3><p>1954 年，Harris 提出<strong>分布假说（distributional hypothesis）</strong>，即“上下文相似的词，其语义也相似”，为词的分布表示提供了理论基础。Firth 在1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（a word is characterized by the company it keeps）。<br>基于分布假说，研究人员提出了多种词表示模型：如基于矩阵的LSA 模型、基于聚类的Brown clustering 模型以及最近使用广泛的神经网络词表示模型。神经网络模型生成的词表示通常被称为<strong>词向量（word embedding）</strong>，是一个低维的实数向量表示，通过这种表示，可以直接对词之间的相似度进行刻画。</p><h4 id="基于矩阵的分布式表示"><a href="#基于矩阵的分布式表示" class="headerlink" title="基于矩阵的分布式表示"></a>基于矩阵的分布式表示</h4><p>利用语料库构建一个$ W\times C $ 共现矩阵$F$，矩阵每一行代表一个词，每一列是某种上下文表示方法。$W$是词表大小。矩阵每个单元的值可以是二值（表示二者是否共现），可以是未经处理的共现次数，也可以是经过处理后的共现tf-idf值，等等。很多可衡量两个对象之间关联的指标都可以用来作为矩阵中每个单元的值。<br>由于矩阵每一行的维度大小都等于词表大小，不便计算，所以需要进行降维。降维技术可以减少噪声带来的影响，但也可能损失一部分信息。最常用的分解技术包括奇异值分解（SVD）、非负矩阵分解（NMF）、典型关联分析（Canonical Correlation Analysis，CCA）、Hellinger PCA（HPCA）。<br>基于矩阵的分布表示在这些步骤的基础上，衍生出了若干不同方法，如经典的LSA就是使用tf-idf 作为矩阵元素的值，并使用SVD分解，得到词的低维向量表示。在这类方法中，最新的为GloVe 模型，下文简单介绍这一模型。</p><h5 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h5><p>总体上看，GloVe 模型是一种对“词-词”矩阵进行分解从而得到词表示的方法。矩阵第$i$行第$j$列的值为词$v<em>i$与词$v_j$在语料中的共现次数$ x</em>{ij} $的对数。在矩阵分解步骤，GloVe 模型借鉴了推荐系统中基于隐因子分解（Latent Factor Model）的方法，在计算重构误差时，只考虑共现次数非零的矩阵元素，同时对矩阵中的行和列加入了偏移项。具体为最小化下式：</p><script type="math/tex; mode=display">\sum_{ik}{f\left( x_{ik} \right) \left( w_{i}^{T}w_k+b_i+b_k-\log x_{ik} \right) ^2}</script><p>其中$ w_i $为词$ v_i $ 作为目标词时的词向量，$ w_j $ 为词$ v_j $ 作为上下文时的词向量，$ b_i $ 、$ b_k $ 为针对词表中各词的偏移向量， $ f\left( x \right)  $ 是一个加权函数，对低频的共现词对进行衰减，减少低频噪声带来的误差，定义为：</p><script type="math/tex; mode=display">f\left( x \right) =\begin{cases}     \left( \frac{x}{x_{\max}} \right) ^{\alpha}\ \ \text{如果}x<x_{\max}\\     \text{1                 其他情况}\\ \end{cases}</script><h4 id="基于聚类的分布式表示"><a href="#基于聚类的分布式表示" class="headerlink" title="基于聚类的分布式表示"></a>基于聚类的分布式表示</h4><p>基于聚类的分布式表示也被称为分布聚类，通过聚类的方法构建词与其上下文之间的关系。最经典的方法是布朗聚类（Brown clustering）。布朗聚类是一种层级聚类方法，聚类结果为每个词的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。具体而言，布朗聚类需要最大化以下似然，其中$ c_i $ 为词$ w_i $ 对应的类别：</p><script type="math/tex; mode=display">P\left( w_i|w_{i-1} \right) =P\left( w_i|c_i \right) P\left( c_i|c_{i-1} \right)</script><p>布朗聚类只考虑了相邻词之间的关系，也就是说，每个词只使用它的上一个词，作为上下文信息。</p><h4 id="基于神经网络的分布式表示"><a href="#基于神经网络的分布式表示" class="headerlink" title="基于神经网络的分布式表示"></a>基于神经网络的分布式表示</h4><p>神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。同时，神经网络语言模型对当前词向量的发展方向有着决定性的作用。</p><h5 id="NNLM2001"><a href="#NNLM2001" class="headerlink" title="NNLM2001"></a>NNLM2001</h5><p>2001年，Bengio 等人正式提出神经网络语言模型（Neural Network Language Model，NNLM）。该模型在学习语言模型的同时，也得到了词向量。NNLM对$n$元语言模型进行建模，估算$ P\left( w<em>i|w</em>{i-\left( n-1 \right)},…,w<em>{i-1} \right)  $  的值。也就是对语料中一段长度为$n$ 的序列 $ w</em>{i-\left( n-1 \right)},…,w_{i-1},w_i $ , 元语言模型需要最大化以下似然：</p><script type="math/tex; mode=display">P\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right)</script><p>其中， $ w<em>i $ 为需要通过语言模型预测的词（目标词）。对于整个模型而言，输入为条件部分的整个词序列： $ w</em>{i-\left( n-1 \right)},…,w<em>{i-1} $ ，输出为目标词的分布。<br><strong>神经网络语言模型结构图</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B01.jpg" alt="神经网络语言模型结构图"><br>输入层将前 $n-1$个词 $ w</em>{i-\left( n-1 \right)},…,w_{i-1} $ 通过查表映射成对应的词向量，然后将这 $n-1$个向量顺序拼接，形成$x$ ：</p><script type="math/tex; mode=display">x=\left[ e\left( w_{i-\left( n-1 \right)} \right) ;...;e\left( w_{i-2} \right) ;e\left( w_{i-1} \right) \right]</script><p>输入层完成对 $x$的拼接后，模型将其依次送入隐藏层$h$ 和输出层 $y$。</p><script type="math/tex; mode=display">h=\tanh \left( b^h+Hx \right)</script><script type="math/tex; mode=display">y=b^y+Wx+Uh</script><p>其中 $ H\in \mathbb{R}^{\left| h \right|\times \left( n-1 \right) \left| e \right|} $ 为输入层到隐藏层的权重矩阵， $ U\in \mathbb{R}^{\left| v \right|\times \left| h \right|} $ 为隐藏层到输出层的权重矩阵，$ \left| V \right| $  表示词表大小， $ \left| e \right| $ 表示词向量维度， $ \left| h \right| $ 为隐藏层维度，$ b^h $  、 $ b^y $ 均为偏置。矩阵$W$ 表示从输入层到输出层的直连边权重矩阵。<br>输出层一共有 $ \left| V \right| $ 个元素，对应下一个词为词表中某个词的可能性。使用softmax将输出值归一化成概率。</p><script type="math/tex; mode=display">p\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right) =\frac{\exp \left( y\left( w_i \right) \right)}{\sum_{k=1}^{\left| V \right|}{\exp \left( y\left( v_k \right) \right)}}</script><p>对于整个预料而言，语言模型需要最大化：</p><script type="math/tex; mode=display">\sum_{w_{i-\left( n-1 \right) :i\in \mathbb{D}}}{\log \left( P\left( w_i \right) |w_{i-\left( n-1 \right)},...,w_{i-1} \right)}</script><p>训练时使用随机梯度下降来优化上述目标。每次迭代，随即从语料中选取一段样本作为训练样本，使用下式进行一次梯度迭代：</p><script type="math/tex; mode=display">\theta \gets \theta +\alpha \frac{\partial \log P\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right)}{\partial \theta}</script><p>其中，$ \alpha  $ 是学习率； $ \theta  $ 为模型中所有参数，包括词向量和网络模型中的权重及偏置。<br>值得注意的是，神经网络语言模型中的词向量出现了两次。在输入层，各词的词向量存在于一个$ \left| e \right|\times \left| V \right| $  维的实数矩阵中，每一列对应一个词向量。隐藏层到输出层的权重矩阵$U$ 的维度是$ \left| V \right|\times \left| h \right| $  ，可看作$ \left| V \right| $  个$ \left| h \right| $  维的行向量，其中每个向量都可以看做某个词的另一种表示$ e’ $  。我们将 $ e\left( w \right)  $ 称为词的上下文表示，将 $ e’\left( w \right)  $ 称为词的目标词表示，通常将$e$ 作为词向量。</p><h5 id="循环神经网络语言模型"><a href="#循环神经网络语言模型" class="headerlink" title="循环神经网络语言模型"></a>循环神经网络语言模型</h5><p>Mikolov等人提出的循环神经网络语言模型（Recurrent Neural Network based Language Model，RNNLM）直接对 $ P\left( w<em>i|w_1,w_2,…,w</em>{i-1} \right)  $ 进行建模。因此，RNNLM 可以利用所有的上文信息，预测下一个词，其模型结构下图所示。<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B02.jpg" alt="循环神经网络语言模型结构图"><br>RNNLM的核心在于其隐藏层算法：</p><script type="math/tex; mode=display">h\left( i \right) =\phi \left( e\left( w_i \right) +Wh\left( i-1 \right) \right)</script><p>其中，$ \phi  $  为非线性激活函数；$h(i)$ 表示文本中第 $i$个词$w_i$ 所对应的隐藏层，该隐藏层由当前词的词向量 $ e\left( w_i \right)  $ 以及上一个词对应的隐藏层$ h\left( i-1 \right)  $  结合得到。通过迭代，每个隐藏层都包含了此前所有词的信息。RNNLM的输出层计算方法与NNLM的输出层一致。</p><h5 id="C-amp-W模型"><a href="#C-amp-W模型" class="headerlink" title="C&amp;W模型"></a>C&amp;W模型</h5><p>C&amp;W模型结构图:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B03.jpg" alt="此处输入图片的描述"><br>不同于之前的模型，C&amp;W模型直接以生成词向量为目标。他们没有去近似地求$P(w<em>t |w_1,w_2,…,w</em>(t-1))$，而是直接去尝试近似$P(w<em>1,w_2,…,w_t)$。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续 $n$个词的打分 $f(w</em>(t-n+1),…,w_(t-1),w_t)$打分越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。有了这个对 $f$的假设，C&amp;W 就直接使用 pair-wise 的方法训练词向量。具体而言，就是最小化下面的目标函数。</p><script type="math/tex; mode=display">\sum_{x\in \mathfrak{X}}{\sum_{w\in \mathfrak{D}}{\max\text{\{0,}1-f\left( x \right) +f\left( x^{\left( w \right)} \right) \}}}</script><p> $ \mathfrak{X} $ 为训练集中的所有连续的$n$ 元短语， $ \mathfrak{D} $ 是整个字典。第一个求和枚举了训练语料中的所有的$n$ 元短语，作为正样本。第二个对字典的枚举是构建负样本。 $ x^{\left( w \right)} $ 是将短语$x$ 的最中间的那个词，替换成$w$ 。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当作负样本也不影响大局）。同时，由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类效果。再回顾这个式子， $x$是正样本， $ x^{\left( w \right)} $ 是负样本，$ f\left( x \right)  $  是对正样本的打分，$ f\left( x^{\left( w \right)} \right)  $  是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高1分。$f$ 函数的结构和NNLM中的网络结构基本一致。同样是把窗口中的$n$ 个词对应的词向量串成一个长的向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同之处在于C&amp;W的输出层只有一个节点，表示得分，而不像NNLM那样的有$ \left| V \right| $  个节点，这么做可以大大降低计算复杂度。</p><h5 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h5><p>Mikolov 等人在2013年开发了word2vec工具，包含CBOW（Continuous Bag of Words）和Skip-gram 两种模型。</p><h6 id="CBOW模型"><a href="#CBOW模型" class="headerlink" title="CBOW模型"></a>CBOW模型</h6><p><strong>CBOW模型结构图</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B04.jpg" alt="此处输入图片的描述"><br>简单来讲，CBOW用上下文作为输入去预测目标词。该模型一方面根据C&amp;W 模型的经验，使用一段文本的中间词作为目标词；另一方面，又以NNLM作为蓝本，并在其基础上做了两个简化。一、CBOW 没有隐藏层，去掉隐藏层之后，模型从神经网络结构直接转化为log 线性结构，与Logistic 回归一致。log 线性结构比三层神经网络结构少了一个矩阵运算，大幅度地提升了模型的训练速度。二、CBOW 去除了上下文各词的词序信息，使用上下文各词词向量的平均值（论文中求和，实际工具求平均），代替神经网络语言模型使用的上文各词词向量的拼接。形式化地，CBOW 模型对于一段训练样本$ w_{i-\left( n-1 \right)},…,w_i $  ，输入为：</p><script type="math/tex; mode=display">x=\frac{1}{n-1}\sum_{w_j\in c}{e\left( wj \right)}</script><p>然后根据上下文表示，对目标词进行预测：</p><script type="math/tex; mode=display">P\left( w|c \right) =\frac{\exp \left( e'\left( w \right) ^Tx \right)}{\sum_{w'\in \mathbb{V}}{\exp \left( e'\left( w' \right) ^Tx \right)}}</script><p>上述二式，目标词$w$ 与上下文$c$ 的定义与C&amp;W模型一致。优化目标与神经网络语言模型一致，即最大化：</p><script type="math/tex; mode=display">\sum_{\left( w,c \right) \in \mathbb{D}}{\log P\left( w|c \right)}</script><h6 id="Skip-gram模型"><a href="#Skip-gram模型" class="headerlink" title="Skip-gram模型"></a>Skip-gram模型</h6><p><strong>Skip-gram模型结构图</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B05.jpg" alt="此处输入图片的描述"><br>与CBOW相反，Skip-gram用目标词作为输入去预测上下文。为了与CBOW统一，可以将其描述为：每次从目标词$w$ 的上下文$c$ 中选择一个词，将其词向量作为模型的输入$x$ ，也就是上下文的表示，这样Skip-gram的任务也变成了通过上下文预测目标词，优化目标为：</p><script type="math/tex; mode=display">\sum_{\left( w,c \right) \in \mathbb{D}}{\sum_{w_j\in c}{\log P\left( w|w_j \right)}}</script><p>其中，</p><script type="math/tex; mode=display">P\left( w|w_j \right) =\frac{\exp \left( e'\left( w \right) ^Te\left( w_j \right) \right)}{\sum_{w'\in \mathbb{V}}{\exp \left( e'\left( w' \right) ^Te\left( w_j \right) \right)}}</script><h5 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h5><p>word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。<br>方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。</p><h5 id="CWE"><a href="#CWE" class="headerlink" title="CWE"></a>CWE</h5><p><strong>CWE模型结构图</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B06.jpg" alt="此处输入图片的描述"><br>在word2vec的CBOW模型基础上，在词向量生成部分进行了改进，引入了单个汉字的信息（论文主要针对的是中文），提升了词向量生成的质量。具体实现方法是将CBOW中 $x$的取值办法由上下文词向量相加求均值：</p><script type="math/tex; mode=display">x_o=\frac{1}{2k}\sum_{j=i-k,...,i+k}{x}_j</script><p>变成词语中所有单字的向量与词向量相加求均值：</p><script type="math/tex; mode=display">x=\frac{1}{2}\{w+\frac{1}{N_j}\sum_{k=1}^{N_J}{c_k}\text{)}</script><p>其中， $ N_j $ 是单词中的汉字个数， $w$是字向量。<br>另外文章还提出了三种办法以解决同一汉字在不同词语中的不同语义问题，分别为：Position-based Character Embedding、Cluster-based Character Embedding、Nonparametric Cluster-based Character Embeddings。</p><h5 id="JWE2017"><a href="#JWE2017" class="headerlink" title="JWE2017"></a>JWE2017</h5><p><strong>JWE模型结构图</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B07.jpg" alt="此处输入图片的描述"><br>根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起联合学习词向量。其中$ w<em>i $  是目标词， $ w</em>{i-1} $ 是目标词左边的词， $ w<em>{i+1} $ 是目标词右边的词；$ c</em>{i-1} $  是目标词左边的词中的单字， $ c<em>{i+1} $ 是目标词右边的词中的单字；$ s</em>{i-1} $  是目标词左边的词拆分的字件，$ s_{i+1} $  是目标词右边的词拆分的字件, $ s_i $ 是目标词的字件；<br>模型的目标是最大化以下对数似然：</p><script type="math/tex; mode=display">L\left( w_i \right) =\sum_{k=1}^3{\log P\left( w_i|h_{i_k} \right)}</script><p>其中, $ h<em>{i_1} $ ， $ h</em>{i<em>2} $ ， $ h</em>{i_3} $ 分别为上下文单词、单字、组件的组合。</p><script type="math/tex; mode=display">P\left( w_i|h_{i_k} \right) =\frac{\exp \left( h_{i_k}^{T}\hat{v}_{w_i} \right)}{\sum_{j=1}^N{\exp \left( h_{i_k}^{T}\hat{v}_{w_j} \right)}}</script><script type="math/tex; mode=display">h_{i_1}=\frac{1}{2T}\sum_{-T\leqslant j\leqslant T,j\ne 0}{v_{w_{i+j}}}</script><p>在以上3个公式中， $ v<em>{w_i} $ ,$ v</em>{c<em>i} $  , $ v</em>{s<em>i} $ 分别为上下文、单字、字件的“<strong>输入向量</strong>”， $ \hat{v}</em>{w<em>j} $ 是“<strong>输出向</strong>量”；$ h</em>{i<em>1} $  为上下文“输入向量”的均值，同理， $ h</em>{i<em>2} $ 为上下文单字“输入向量”的均值，$ h</em>{i_3} $  为上下文字件“输入向量”的均值。<br>给定语料$ D $  ,模型的目标是最大化：</p><script type="math/tex; mode=display">L\left( D \right) =\sum_{w_i\in D}{L\left( w_i \right)} $ #####GWE2017**GWE模型结构图（一）**:![此处输入图片的描述][9]通过convolutional auto-encoder（convAE）提取词中字的glyph特征，取均值后与上下文词向量均值及上下文中字向量的均值合并，得到新的上下文表示，预测目标词。词向量表示为： $$ \vec{w}_{i}^{ctxG}=\vec{w}_i+\frac{1}{\left| C\left( i \right) \right|}\sum_{c_j\in C\left( i \right)}{\left( \vec{c}_j+\vec{g}_j \right)}</script><p><strong>GWE模型结构图（二）</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B09.jpg" alt="此处输入图片的描述"><br>这种模型将当前词中字的glyph特征与上下文结合，一起预测当前词。<br>以下两种模型直接从特征中学习词向量，没有使用上下文信息。</p><p><strong>GWE模型结构图（三）</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B010.jpg" alt="此处输入图片的描述"><br>在Skip-gram的基础上，以特征作为输入，两层GRU，两层全连接ELU，预测当前词的上下文。</p><p><strong>GWE模型结构图（四）</strong>:<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B011.jpg" alt="此处输入图片的描述"><br>类似于GloVe模型，将原模型中词向量部分改写为当前词的特征。</p><h5 id="cw2vec"><a href="#cw2vec" class="headerlink" title="cw2vec"></a>cw2vec</h5><p>cw2vec在Skip-Gram基础之上进行改进，把词语的n-gram笔画特征信息代替词语进行训练，cw2vec模型如下图：<br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B012.jpg" alt="此处输入图片的描述"><br><img src="/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B013.jpg" alt="此处输入图片的描述"><br> 首先将词语分割，获取中文字符。然后获取笔画信息，并合并笔画信息。将笔画信息数字化。最后提取笔画信息的n-gram特征。</p><h2 id="词向量的评价"><a href="#词向量的评价" class="headerlink" title="词向量的评价"></a>词向量的评价</h2><p>根据来斯惟的博士论文《基于神经网络的词和文档语义向量》，词向量的用法可以分为三大类，分别为：一、利用词向量的语言学特性完成任务；二、将词向量作为特征，提高自然语言处理任务的性能；三、将词向量作为神经网络的初始值，提升神经网络模型的优化效果。他在这三大类用法的基础上，选取了八个有代表性的具体任务，作为词向量的评价指标。</p><h3 id="词向量的语言学特征"><a href="#词向量的语言学特征" class="headerlink" title="词向量的语言学特征"></a>词向量的语言学特征</h3><p>各词向量模型均基于分布假说设计而成，因此无论哪种词向量模型，都会符合分布假说所提出的性质：具有相似上下文的词，会拥有相似的语义，并且其词向量的空间距离更接近。本文选取了三个代表性任务。</p><h4 id="语义相关性-ws"><a href="#语义相关性-ws" class="headerlink" title="语义相关性(ws)"></a>语义相关性(ws)</h4><p>衡量语义相关性最经典的是WordSim353 数据集，该数据集包含了353 个词对，其中每一个词对有至少十位标注者对其进行0 到10 之间的打分，分数越高表示标注人员认为这两个词的语义更相关或者更相似。例如，词对“student,professor”的平均打分为6.81，而词对“professor, cucumber”的打分为0.31。评价时，对于每个词对，本文使用所有标注者打分的平均值作为参考得分$X$ ，以词对的两个词向量的余弦距离作为模型得到的相关性得分$Y$ ，并衡量这两组数值之间的皮尔逊相关系数。皮尔逊相关系数衡量了两个变量之间的线性相关性，值在-1到1之间，如果模型得到的打分与人工标注的打分一致，得分就越高。具体而言，$X$ 和$Y$ 之间的皮尔逊相关系数定义为 $X$和 $Y$之间的协方差与它们标准差的商：</p><script type="math/tex; mode=display">\rho _{X,y}=\frac{cov\left( X,Y \right)}{\sigma _X\sigma _Y}</script><h4 id="同义词检测-tfl"><a href="#同义词检测-tfl" class="headerlink" title="同义词检测(tfl)"></a>同义词检测(tfl)</h4><p>托福考试（TOEFL）数据集包含80 个单选题，每个题目包含一个问题词以及四个选项，要求从四个选项中选出一个与问题词同义的词语。例如：问题“levied”，选项“imposed”、“believed”、“requested”、“correlated”，正确答案为“imposed”。对于每一个问题，需要计算问题词与选项词对应词向量之间的余弦距离，并选用距离最近的选项词，作为答案。在评价词向量时，可以直接使用80 个问题的准确率。</p><h4 id="单词类比-sem、syn"><a href="#单词类比-sem、syn" class="headerlink" title="单词类比(sem、syn)"></a>单词类比(sem、syn)</h4><p>英文单词类比数据集由Mikolov 等人于2013 年的word2vec相关论文中提出，该数据集包含了9000 个语义类比问题以及1 万个句法类比问题。语义类比问题包括国家首都、家庭成员称谓、国家货币等五类问题，如，“‘king’对‘queen’如同‘man’对什么？”，答案为“woman”。句法类比问题有比较级、最高级、名词单复数等九类问题，如“‘dance’对‘dancing’如同‘predict’对什么？”，答案为“predicting”。<br>为了回答这类类比问题，Mikolov 等人根据相似关系词对的词向量之差也相似的特点，提出使用词向量的加减法来完成这一任务。例如，对于问题“‘king’对‘queen’如同‘man’对什么？”，该方法直接从词表中寻找与$ \overrightarrow{queen}-\overrightarrow{king}+\overrightarrow{man} $  最相似的词，作为答案。评价时使用回答问题的准确率。<br>单词类比任务的数据集相对前两个任务规模较大，因此在实验中，结果较为稳定，该指标也成为评价词向量的经典指标。</p><h3 id="词向量用作特征"><a href="#词向量用作特征" class="headerlink" title="词向量用作特征"></a>词向量用作特征</h3><p>词向量可以从无标注文本中学习到句法和词法的特征，很多现有工作直接使用词向量作为机器学习系统的特征，并以此提高系统的性能。</p><h4 id="基于平均词向量的文本分类-avg"><a href="#基于平均词向量的文本分类-avg" class="headerlink" title="基于平均词向量的文本分类(avg)"></a>基于平均词向量的文本分类(avg)</h4><p>该任务直接以文本中各词词向量的加权平均值作为文档的表示，以此为特征，利用Logistic 回归完成文本分类任务。其中权重为文档中各词的词频。可以选用IMDB 数据集做文本分类实验。该数据集包含三部分，其中训练集和测试集各2.5 万篇文档，用来做文本分类的训练和测试；无标注部分共5 万篇文档，用于训练词向量。任务的评价指标为文本分类的准确率。</p><h4 id="命名实体识别-ner"><a href="#命名实体识别-ner" class="headerlink" title="命名实体识别(ner)"></a>命名实体识别(ner)</h4><p>命名实体识别（Named entity recognition，NER）在机器学习框架下，通常作为一个序列标注问题处理。在这一评价指标中，将词向量作为现有命名实体识别系统的额外特征，该系统的性能接近现有系统的最好性能。任务的评价指标为命名实体识别的F1值，测试集可以是CoNLL03 多任务数据集的测试集。</p><h3 id="词向量用作神经网络初始值"><a href="#词向量用作神经网络初始值" class="headerlink" title="词向量用作神经网络初始值"></a>词向量用作神经网络初始值</h3><p>在上一类词向量的用法（将词向量作为特征）中，词向量是模型的固定输入值，在模型的训练过程中，输入值不会改变，只有模型中的参数会改变。然而，将神经网络的初始值赋值为词向量之后，神经网络在训练过程中会改变设置的初始值。因此这两类词向量的用法表面上看非常相似，实质上却是不同的。</p><h4 id="基于卷积神经网络的文本分类-cnn"><a href="#基于卷积神经网络的文本分类-cnn" class="headerlink" title="基于卷积神经网络的文本分类(cnn)"></a>基于卷积神经网络的文本分类(cnn)</h4><p>卷积神经网络（Convolutional neural networks，CNN）是表示文本的有效模型。2014 年，Lebret等人以及Kim等人同时提出用于文本分类任务的卷积神经网络。<br>选取斯坦福情感树库（Stanford Sentiment Treebank）数据集作为文本分类的训练集、验证集和测试集。由于该数据集规模较小，文本分类的效果受网络初始值的影响较大，导致了评价指标的不稳定。为了更客观地评价卷积网络中，不同词向量对文本分类性能的影响，对每一份词向量重复做多次实验。在每次实验中，输入层词表示均初始化为这份词向量，网络结构中的其它参数则初始化为不同的随机值。对于每一次实验，在训练集上训练卷积神经网络，取验证集上准确率最高的点，并报告其在测试集上的准确率。最后将5 组实验的测试集准确率的平均值作为最终的评价指标。</p><h4 id="词性标注-pos"><a href="#词性标注-pos" class="headerlink" title="词性标注(pos)"></a>词性标注(pos)</h4><p>词性标注（part-of-speech tagging）是一个经典的序列标注问题。在这个任务中，使用Collobert 等人提出的网络，对句子中的每个词做序列标注。该任务选用华尔街日报数据集。评价指标为模型在验证集上达到最佳效果时，测试集上的准确率。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了词向量的多种表示方法，主要为word2vec基础上的多种模型，侧重于中文词向量模型。&lt;br&gt;&lt;img src=&quot;/2018/07/23/词向量综述/%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%91%E5%B1%95%E5%9B%BE.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;2018年7月23日更新，在NLP课上宗成庆老师在讲词向量时提到，除了基于文本的词汇语义表示模型，还有基于图像、语音、多模态信息学习词汇语义表示等，这一些有空再补充。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>GWE:Learning Chinese Word Representations From Glyphs Of Characters读书笔记</title>
    <link href="http://yoursite.com/2018/07/15/GWE%EF%BC%9ALearning-Chinese-Word-Representations-From-Glyphs-Of-Characters%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/</id>
    <published>2018-07-15T06:27:07.000Z</published>
    <updated>2018-11-29T15:01:56.097Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文GWE:Learning Chinese Word Representations From <strong>Glyphs</strong> Of Characters中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Learning Chinese Word Representations From Glyphs Of Characters》是国立台湾大学2017年在EMNLP发表的，在词向量中引入了convolutional auto-encoder（convAE），提取字的信息，提升了词向量的质量。<br>论文中第二部分关于词向量在汉字领域的相关工作研究做的比较充分，在写论文的时候可以适当参考引用。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型主要有两部分，分别是特征提取器和词向量训练模型。</p><h3 id="Character-Bitmap-Feature-Extraction"><a href="#Character-Bitmap-Feature-Extraction" class="headerlink" title="Character Bitmap Feature Extraction"></a>Character Bitmap Feature Extraction</h3><p>本文使用的是Masci等在2011年提出的convAE，结构图如下：<br><img src="/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/GWE1.png" alt="此处输入图片的描述"><br>得到字的glyph特征。</p><h3 id="词向量训练"><a href="#词向量训练" class="headerlink" title="词向量训练"></a>词向量训练</h3><p>针对词向量训练，作者提出了四种不同的模型。</p><h4 id="Enhanced-by-ContextWord-Glyphs"><a href="#Enhanced-by-ContextWord-Glyphs" class="headerlink" title="Enhanced by ContextWord Glyphs"></a>Enhanced by ContextWord Glyphs</h4><p><img src="/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/GWE2.png" alt="此处输入图片的描述"><br>这种模型是在CWE的基础上改进而来，词$w_i$的词向量表示为：</p><script type="math/tex; mode=display">\vec{w}_{i}^{ctxG}=\vec{w}_i+\frac{1}{\left| C\left( i \right) \right|}\sum_{c_j\in C\left( i \right)}{\left( \vec{c}_j+\vec{g}_j \right)}</script><p>其中$\vec{g}_j$是由特征提取器提取出的特征。</p><h4 id="Enhanced-by-TargetWord-Glyphs"><a href="#Enhanced-by-TargetWord-Glyphs" class="headerlink" title="Enhanced by TargetWord Glyphs"></a>Enhanced by TargetWord Glyphs</h4><p><img src="/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/GWE3.png" alt="此处输入图片的描述"><br>这种模型将当前词中字的特征与上下文结合，一起预测当前词。</p><h4 id="RNN-Skipgram"><a href="#RNN-Skipgram" class="headerlink" title="RNN-Skipgram"></a>RNN-Skipgram</h4><p>以下两种模型直接从特征中学习词向量，没有使用上下文信息。<br><img src="/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/GWE4.png" alt="此处输入图片的描述"><br>在Skip-gram的基础上，以特征作为输入，两层GRU，两层全连接ELU，预测当前词的上下文。</p><h4 id="RNN-GloVe"><a href="#RNN-GloVe" class="headerlink" title="RNN-GloVe"></a>RNN-GloVe</h4><p><img src="/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/GWE5.png" alt="此处输入图片的描述"><br>类似于GloVe模型，将原模型中词向量部分改写为当前词的特征。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验从Word Similarity、Word Analogy、Case Study三方面对比GWE与其他模型的优劣。其中在有的语料上的表现并不如之前的模型。作者将这种情况归结为“If character in iformation does not play a role in learning word representations, character glyphs may not be useful.”说明不要管模型复杂与否，适合应用场景的才是最好的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文GWE:Learning Chinese Word Representations From &lt;strong&gt;Glyphs&lt;/strong&gt; Of Characters中的核心思想。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>JWE:Joint Embeddings of Chinese Words,Characters,and Fine-grained Subcharacter Components</title>
    <link href="http://yoursite.com/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components读书笔记/</id>
    <published>2018-07-14T09:27:28.000Z</published>
    <updated>2018-11-29T15:33:15.630Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文JWE：Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components》是香港科技大学2017年在EMNLP发表的，同样是在词向量生成部分进行了改进，引入了人工总结的<strong>“字件信息”</strong>，提升了词向量的质量。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components读书笔记/JWE.png" alt="此处输入图片的描述"><br>根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起联合学习词向量。其中$w<em>i$是目标词，$w</em>{i-1}$是目标词左边的词，$w<em>{i+1}$是目标词右边的词；$c</em>{i-1}$是目标词左边的词中的单字，$c<em>{i+1}$是目标词右边的词中的单字；$s</em>{i-1}$是目标词左边的词拆分的字件，$s_{i+1}$是目标词右边的词拆分的字件,$s_i$是目标词的字件；<br>模型的目标是最大化以下对数似然：</p><script type="math/tex; mode=display">L\left( w_i \right)=\sum_{k=1}^3{\log P\left( w_i|h_{i_k} \right)}\tag1</script><p>其中,$h<em>{i_1}$,$h</em>{i<em>2}$,$h</em>{i_3}$分别为上下文单词、单字、组件的组合。</p><script type="math/tex; mode=display">P\left( w_i|h_{i_k} \right) =\frac{\exp \left( h_{i_k}^{T}\hat{v}_{w_i} \right)}{\sum_{j=1}^N{\exp \left( h_{i_k}^{T}\hat{v}_{w_j} \right)}}\tag2</script><script type="math/tex; mode=display">h_{i_1}=\frac{1}{2T}\sum_{-T\leqslant j\leqslant T,j\ne 0}{v_{w_{i+j}}}\tag3</script><p>在以上3个公式中，$v<em>{w_i}$,$v</em>{c<em>i}$,$v</em>{s<em>i}$分别为上下文、单字、字件的<strong>“输入向量”</strong>，$\hat{v}</em>{w<em>j}$是<strong>“输出向量”</strong>；$h</em>{i<em>1}$为上下文“输入向量”的均值，同理，$h</em>{i<em>2}$为上下文单字“输入向量”的均值，$h</em>{i_3}$为上下文字件“输入向量”的均值。<br>给定语料$D$,模型的目标是最大化：</p><script type="math/tex; mode=display">L(D)=\sum_{w_i \in D}{L(w_i)}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章在word similarity evaluation和word analogy tasks两个任务上对比JWE与之前词向量模型。<br>发现在word analogy tasks任务上取得了显著提高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文JWE：Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components中的核心思想。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>Character and Word Embedding读书报告</title>
    <link href="http://yoursite.com/2018/07/14/Character-and-Word-Embedding%E8%AF%BB%E4%B9%A6%E6%8A%A5%E5%91%8A/"/>
    <id>http://yoursite.com/2018/07/14/Character-and-Word-Embedding读书报告/</id>
    <published>2018-07-14T06:45:41.000Z</published>
    <updated>2018-11-29T15:21:20.090Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文CWE：Joint Learning of Character and Word Embeddings中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Joint Learning of Character and Word Embeddings》是刘知远老师团队2015年在顶会Ijcai上发表的，在词向量生成部分进行了改进，引入了单个汉字的信息（论文主要针对的是中文），提升了词向量生成的质量。因为模型名称叫做“character-enhanced word embeddding model”，故模型简称为CWE。<br>从论文的题目可以看出，这篇paper在进行词向量训练的时候，把组成词语的汉字单独抽取出来，和词语一起进行训练。这样就使那些共享汉字的词语之间产生了联系，因为paper的假设是<strong>“semantically compositional”</strong>的词语中的汉字对词语的意思具有一定的表征作用，比方说词语“智能”。但是在汉语中并不是所有的词语都是semantically compositional，比方说一些<strong>音译词</strong>“巧克力”，“沙发”，再比方说一些实体的名称，比方说一些人名、地名和国家名。在这些词语中，单个汉字的意思可能和本来这个词语要表达的意思是完全没有关系的。在本篇paper中，作者做了大量的工作去把这些没有semantically compositional性质的词语全部人工的挑选出来，对于这些词语不去进行单个字的拆分处理。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2018/07/14/Character-and-Word-Embedding读书报告/CWE1.png" alt="此处输入图片的描述"><br>这篇文章提出的模型是在word2vec的CBOW模型基础上改进而来，模型的优化函数：</p><script type="math/tex; mode=display">\frac{1}{M}\sum_{i=k}^{N-k}logPr(x_i|x_{i-k},....,x_{i+k})\tag{1}</script><p>其中，</p><script type="math/tex; mode=display">Pr(x_i|x_{i-k},....,x_{i+k})=\frac{exp(x_o\cdot x_i)}{\sum_{x_j'\in{Dictionary}}exp(x_o\cdot x_j')}\tag{2}</script><p>在CBOW模型中，context的表示是$w_i$前后窗口内的词向量的相加求均值。</p><script type="math/tex; mode=display">x_o=\frac{1}{2k}\sum_{j=i-k,....i+k}x_j\tag{3}</script><p>而在CWE模型中，对于context中的词语的表征，一方面来自于词向量，还有一部分在自于这些词语中的字的向量，具体的计算方式如下：</p><script type="math/tex; mode=display">x_j=w_j\oplus\frac{1}{N_j}\sum_{k=1}^{N_j}c_k\tag{4}</script><p>其中，其中，$N_j$是单词$w_j$中的汉字个数，$c_k$是字向量。$\oplus$对应的操作有拼接和相加两种方式，paper里说拼接方式虽然增加了模型的复杂度，但是对于效果的提升并不明显，因此后面的模型中直接就采用了相加的方式，公式如下所示： </p><script type="math/tex; mode=display">x_j=\frac{1}{2}(w_j+\frac{1}{N_j}\sum_{k=1}^{N_j}c_k)\tag{5}</script><p>注意上述公式中的$\frac{1}{2}$非常重要，它保证了具有semantically compositional的词语和不具有semantically compositional词语在计算距离时的一致性。同时paper指出，为了简化起见只对context的生成考虑字向量信息，target部分不予考虑。其中对于$\sum_{k=1}^{N_j}c_k$计算部分只是把一个词语中的汉字向量进行<strong>等权</strong>相加，如果利用<strong>attention机制</strong>，可能效果更好。</p><h2 id="单字不同语义的解决办法"><a href="#单字不同语义的解决办法" class="headerlink" title="单字不同语义的解决办法"></a>单字不同语义的解决办法</h2><p>同一个汉字，在不同的词语中可能具有完全不同的语义，如果使用一个向量来表征一个字，那么很可能会无法标识出这些差异性，故使用多个向量来表征同一个汉字，有下面几种方式：</p><h3 id="Position-based-Character-Embedding"><a href="#Position-based-Character-Embedding" class="headerlink" title="Position-based Character Embedding"></a>Position-based Character Embedding</h3><p>从名字可以看出，在该模型中同一个汉字根据其在词语中出现的位置不同，对应不同位置的向量表示形式。分析可知，汉字在词语中出现的位置有：Begin,Middle,End这三种情况，故每一个汉字都有三种向量表示形式，在进行$x<em>j=\frac{1}{2}(w_j+\frac{1}{N_j}\sum</em>{k=1}^{N_j}c_k)$ 生成向量操作的时候，对于$c_k$按照其在词语中出现的位置进行合理的筛选。这种方式比较简单，但是缺点也是比较明显的，它假设的前提是同一个汉字只要位于不同单词的同一个位置就具有相同的语义，这显然在一些情况下是不成立的。</p><h3 id="Cluster-based-Character-Embedding"><a href="#Cluster-based-Character-Embedding" class="headerlink" title="Cluster_based Character Embedding"></a>Cluster_based Character Embedding</h3><p>这种方法看的不是很懂，简单来讲就是：对于每一个汉字提前分配x个字向量，称之为模式向量。利用该词对应的词语的context信息，从一个汉字的所有模式向量中选择一个和context语义计算上最相似的作为该汉字对应的向量。</p><h3 id="Nonparametric-Cluster-based-Character-Embeddings"><a href="#Nonparametric-Cluster-based-Character-Embeddings" class="headerlink" title="Nonparametric Cluster-based Character Embeddings"></a>Nonparametric Cluster-based Character Embeddings</h3><p>该模型和Cluster_based Character Embedding模型是很相似的，唯一不同的是，Cluster_based Character Embedding中的每一个汉字对应的模型向量的数量是一个预先设定的固定值，也就是作为模型的超参数。而在Nonparametric Cluster-based Character Embeddings模型中，该值是一个模型自动学习的值。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>1 本文提出的汉字和词语结合的方式就是简单的<strong>向量相加</strong>操作，也许应用复杂一点的操作（比如矩阵变换）等方式可以把二者更加合理地结合在一起；<br>2 在相加的时候，需要给以每一个汉字不同权重，这也和我之前说的一致，可以利用attention机制，只不过在2015年的时候还没有attention的概念。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文CWE：Joint Learning of Character and Word Embeddings中的核心思想。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
</feed>
