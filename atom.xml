<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-09-26T02:42:26.569Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>KaiQiang Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HyperLedger Fabric架构篇</title>
    <link href="http://yoursite.com/2018/09/26/HyperLedger-Fabric%E6%9E%B6%E6%9E%84%E7%AF%87/"/>
    <id>http://yoursite.com/2018/09/26/HyperLedger-Fabric架构篇/</id>
    <published>2018-09-26T01:41:09.000Z</published>
    <updated>2018-09-26T02:42:26.569Z</updated>
    
    <content type="html"><![CDATA[<p>调研了一下HyperLedger Fabric的架构，因为它的官网文档并没有及时更新，所以有的内容参考的还是以前版本的文档。<br><a id="more"></a></p><h2 id="chaincode-可以用Go-java开发"><a href="#chaincode-可以用Go-java开发" class="headerlink" title="chaincode(可以用Go,java开发)"></a>chaincode(可以用Go,java开发)</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric1.png" alt="Fabric1"></p><h2 id="Ledger"><a href="#Ledger" class="headerlink" title="Ledger"></a>Ledger</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric2.png" alt="Fabric2"><br>整个区块结构分为文件系统存储的<strong>Block结构</strong>和数据库维护的<strong>State状态</strong>，其中state的存储结构是可以替换的，可选的实现包括各种KV数据库（LevelDB，CouchDB等）</p><h2 id="node有三种："><a href="#node有三种：" class="headerlink" title="node有三种："></a>node有三种：</h2><ul><li>client或submitting-client：负责向背书人（endorsers）提交实际的事务调用，并将交易提议广播到订阅服务</li><li>peer：负责提交交易，维持state，有的peer还担任endorser（审查交易合法性，合法则签名）</li><li>Ordering-service-node or orderer: 负责通信服务</li></ul><h2 id="交易的基本工作流程"><a href="#交易的基本工作流程" class="headerlink" title="交易的基本工作流程"></a>交易的基本工作流程</h2><ol><li>client创建一个transaction，并根据endorse policy发送给某些endorser peers</li><li>peers模拟transaction，并产生认可签名</li><li>client收集到足够的peers对transaction的认可，并交给ordering service，orderer会汇总各client递交过来的trasaction交易，排序、打包</li><li>orderer将交易打包成区块block，然后通知所有commit peer，各peer各自验证结果，最后将区块block记录到自己的ledger账本中</li></ol><p>如以下例子所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric3.png" alt="Fabric3"></p><ol><li>首先，client发起一个transaction交易，含有<clientid,chaincodeid,txpayload,timrstamp,clientsig>等信息，指明了3W要素：消息是谁who在什么时间when发送了什么what。该消息根据chaincode中的背书策略，发向EP1、EP2、EP3这三个peer节点。</clientid,chaincodeid,txpayload,timrstamp,clientsig></li><li>这三个peer节点模拟执行智能合约，并将结果及其各自的CA证书签名发还client。client收集到足够数量的结果后再进行下一步。</li><li>client将含背书结果的tx交易发向ordering service。</li><li>ordering service将打包好的block交给committing peer CP1以及EP1、EP2、EP3这三个背书者，背书者此时会校验结果并写入世界状态以及账本中。同时，client由于订阅了消息，也会收到通知。</li></ol><h2 id="构建企业级区块链的要素"><a href="#构建企业级区块链的要素" class="headerlink" title="构建企业级区块链的要素"></a>构建企业级区块链的要素</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric4.png" alt="Fabric4"></p><h2 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric5.png" alt="Fabric5"><br>0.6运行时架构与1.0运行时架构：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric6.png" alt="Fabric6"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/Fabric7.png" alt="Fabric7"></p><p>两者对比：</p><ul><li>分拆Peer的功能，将Blockchain的数据维护和共识服务进行分离，共识服务从Peer节点中完全分离出来，独立为Orderer节点提供共识服务；</li><li>基于新的架构，实现多通道（channel）的结构，实现了更为灵活的业务适应性（业务隔离、安全性等方面）</li><li>支持更强的配置功能和策略管理功能，进一步增强系统的灵活性和适应性；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;调研了一下HyperLedger Fabric的架构，因为它的官网文档并没有及时更新，所以有的内容参考的还是以前版本的文档。&lt;br&gt;
    
    </summary>
    
      <category term="区块链" scheme="http://yoursite.com/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
      <category term="区块链" scheme="http://yoursite.com/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
  </entry>
  
  <entry>
    <title>2018.08.22收获与总结</title>
    <link href="http://yoursite.com/2018/08/22/2018-08-22%E6%94%B6%E8%8E%B7%E4%B8%8E%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2018/08/22/2018-08-22收获与总结/</id>
    <published>2018-08-22T13:52:05.000Z</published>
    <updated>2018-08-22T14:06:55.047Z</updated>
    
    <content type="html"><![CDATA[<p>今天处理了一下THUC的新闻数据集，具体代码在下面。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>sklearn里的shuffle可以将数据打乱，我在之前处理南开数据集的时候忽视了这一点。同样，pandas中的sample()也是同样的作用，numpy库中的方法不推荐，会导致内存溢出。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>离开实验室的时候跑了TextCNN的模型，结果到家看了一下结果，从第九个epoch开始准确率都为1（一共15个epochs）。一开始我纳闷为啥准确率这么高，因为在复旦数据集上也就0.86左右的准确率。后来看了一下下面的代码。最后两行是我生成训练集和测试集的方法，仔细看知道了测试集就是训练集的一个子集！怪不得准确率这么高，因为已经告诉你label了啊！明天重新生成一下训练集和测试集。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_to_mini</span><span class="params">(dirname, targetname, per_class_max_docs=<span class="number">1000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    处理清华大学语料库，将类别和文档处理成1个txt</span></span><br><span class="line"><span class="string">    dirname:原始路径</span></span><br><span class="line"><span class="string">    targetname:保存路径</span></span><br><span class="line"><span class="string">    per_class_max_docs:每类文档保留的文档数量</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># f_train = io.open(targetname, 'w', encoding='utf-8')</span></span><br><span class="line">    labels = []</span><br><span class="line">    contents = []</span><br><span class="line">    ids = []</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(dirname):</span><br><span class="line">        print(<span class="string">'path error'</span>)</span><br><span class="line">    <span class="keyword">for</span> category <span class="keyword">in</span> os.listdir(dirname):  <span class="comment"># 分类目录</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        cat_dir = os.path.join(dirname, category)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(cat_dir):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        files = os.listdir(cat_dir)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> cur_file <span class="keyword">in</span> files: <span class="comment">#具体文件</span></span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count &gt; per_class_max_docs:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            filename = os.path.join(cat_dir, cur_file)</span><br><span class="line">            <span class="keyword">with</span> io.open(filename, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                content = f.read().replace(<span class="string">'\n'</span>, <span class="string">''</span>).replace(<span class="string">'\t'</span>, <span class="string">''</span>).replace(<span class="string">'\u3000'</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="comment"># content = is_ustr(content)</span></span><br><span class="line">                <span class="comment"># line_content = category + '\t' + content + '\t' _ str(count) + '\n'</span></span><br><span class="line">                <span class="comment"># f_train.write(category + '\t' + content + '\t' + str(count) + '\n')</span></span><br><span class="line">            labels.append(label_dict[category])</span><br><span class="line">            contents.append(content.replace(<span class="string">'\n'</span>, <span class="string">''</span>))</span><br><span class="line">            ids.append(str(count))</span><br><span class="line">        print(<span class="string">'Finished:'</span>, category)</span><br><span class="line">    <span class="comment"># train_len=int(len(ids)*0.8)</span></span><br><span class="line">    <span class="comment"># train_df = pd.DataFrame(&#123;'label': labels[:train_len], 'content': contents[:train_len], 'id': ids[:train_len]&#125;)</span></span><br><span class="line">        val_df = pd.DataFrame(&#123;<span class="string">'label'</span>: labels[:], <span class="string">'content'</span>: contents[:], <span class="string">'id'</span>: ids[:]&#125;)</span><br><span class="line">    <span class="comment"># train_df.to_csv("/home/zkq/data/small_train.csv", index=False, sep='\t')</span></span><br><span class="line">        val_df = shuffle(val_df) <span class="comment">#打乱每行顺序</span></span><br><span class="line">        val_df.to_csv(targetname , index=<span class="keyword">False</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">'Finished Tran'</span>)</span><br><span class="line">    <span class="comment"># f_train.close()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    label_dict = &#123;<span class="string">'星座'</span>: <span class="string">'0'</span>, <span class="string">'股票'</span>: <span class="string">'1'</span>, <span class="string">'房产'</span>: <span class="string">'2'</span>, <span class="string">'时尚'</span>: <span class="string">'3'</span>, <span class="string">'体育'</span>: <span class="string">'4'</span> , <span class="string">'社会'</span>:<span class="string">'5'</span>,<span class="string">'家居'</span>:<span class="string">'6'</span>,<span class="string">'游戏'</span>:<span class="string">'7'</span>,<span class="string">'彩票'</span>:<span class="string">'8'</span>,<span class="string">'科技'</span>:<span class="string">'9'</span>,<span class="string">'教育'</span>:<span class="string">'10'</span>,<span class="string">'时政'</span>:<span class="string">'11'</span>,<span class="string">'娱乐'</span>:<span class="string">'12'</span>,<span class="string">'财经'</span>:<span class="string">'13'</span>&#125;</span><br><span class="line">    <span class="comment"># category = ['星座', '股票', '房产', '时尚', '体育', '社会', '家居', '游戏', '彩票', '科技', '教育', '时政', '娱乐', '财经']</span></span><br><span class="line">    <span class="comment">#合并为一个文件</span></span><br><span class="line">    <span class="comment"># corpus = load_data_to_mini('/home/zkq/data/THUCNews', '/home/zkq/data/thuc_train.csv', 1000)</span></span><br><span class="line">    corpus = load_data_to_mini(<span class="string">'/home/zkq/data/THUCNews'</span>, <span class="string">'/home/zkq/data/thuc_val.csv'</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天处理了一下THUC的新闻数据集，具体代码在下面。&lt;/p&gt;
&lt;h2 id=&quot;收获&quot;&gt;&lt;a href=&quot;#收获&quot; class=&quot;headerlink&quot; title=&quot;收获&quot;&gt;&lt;/a&gt;收获&lt;/h2&gt;&lt;p&gt;sklearn里的shuffle可以将数据打乱，我在之前处理南开数据集的时候忽视了这一点。同样，pandas中的sample()也是同样的作用，numpy库中的方法不推荐，会导致内存溢出。&lt;/p&gt;
&lt;h2 id=&quot;总结&quot;&gt;&lt;a href=&quot;#总结&quot; class=&quot;headerlink&quot; title=&quot;总结&quot;&gt;&lt;/a&gt;总结&lt;/h2&gt;&lt;p&gt;离开实验室的时候跑了TextCNN的模型，结果到家看了一下结果，从第九个epoch开始准确率都为1（一共15个epochs）。一开始我纳闷为啥准确率这么高，因为在复旦数据集上也就0.86左右的准确率。后来看了一下下面的代码。最后两行是我生成训练集和测试集的方法，仔细看知道了测试集就是训练集的一个子集！怪不得准确率这么高，因为已经告诉你label了啊！明天重新生成一下训练集和测试集。&lt;br&gt;
    
    </summary>
    
      <category term="笔记" scheme="http://yoursite.com/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>scipy.stats.spearmanr用法</title>
    <link href="http://yoursite.com/2018/08/13/scipy-stats-spearmanr%E7%94%A8%E6%B3%95/"/>
    <id>http://yoursite.com/2018/08/13/scipy-stats-spearmanr用法/</id>
    <published>2018-08-13T08:26:00.000Z</published>
    <updated>2018-08-13T08:28:06.862Z</updated>
    
    <content type="html"><![CDATA[<p>计算<strong>Spearman秩相关系数</strong>和<strong>P值</strong>（非相关性检验）。<br>在计算word similarity时用到的。具体用法是数据集中每行有一对词和人工标注的相关性，如(李白 诗 9.2)。程序先从词向量中读取两个词的向量，求得两个向量的余弦相似性，再用spearmanr求得相关系数和P值。最后的实验分析用的是相关系数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;计算&lt;strong&gt;Spearman秩相关系数&lt;/strong&gt;和&lt;strong&gt;P值&lt;/strong&gt;（非相关性检验）。&lt;br&gt;在计算word similarity时用到的。具体用法是数据集中每行有一对词和人工标注的相关性，如(李白 诗 9.2)。程序先从词向量中读取两个
      
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>keras中的embedding层</title>
    <link href="http://yoursite.com/2018/08/08/keras%E4%B8%AD%E7%9A%84embedding%E5%B1%82/"/>
    <id>http://yoursite.com/2018/08/08/keras中的embedding层/</id>
    <published>2018-08-08T01:23:20.000Z</published>
    <updated>2018-08-08T02:04:32.804Z</updated>
    
    <content type="html"><![CDATA[<h2 id="keras中的Embedding层"><a href="#keras中的Embedding层" class="headerlink" title="keras中的Embedding层"></a>keras中的Embedding层</h2><p>将索引映射为固定维度的稠密向量，如[[4],[20]]-&gt;[[0.25,0.1],[0.6,-0.2]]。<br>Embedding层只能作为模型的第一层。</p><h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><ol><li>从头训练<br>就像word2vec一样, 这一层是可学习的, 用随机数initialize , 通过BP去调整.</li><li>pre-trained + fine tuning<br>用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 然后继续学习.</li><li>pre-trained + static<br>用其他网络(如 word2vec) 训练好的现成的词向量, 作为初始化参数, 并且这些参数保持固定, 不参与网络的学习.<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer=<span class="string">'uniform'</span>, embeddings_regularizer=<span class="keyword">None</span>, activity_regularizer=<span class="keyword">None</span>, embeddings_constraint=<span class="keyword">None</span>, mask_zero=<span class="keyword">False</span>, input_length=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></li></ol><p>input_dim：大或等于0的整数，字典长度（词汇量），即输入数据最大下标+1<br>output_dim：大于0的整数，代表词向量的维度<br>embeddings_initializer: 初始化方法<br>embeddings_regularizer: 嵌入矩阵的正则项，为Regularizer对象<br>embeddings_constraint: 嵌入矩阵的约束项，为Constraints对象<br>mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为$|vocabulary| + 1$。<br>input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;keras中的Embedding层&quot;&gt;&lt;a href=&quot;#keras中的Embedding层&quot; class=&quot;headerlink&quot; title=&quot;keras中的Embedding层&quot;&gt;&lt;/a&gt;keras中的Embedding层&lt;/h2&gt;&lt;p&gt;将索引映射为固定维度
      
    
    </summary>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="keras" scheme="http://yoursite.com/tags/keras/"/>
    
  </entry>
  
  <entry>
    <title>文本分类综述</title>
    <link href="http://yoursite.com/2018/08/03/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/08/03/文本分类综述/</id>
    <published>2018-08-03T03:58:38.609Z</published>
    <updated>2018-08-03T03:58:38.609Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了神经网络在文本分类方面的一些应用，其中多篇论文被广泛引用。如果有最新的相关研究会及时更新。<br>文章将按照这些论文提出年份展开介绍，发展历史如下图所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B015.jpg" alt="此处输入图片的描述"><br><a id="more"></a></p><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><p>文章提出的方法类似于word2vec的cbow模型，并在此基础上加上bag of n-grams（考虑单词的顺序关系），下图是FastText文本分类的模型，$w$是语句中的词语，词语的向量相加求平均值作为文本表示然后做一个线性分类。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B02.jpg" alt="此处输入图片的描述"><br>但与cbow不同的是，fasttext不是预测中心词，而是直接预测标签。模型以语句中的词语作为输入，输出语句属于各类别上的概率。<br>    模型比较简单，训练速度很快，但是准确率不高。</p><h2 id="TextCNN2014"><a href="#TextCNN2014" class="headerlink" title="TextCNN2014"></a>TextCNN2014</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B03.jpg" alt="此处输入图片的描述"><br>输入层将一个句子所有单词的词向量拼接成一个矩阵，每一行代表一个词。图中有两个channels，原文中用的是static和non-static，即使用的词向量是否随着训练发生变化；也可以使用不同的词向量方法生成的词向量作为不同channel。对于未登录词的向量，使用0或随机正数来填充。<br>    卷积层的每个卷积核的大小为$filter<em>-size\times embedding</em>-size$。$filter<em>-size$代表卷积核纵向上包含的单词个数，即认为相邻几个词之间有词序关系，代码里使用的是[3，4，5]。$embedding</em>-size$就是词向量的维度。每个卷积核计算完成之后我们就得到了1个列向量，代表着该卷积核从句子中提取出来的特征。<br>池化层使用Max-over-time Pooling的方法。这种方法就是简单地从之前的Feature Map中提出最大的值（文中解释最大值代表着最重要的信号）。可以看出，这种Pooling方式可以解决可变长度的句子输入问题（因为不管Feature Map中有多少个值，只需要提取其中的最大值）。最终池化层的输出为各个Feature Map的最大值，即一个一维的向量。<br>全连接层，为了将pooling层输出的向量转化为我们想要的预测结果，加上一个softmax层。文中还提到了过拟合的问题，在倒数第二层的全连接部分上使用<strong>Dropout</strong>技术，即对全连接层上的权值参数给予L2正则化的限制。这样做的好处是防止隐藏层单元自适应（或者对称），从而减轻过拟合的程度。<br><strong>本文使用的词向量是CBOW在Google News上的训练结果。</strong></p><h2 id="RCNN"><a href="#RCNN" class="headerlink" title="RCNN"></a>RCNN</h2><p>分析了Recursive Neural Network、RNN、CNN做文本分类的优缺点：<br>Recursive Neural Network效果完全依赖于文本树的构建，并且构建文本树所需的时间是$ O\left( n^2 \right)  $  。并且两个句子的关系也不能通过一颗树表现出来。因此不适合于长句子或者文本。<br>    RNN是有偏的模型，后面的词比前面的词更重要。<br>    CNN卷积核的尺寸难以设置。如果选小了容易造成信息的丢失；如果选大了，会造成巨大的参数空间。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B04.jpg" alt="此处输入图片的描述"><br>卷积层是一个BiRNN（双向LSTM），通过正向和反向循环来构建一个单词的上文和下文，如下式：</p><script type="math/tex; mode=display">c_l\left( w_i \right) =f\left( W^{\left( l \right)}c_l\left( w_{i-1} \right) +W^{\left( sl \right)}e\left( w_{i-1} \right) \right)</script><script type="math/tex; mode=display">c_r\left( w_i \right) =f\left( W^{\left( r \right)}c_r\left( w_{i+1} \right) +W^{\left( sr \right)}e\left( w_{i+1} \right) \right)</script><p>得到上下文表示后，拼接表示当前词：</p><script type="math/tex; mode=display">x_i=\left[ c_l\left( w_i \right) ;e\left( w_i \right) ;c_r\left( w_i \right) \right]</script><p>使用$tanh$函数激活得到：</p><script type="math/tex; mode=display">y_{i}^{\left( 2 \right)}=\tanh \left( W^{\left( 2 \right)}x_i+b^{\left( 2 \right)} \right)</script><p>池化层使用最大池化，使用所有单词在每个维度上的最大值表示文本的信息。最后输出层使用softmax得到分类结果。<br>    <strong>本文使用的词向量是使用Skip-gram训练的中英文Wikipedia。</strong></p><h2 id="TextRNN"><a href="#TextRNN" class="headerlink" title="TextRNN"></a>TextRNN</h2><p>传统的RNN（LSTM）结构：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B05.jpg" alt="此处输入图片的描述"><br>下面的公式为LSTM中各门的公式：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B012.jpg" alt="此处输入图片的描述"><br>文章《Recurrent Neural Network for Text Classification with Multi-Task Learning》中介绍了RNN用于文本分类的模型设计，主要提出了以下三种模型：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B06.jpg" alt="此处输入图片的描述"><br>模型（a）中多任务共享LSTM结构以及公有的词向量；针对于任务$m$，输入$ \hat{x}_t $ 为：</p><script type="math/tex; mode=display">\hat{x}_{t}^{\left( m \right)}=x_{t}^{\left( m \right)}\oplus x_{t}^{\left( s \right)}</script><p>其中，$ x<em>{t}^{\left( m \right)} $ ，$ x</em>{t}^{\left( s \right)} $  分别表示任务私有的词向量和公有的词向量。最后一个时刻的hidden state则作为输入传入softmax。<br>模型（b）每个任务具有自己独立的LSTM层，但是每一时刻所有任务的hidden state则会和下一时刻的character一起作为输入，最后一个时刻的hidden state进行分类。作者修改了候选状态的计算公式：</p><script type="math/tex; mode=display">\tilde{c}_{t}^{\left( m \right)}=\tanh \left( W_{t}^{\left( m \right)}x_t+\sum_{i\in \left\{ m,n \right\}}{g^{\left( i\rightarrow m \right)}U_{c}^{\left( i\rightarrow m \right)}h_{t-1}^{\left( i \right)}} \right)</script><script type="math/tex; mode=display">g^{\left( i\rightarrow m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( i \right)}h_{t-1}^{\left( i \right)} \right)</script><p>模型（c）除了一个共享的BI-LSTM层用于获取共享信息，每个任务有自己独立的LSTM层，LSTM的输入包括每一时刻的character和BI-LSTM的hidden state，同模型2一样，作者修改了候选状态的计算公式：</p><script type="math/tex; mode=display">\tilde{c}_{t}^{\left( m \right)}=\tanh \left( W_{t}^{\left( m \right)}x_t+g^{\left( m \right)}U_{c}^{\left( m \right)}h_{t-1}^{\left( m \right)}+g^{\left( s\rightarrow m \right)}U_{c}^{\left( s \right)}h_{t}^{\left( s \right)} \right)</script><script type="math/tex; mode=display">g^{\left( m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( m \right)}h_{t-1}^{\left( s \right)} \right)</script><script type="math/tex; mode=display">g^{\left( s\rightarrow m \right)}=\sigma \left( W_{g}^{\left( m \right)}x_t+U_{g}^{\left( s\rightarrow m \right)}h_{t}^{\left( s \right)} \right)</script><p><strong>本文使用的词向量是使用Word2Vec训练的Wikipedia语料。</strong></p><h2 id="CharCNN"><a href="#CharCNN" class="headerlink" title="CharCNN"></a>CharCNN</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B07.jpg" alt="此处输入图片的描述"><br>首先对文本编码，使用的字母表共有69个字符，对其使用one-hot编码，外加一个全零向量（用于处理不在该字符表中的字符），所以共70个。经过6个卷积层和3个全连接层得到输出。<br>优点是是不需要使用预训练好的词向量和语法句法结构等信息，并且可以很容易的推广到所有语言。针对于汉语，作者使用拼音代替汉字实现的编码。<br>对于几百上千等小规模数据集，可以优先考虑传统方法，对于百万规模的数据集，CharCNN表现不错。CharCNN适用于用户生成数据(user-generated data)（如拼写错误，表情符号等）。</p><h2 id="GRNN（Conv、LSTM）"><a href="#GRNN（Conv、LSTM）" class="headerlink" title="GRNN（Conv、LSTM）"></a>GRNN（Conv、LSTM）</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B08.jpg" alt="此处输入图片的描述"><br>首先使用CNN/LSTM来建模句子表示，接下来使用双向GRU模型对句子表示进行编码得到文档表示，得到的文档表示用于Softmax情感分类。<br><strong>在上图中，底层的词向量是由word2vec预训练得到。</strong>使用CNN/LSTM学习得到句子的表示，这里会把变长的句子表示表示成相同维度的向量。卷积模型如下：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B09.jpg" alt="此处输入图片的描述"><br>filter的宽度分别取1，2，3来编码unigrams，bigrams和trigrams的语义信息。最后使用一个Average层捕获全局信息并转化为输出向量。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B010.jpg" alt="此处输入图片的描述"><br>使用GRU模型，输入是变长的句子向量，输出固定长度的文本向量，这里会对最后每个单元的输出向量进行取平均操作</p><h2 id="HAN（RNN-Attention）"><a href="#HAN（RNN-Attention）" class="headerlink" title="HAN（RNN+Attention）"></a>HAN（RNN+Attention）</h2><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B011.jpg" alt="此处输入图片的描述"><br>    一个句子中每个单词的重要性不同。一篇文档中每个句子的重要性也不同。因此本文主要思想是，首先考虑文档的分层结构：单词构成句子，句子构成文档，所以建模时也分这两部分进行。其次，不同的单词和句子具有不同的信息量，不能单纯的统一对待所以引入Attention机制。而且引入Attention机制除了提高模型的精确度之外还可以进行单词、句子重要性的分析和可视化，让我们对文本分类的内部有一定了解。模型主要可以分为四个部分，如上图所示：<br>    Word encoder和sentence encoder都是双向GRU，公式如下：<br>Word encoder中公式如下：</p><script type="math/tex; mode=display">x_{it}=W_ew_{it},t\in \left[ \text{1,}T \right]  \\ \vec{h}_{it}=\overrightarrow{GRU}\left( x_{it} \right) ,t\in \left[ \text{1,}T \right]  \\ \vec{h}_{it}=\overleftrightarrow{GRU}\left( x_{it} \right) ,t\in \left[ T,1 \right]</script><p>Word attention中公式如下：</p><script type="math/tex; mode=display">u_{it}=\tanh \left( W_wh_{it}+b_w \right)  \\ \alpha _{it}=\frac{\exp \left( u_{it}^{\top}u_w \right)}{\sum_t^{}{\exp \left( u_{it}^{\top}u_w \right)}} \\ s_i=\sum_t{\alpha _{it}h_{it}}</script><p>Sentence encoder中公式如下：</p><script type="math/tex; mode=display">\vec{h}_i=\overrightarrow{GRU}\left( s_i \right) ,i\in \left[ \text{1,}L \right]  \\ \vec{h}_i=\overleftrightarrow{GRU}\left( s_i \right) ,i\in \left[ L,1 \right]</script><p>Sentence attention中公式如下：</p><script type="math/tex; mode=display">u_i=\tanh \left( W_sh_i+b_s \right)  \\ \alpha _i=\frac{\exp \left( u_{i}^{\top}u_s \right)}{\sum_i^{}{\exp \left( u_{i}^{\top}u_s \right)}} \\ v=\sum_i{\alpha _ih_i}</script><p>每个词语对应的hidden vector的输出经过变换（转置和$tanh$）之后和$u_w$ 相互作用（点积），结果就是每个词语的权重。加权以后就可以产生整个sentence的表示。从高一级的层面来看(hierarchical的由来)，每个document有$L$个句子组成，那么这$L$个句子就可以连接成另一个sequence model, 同样是双向GRU，同样的对输出层进行变换后和 相互作用，产生每个句子的权重，加权以后我们就产生了对整个document的表示。最后用softmax就可以产生对分类的预测。</p><h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B013.jpg" alt="此处输入图片的描述"><br>表征学习是自然语言处理中的一个基本问题。文章《Learning Structured Representation for Text Classification via Reinforcement Learning》研究如何学习文本分类的结构化表示。与大多数既不使用结构也不依赖于预定义结构的现有表示模型不同，作者提出了一种强化学习（RL）方法，通过自动地优化结构来学习句子表示。<br>作者在文章中提出两种结构表示模型（模型第二部分）：Information Distilled LSTM (ID-LSTM) 和 Hierarchically Structured LSTM (HS-LSTM)。其中 ID-LSTM 只选择重要的任务相关的单词，HS-LSTM 则去发现句子中的短语结构。两种表示模型中的结构发现被表述为一个顺序决策问题，结构发现的当前决策影响随后的决策，这可以通过策略梯度 RL 来解决。<br>结果表明，这种方法可以通过识别重要的词或任务相关的结构而无需明确的结构注释来学习任务友好的表示，从而获得有竞争力的表现。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了神经网络在文本分类方面的一些应用，其中多篇论文被广泛引用。如果有最新的相关研究会及时更新。&lt;br&gt;文章将按照这些论文提出年份展开介绍，发展历史如下图所示：&lt;br&gt;&lt;img src=&quot;http://p5vuwy2ht.bkt.clouddn.com/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E7%BB%BC%E8%BF%B015.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="文本分类" scheme="http://yoursite.com/categories/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="文本分类" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>词向量综述</title>
    <link href="http://yoursite.com/2018/07/23/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2018/07/23/词向量综述/</id>
    <published>2018-07-23T08:14:07.000Z</published>
    <updated>2018-08-03T03:45:28.774Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了词向量的多种表示方法，主要为word2vec基础上的多种模型，侧重于中文词向量模型。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%91%E5%B1%95%E5%9B%BE.jpg" alt="此处输入图片的描述"><br>2018年7月23日更新，在NLP课上宗成庆老师在讲词向量时提到，除了基于文本的词汇语义表示模型，还有基于图像、语音、多模态信息学习词汇语义表示等，这一些有空再补充。<br><a id="more"></a></p><h2 id="什么是词向量"><a href="#什么是词向量" class="headerlink" title="什么是词向量"></a>什么是词向量</h2><p>自然语言是符合语法、具有一定的信息、为人类所能理解并进行沟通的语言。但当我们使用计算机来处理自然语言时，计算机是无法直接理解这些符号的（汉字、字母、标点等）。这些符号需要经过数值化后才能输入计算机进行后续的处理。然而只用单个数字来表示单词是毫无意义的，它只是一个id，无法体现单词的属性，因此需要将单词进行向量化。</p><h2 id="词向量的表示"><a href="#词向量的表示" class="headerlink" title="词向量的表示"></a>词向量的表示</h2><p>词向量的表示主要有两种方式：独热编码（One-hot Representation）和分布式表示（Distributed Representation）。</p><h3 id="One-hot表示"><a href="#One-hot表示" class="headerlink" title="One-hot表示"></a>One-hot表示</h3><p>独热编码是最直观、也最常用的方法。是一种稀疏的表示方式。其思路为将每个词都表示成一个很长的向量，该向量的维度等于词表大小，其中只有一个维度的值为1（维度的位置等于词的id），其余维度都为0。举个例子：<br>假设我们从语料库中为每个词分配一个数字id（从0开始分配），得到“话筒”这个词的id为3，“麦克”为8，那么用独热编码为：</p><blockquote><p>话筒：[0 0 0 1 0 0 0 0 0 0 …]<br>麦克: [0 0 0 0 0 0 0 0 1 0 …]</p></blockquote><p>每个词都是茫茫0海中的一个1。如果要编程实现的话，用 Hash 表给每个词分配一个id就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。<br>如此简洁的方法自然有它的缺点：<br>1、向量维度会随着词表增大而增大：</p><ul><li>存储效率低</li><li>若词表扩容，则每个词维度也必须相应增加</li><li>若某个词出现次数很少的话，则相应的权重会容易被错误估计</li></ul><p>2、“词汇鸿沟”问题：每个维度彼此正交，即所有词彼此孤立，无法表示词与词之间的相关信息，例如余弦相似度。由于任意一对向量 的余弦相似度为</p><script type="math/tex; mode=display">\frac{x^{\top}y}{\lVert x \rVert \lVert y \rVert}\in \left[ -\text{1,}1 \right]</script><p>任何一对词的one-hot向量的余弦相似度都为0。</p><h3 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h3><p>1954 年，Harris 提出<strong>分布假说（distributional hypothesis）</strong>，即“上下文相似的词，其语义也相似”，为词的分布表示提供了理论基础。Firth 在1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（a word is characterized by the company it keeps）。<br>基于分布假说，研究人员提出了多种词表示模型：如基于矩阵的LSA 模型、基于聚类的Brown clustering 模型以及最近使用广泛的神经网络词表示模型。神经网络模型生成的词表示通常被称为<strong>词向量（word embedding）</strong>，是一个低维的实数向量表示，通过这种表示，可以直接对词之间的相似度进行刻画。</p><h4 id="基于矩阵的分布式表示"><a href="#基于矩阵的分布式表示" class="headerlink" title="基于矩阵的分布式表示"></a>基于矩阵的分布式表示</h4><p>利用语料库构建一个$ W\times C $ 共现矩阵$F$，矩阵每一行代表一个词，每一列是某种上下文表示方法。$W$是词表大小。矩阵每个单元的值可以是二值（表示二者是否共现），可以是未经处理的共现次数，也可以是经过处理后的共现tf-idf值，等等。很多可衡量两个对象之间关联的指标都可以用来作为矩阵中每个单元的值。<br>由于矩阵每一行的维度大小都等于词表大小，不便计算，所以需要进行降维。降维技术可以减少噪声带来的影响，但也可能损失一部分信息。最常用的分解技术包括奇异值分解（SVD）、非负矩阵分解（NMF）、典型关联分析（Canonical Correlation Analysis，CCA）、Hellinger PCA（HPCA）。<br>基于矩阵的分布表示在这些步骤的基础上，衍生出了若干不同方法，如经典的LSA就是使用tf-idf 作为矩阵元素的值，并使用SVD分解，得到词的低维向量表示。在这类方法中，最新的为GloVe 模型，下文简单介绍这一模型。</p><h5 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h5><p>总体上看，GloVe 模型是一种对“词-词”矩阵进行分解从而得到词表示的方法。矩阵第$i$行第$j$列的值为词$v<em>i$与词$v_j$在语料中的共现次数$ x</em>{ij} $的对数。在矩阵分解步骤，GloVe 模型借鉴了推荐系统中基于隐因子分解（Latent Factor Model）的方法，在计算重构误差时，只考虑共现次数非零的矩阵元素，同时对矩阵中的行和列加入了偏移项。具体为最小化下式：</p><script type="math/tex; mode=display">\sum_{ik}{f\left( x_{ik} \right) \left( w_{i}^{T}w_k+b_i+b_k-\log x_{ik} \right) ^2}</script><p>其中$ w_i $为词$ v_i $ 作为目标词时的词向量，$ w_j $ 为词$ v_j $ 作为上下文时的词向量，$ b_i $ 、$ b_k $ 为针对词表中各词的偏移向量， $ f\left( x \right)  $ 是一个加权函数，对低频的共现词对进行衰减，减少低频噪声带来的误差，定义为：</p><script type="math/tex; mode=display">f\left( x \right) =\begin{cases}     \left( \frac{x}{x_{\max}} \right) ^{\alpha}\ \ \text{如果}x<x_{\max}\\     \text{1                 其他情况}\\ \end{cases}</script><h4 id="基于聚类的分布式表示"><a href="#基于聚类的分布式表示" class="headerlink" title="基于聚类的分布式表示"></a>基于聚类的分布式表示</h4><p>基于聚类的分布式表示也被称为分布聚类，通过聚类的方法构建词与其上下文之间的关系。最经典的方法是布朗聚类（Brown clustering）。布朗聚类是一种层级聚类方法，聚类结果为每个词的多层类别体系。因此可以根据两个词的公共类别判断这两个词的语义相似度。具体而言，布朗聚类需要最大化以下似然，其中$ c_i $ 为词$ w_i $ 对应的类别：</p><script type="math/tex; mode=display">P\left( w_i|w_{i-1} \right) =P\left( w_i|c_i \right) P\left( c_i|c_{i-1} \right)</script><p>布朗聚类只考虑了相邻词之间的关系，也就是说，每个词只使用它的上一个词，作为上下文信息。</p><h4 id="基于神经网络的分布式表示"><a href="#基于神经网络的分布式表示" class="headerlink" title="基于神经网络的分布式表示"></a>基于神经网络的分布式表示</h4><p>神经网络词向量表示技术通过神经网络技术对上下文，以及上下文与目标词之间的关系进行建模。神经网络词向量模型与其它分布表示方法一样，均基于分布假说，核心依然是上下文的表示以及上下文与目标词之间的关系的建模。构建上下文与目标词之间的关系，最自然的一种思路就是使用语言模型。从历史上看，早期的词向量只是神经网络语言模型的副产品。同时，神经网络语言模型对当前词向量的发展方向有着决定性的作用。</p><h5 id="NNLM2001"><a href="#NNLM2001" class="headerlink" title="NNLM2001"></a>NNLM2001</h5><p>2001年，Bengio 等人正式提出神经网络语言模型（Neural Network Language Model，NNLM）。该模型在学习语言模型的同时，也得到了词向量。NNLM对$n$元语言模型进行建模，估算$ P\left( w<em>i|w</em>{i-\left( n-1 \right)},…,w<em>{i-1} \right)  $  的值。也就是对语料中一段长度为$n$ 的序列 $ w</em>{i-\left( n-1 \right)},…,w_{i-1},w_i $ , 元语言模型需要最大化以下似然：</p><script type="math/tex; mode=display">P\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right)</script><p>其中， $ w<em>i $ 为需要通过语言模型预测的词（目标词）。对于整个模型而言，输入为条件部分的整个词序列： $ w</em>{i-\left( n-1 \right)},…,w<em>{i-1} $ ，输出为目标词的分布。<br><strong>神经网络语言模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B01.jpg" alt="神经网络语言模型结构图"><br>输入层将前 $n-1$个词 $ w</em>{i-\left( n-1 \right)},…,w_{i-1} $ 通过查表映射成对应的词向量，然后将这 $n-1$个向量顺序拼接，形成$x$ ：</p><script type="math/tex; mode=display">x=\left[ e\left( w_{i-\left( n-1 \right)} \right) ;...;e\left( w_{i-2} \right) ;e\left( w_{i-1} \right) \right]</script><p>输入层完成对 $x$的拼接后，模型将其依次送入隐藏层$h$ 和输出层 $y$。</p><script type="math/tex; mode=display">h=\tanh \left( b^h+Hx \right)</script><script type="math/tex; mode=display">y=b^y+Wx+Uh</script><p>其中 $ H\in \mathbb{R}^{\left| h \right|\times \left( n-1 \right) \left| e \right|} $ 为输入层到隐藏层的权重矩阵， $ U\in \mathbb{R}^{\left| v \right|\times \left| h \right|} $ 为隐藏层到输出层的权重矩阵，$ \left| V \right| $  表示词表大小， $ \left| e \right| $ 表示词向量维度， $ \left| h \right| $ 为隐藏层维度，$ b^h $  、 $ b^y $ 均为偏置。矩阵$W$ 表示从输入层到输出层的直连边权重矩阵。<br>输出层一共有 $ \left| V \right| $ 个元素，对应下一个词为词表中某个词的可能性。使用softmax将输出值归一化成概率。</p><script type="math/tex; mode=display">p\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right) =\frac{\exp \left( y\left( w_i \right) \right)}{\sum_{k=1}^{\left| V \right|}{\exp \left( y\left( v_k \right) \right)}}</script><p>对于整个预料而言，语言模型需要最大化：</p><script type="math/tex; mode=display">\sum_{w_{i-\left( n-1 \right) :i\in \mathbb{D}}}{\log \left( P\left( w_i \right) |w_{i-\left( n-1 \right)},...,w_{i-1} \right)}</script><p>训练时使用随机梯度下降来优化上述目标。每次迭代，随即从语料中选取一段样本作为训练样本，使用下式进行一次梯度迭代：</p><script type="math/tex; mode=display">\theta \gets \theta +\alpha \frac{\partial \log P\left( w_i|w_{i-\left( n-1 \right)},...,w_{i-1} \right)}{\partial \theta}</script><p>其中，$ \alpha  $ 是学习率； $ \theta  $ 为模型中所有参数，包括词向量和网络模型中的权重及偏置。<br>值得注意的是，神经网络语言模型中的词向量出现了两次。在输入层，各词的词向量存在于一个$ \left| e \right|\times \left| V \right| $  维的实数矩阵中，每一列对应一个词向量。隐藏层到输出层的权重矩阵$U$ 的维度是$ \left| V \right|\times \left| h \right| $  ，可看作$ \left| V \right| $  个$ \left| h \right| $  维的行向量，其中每个向量都可以看做某个词的另一种表示$ e’ $  。我们将 $ e\left( w \right)  $ 称为词的上下文表示，将 $ e’\left( w \right)  $ 称为词的目标词表示，通常将$e$ 作为词向量。</p><h5 id="循环神经网络语言模型"><a href="#循环神经网络语言模型" class="headerlink" title="循环神经网络语言模型"></a>循环神经网络语言模型</h5><p>Mikolov等人提出的循环神经网络语言模型（Recurrent Neural Network based Language Model，RNNLM）直接对 $ P\left( w<em>i|w_1,w_2,…,w</em>{i-1} \right)  $ 进行建模。因此，RNNLM 可以利用所有的上文信息，预测下一个词，其模型结构下图所示。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B02.jpg" alt="循环神经网络语言模型结构图"><br>RNNLM的核心在于其隐藏层算法：</p><script type="math/tex; mode=display">h\left( i \right) =\phi \left( e\left( w_i \right) +Wh\left( i-1 \right) \right)</script><p>其中，$ \phi  $  为非线性激活函数；$h(i)$ 表示文本中第 $i$个词$w_i$ 所对应的隐藏层，该隐藏层由当前词的词向量 $ e\left( w_i \right)  $ 以及上一个词对应的隐藏层$ h\left( i-1 \right)  $  结合得到。通过迭代，每个隐藏层都包含了此前所有词的信息。RNNLM的输出层计算方法与NNLM的输出层一致。</p><h5 id="C-amp-W模型"><a href="#C-amp-W模型" class="headerlink" title="C&amp;W模型"></a>C&amp;W模型</h5><p>C&amp;W模型结构图:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B03.jpg" alt="此处输入图片的描述"><br>不同于之前的模型，C&amp;W模型直接以生成词向量为目标。他们没有去近似地求$P(w<em>t |w_1,w_2,…,w</em>(t-1))$，而是直接去尝试近似$P(w<em>1,w_2,…,w_t)$。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续 $n$个词的打分 $f(w</em>(t-n+1),…,w_(t-1),w_t)$打分越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。有了这个对 $f$的假设，C&amp;W 就直接使用 pair-wise 的方法训练词向量。具体而言，就是最小化下面的目标函数。</p><script type="math/tex; mode=display">\sum_{x\in \mathfrak{X}}{\sum_{w\in \mathfrak{D}}{\max\text{\{0,}1-f\left( x \right) +f\left( x^{\left( w \right)} \right) \}}}</script><p> $ \mathfrak{X} $ 为训练集中的所有连续的$n$ 元短语， $ \mathfrak{D} $ 是整个字典。第一个求和枚举了训练语料中的所有的$n$ 元短语，作为正样本。第二个对字典的枚举是构建负样本。 $ x^{\left( w \right)} $ 是将短语$x$ 的最中间的那个词，替换成$w$ 。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当作负样本也不影响大局）。同时，由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类效果。再回顾这个式子， $x$是正样本， $ x^{\left( w \right)} $ 是负样本，$ f\left( x \right)  $  是对正样本的打分，$ f\left( x^{\left( w \right)} \right)  $  是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高1分。$f$ 函数的结构和NNLM中的网络结构基本一致。同样是把窗口中的$n$ 个词对应的词向量串成一个长的向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同之处在于C&amp;W的输出层只有一个节点，表示得分，而不像NNLM那样的有$ \left| V \right| $  个节点，这么做可以大大降低计算复杂度。</p><h5 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h5><p>Mikolov 等人在2013年开发了word2vec工具，包含CBOW（Continuous Bag of Words）和Skip-gram 两种模型。</p><h6 id="CBOW模型"><a href="#CBOW模型" class="headerlink" title="CBOW模型"></a>CBOW模型</h6><p><strong>CBOW模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B04.jpg" alt="此处输入图片的描述"><br>简单来讲，CBOW用上下文作为输入去预测目标词。该模型一方面根据C&amp;W 模型的经验，使用一段文本的中间词作为目标词；另一方面，又以NNLM作为蓝本，并在其基础上做了两个简化。一、CBOW 没有隐藏层，去掉隐藏层之后，模型从神经网络结构直接转化为log 线性结构，与Logistic 回归一致。log 线性结构比三层神经网络结构少了一个矩阵运算，大幅度地提升了模型的训练速度。二、CBOW 去除了上下文各词的词序信息，使用上下文各词词向量的平均值（论文中求和，实际工具求平均），代替神经网络语言模型使用的上文各词词向量的拼接。形式化地，CBOW 模型对于一段训练样本$ w_{i-\left( n-1 \right)},…,w_i $  ，输入为：</p><script type="math/tex; mode=display">x=\frac{1}{n-1}\sum_{w_j\in c}{e\left( wj \right)}</script><p>然后根据上下文表示，对目标词进行预测：</p><script type="math/tex; mode=display">P\left( w|c \right) =\frac{\exp \left( e'\left( w \right) ^Tx \right)}{\sum_{w'\in \mathbb{V}}{\exp \left( e'\left( w' \right) ^Tx \right)}}</script><p>上述二式，目标词$w$ 与上下文$c$ 的定义与C&amp;W模型一致。优化目标与神经网络语言模型一致，即最大化：</p><script type="math/tex; mode=display">\sum_{\left( w,c \right) \in \mathbb{D}}{\log P\left( w|c \right)}</script><h6 id="Skip-gram模型"><a href="#Skip-gram模型" class="headerlink" title="Skip-gram模型"></a>Skip-gram模型</h6><p><strong>Skip-gram模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B05.jpg" alt="此处输入图片的描述"><br>与CBOW相反，Skip-gram用目标词作为输入去预测上下文。为了与CBOW统一，可以将其描述为：每次从目标词$w$ 的上下文$c$ 中选择一个词，将其词向量作为模型的输入$x$ ，也就是上下文的表示，这样Skip-gram的任务也变成了通过上下文预测目标词，优化目标为：</p><script type="math/tex; mode=display">\sum_{\left( w,c \right) \in \mathbb{D}}{\sum_{w_j\in c}{\log P\left( w|w_j \right)}}</script><p>其中，</p><script type="math/tex; mode=display">P\left( w|w_j \right) =\frac{\exp \left( e'\left( w \right) ^Te\left( w_j \right) \right)}{\sum_{w'\in \mathbb{V}}{\exp \left( e'\left( w' \right) ^Te\left( w_j \right) \right)}}</script><h5 id="fastText"><a href="#fastText" class="headerlink" title="fastText"></a>fastText</h5><p>word2vec在词汇建模方面产生了巨大的贡献，然而其依赖于大量的文本数据进行学习，如果一个word出现次数较少那么学到的vector质量也不理想。针对这一问题作者提出使用subword信息来弥补这一问题，简单来说就是通过词缀的vector来表示词。比如unofficial是个低频词，其数据量不足以训练出高质量的vector，但是可以通过un+official这两个高频的词缀学习到不错的vector。<br>方法上，本文沿用了word2vec的skip-gram模型，主要区别体现在特征上。word2vec使用word作为最基本的单位，即通过中心词预测其上下文中的其他词汇。而subword model使用字母n-gram作为单位，本文n取值为3~6。这样每个词汇就可以表示成一串字母n-gram，一个词的embedding表示为其所有n-gram的和。这样我们训练也从用中心词的embedding预测目标词，转变成用中心词的n-gram embedding预测目标词。</p><h5 id="CWE"><a href="#CWE" class="headerlink" title="CWE"></a>CWE</h5><p><strong>CWE模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B06.jpg" alt="此处输入图片的描述"><br>在word2vec的CBOW模型基础上，在词向量生成部分进行了改进，引入了单个汉字的信息（论文主要针对的是中文），提升了词向量生成的质量。具体实现方法是将CBOW中 $x$的取值办法由上下文词向量相加求均值：</p><script type="math/tex; mode=display">x_o=\frac{1}{2k}\sum_{j=i-k,...,i+k}{x}_j</script><p>变成词语中所有单字的向量与词向量相加求均值：</p><script type="math/tex; mode=display">x=\frac{1}{2}\{w+\frac{1}{N_j}\sum_{k=1}^{N_J}{c_k}\text{)}</script><p>其中， $ N_j $ 是单词中的汉字个数， $w$是字向量。<br>另外文章还提出了三种办法以解决同一汉字在不同词语中的不同语义问题，分别为：Position-based Character Embedding、Cluster-based Character Embedding、Nonparametric Cluster-based Character Embeddings。</p><h5 id="JWE2017"><a href="#JWE2017" class="headerlink" title="JWE2017"></a>JWE2017</h5><p><strong>JWE模型结构图</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B07.jpg" alt="此处输入图片的描述"><br>根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起联合学习词向量。其中$ w<em>i $  是目标词， $ w</em>{i-1} $ 是目标词左边的词， $ w<em>{i+1} $ 是目标词右边的词；$ c</em>{i-1} $  是目标词左边的词中的单字， $ c<em>{i+1} $ 是目标词右边的词中的单字；$ s</em>{i-1} $  是目标词左边的词拆分的字件，$ s_{i+1} $  是目标词右边的词拆分的字件, $ s_i $ 是目标词的字件；<br>模型的目标是最大化以下对数似然：</p><script type="math/tex; mode=display">L\left( w_i \right) =\sum_{k=1}^3{\log P\left( w_i|h_{i_k} \right)}</script><p>其中, $ h<em>{i_1} $ ， $ h</em>{i<em>2} $ ， $ h</em>{i_3} $ 分别为上下文单词、单字、组件的组合。</p><script type="math/tex; mode=display">P\left( w_i|h_{i_k} \right) =\frac{\exp \left( h_{i_k}^{T}\hat{v}_{w_i} \right)}{\sum_{j=1}^N{\exp \left( h_{i_k}^{T}\hat{v}_{w_j} \right)}}</script><script type="math/tex; mode=display">h_{i_1}=\frac{1}{2T}\sum_{-T\leqslant j\leqslant T,j\ne 0}{v_{w_{i+j}}}</script><p>在以上3个公式中， $ v<em>{w_i} $ ,$ v</em>{c<em>i} $  , $ v</em>{s<em>i} $ 分别为上下文、单字、字件的“<strong>输入向量</strong>”， $ \hat{v}</em>{w<em>j} $ 是“<strong>输出向</strong>量”；$ h</em>{i<em>1} $  为上下文“输入向量”的均值，同理， $ h</em>{i<em>2} $ 为上下文单字“输入向量”的均值，$ h</em>{i_3} $  为上下文字件“输入向量”的均值。<br>给定语料$ D $  ,模型的目标是最大化：</p><script type="math/tex; mode=display">L\left( D \right) =\sum_{w_i\in D}{L\left( w_i \right)} $ #####GWE2017**GWE模型结构图（一）**:![此处输入图片的描述][9]通过convolutional auto-encoder（convAE）提取词中字的glyph特征，取均值后与上下文词向量均值及上下文中字向量的均值合并，得到新的上下文表示，预测目标词。词向量表示为： $$ \vec{w}_{i}^{ctxG}=\vec{w}_i+\frac{1}{\left| C\left( i \right) \right|}\sum_{c_j\in C\left( i \right)}{\left( \vec{c}_j+\vec{g}_j \right)}</script><p><strong>GWE模型结构图（二）</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B09.jpg" alt="此处输入图片的描述"><br>这种模型将当前词中字的glyph特征与上下文结合，一起预测当前词。<br>以下两种模型直接从特征中学习词向量，没有使用上下文信息。</p><p><strong>GWE模型结构图（三）</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B010.jpg" alt="此处输入图片的描述"><br>在Skip-gram的基础上，以特征作为输入，两层GRU，两层全连接ELU，预测当前词的上下文。</p><p><strong>GWE模型结构图（四）</strong>:<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B011.jpg" alt="此处输入图片的描述"><br>类似于GloVe模型，将原模型中词向量部分改写为当前词的特征。</p><h5 id="cw2vec"><a href="#cw2vec" class="headerlink" title="cw2vec"></a>cw2vec</h5><p>cw2vec在Skip-Gram基础之上进行改进，把词语的n-gram笔画特征信息代替词语进行训练，cw2vec模型如下图：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B012.jpg" alt="此处输入图片的描述"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E7%BB%BC%E8%BF%B013.jpg" alt="此处输入图片的描述"><br> 首先将词语分割，获取中文字符。然后获取笔画信息，并合并笔画信息。将笔画信息数字化。最后提取笔画信息的n-gram特征。</p><h2 id="词向量的评价"><a href="#词向量的评价" class="headerlink" title="词向量的评价"></a>词向量的评价</h2><p>根据来斯惟的博士论文《基于神经网络的词和文档语义向量》，词向量的用法可以分为三大类，分别为：一、利用词向量的语言学特性完成任务；二、将词向量作为特征，提高自然语言处理任务的性能；三、将词向量作为神经网络的初始值，提升神经网络模型的优化效果。他在这三大类用法的基础上，选取了八个有代表性的具体任务，作为词向量的评价指标。</p><h3 id="词向量的语言学特征"><a href="#词向量的语言学特征" class="headerlink" title="词向量的语言学特征"></a>词向量的语言学特征</h3><p>各词向量模型均基于分布假说设计而成，因此无论哪种词向量模型，都会符合分布假说所提出的性质：具有相似上下文的词，会拥有相似的语义，并且其词向量的空间距离更接近。本文选取了三个代表性任务。</p><h4 id="语义相关性-ws"><a href="#语义相关性-ws" class="headerlink" title="语义相关性(ws)"></a>语义相关性(ws)</h4><p>衡量语义相关性最经典的是WordSim353 数据集，该数据集包含了353 个词对，其中每一个词对有至少十位标注者对其进行0 到10 之间的打分，分数越高表示标注人员认为这两个词的语义更相关或者更相似。例如，词对“student,professor”的平均打分为6.81，而词对“professor, cucumber”的打分为0.31。评价时，对于每个词对，本文使用所有标注者打分的平均值作为参考得分$X$ ，以词对的两个词向量的余弦距离作为模型得到的相关性得分$Y$ ，并衡量这两组数值之间的皮尔逊相关系数。皮尔逊相关系数衡量了两个变量之间的线性相关性，值在-1到1之间，如果模型得到的打分与人工标注的打分一致，得分就越高。具体而言，$X$ 和$Y$ 之间的皮尔逊相关系数定义为 $X$和 $Y$之间的协方差与它们标准差的商：</p><script type="math/tex; mode=display">\rho _{X,y}=\frac{cov\left( X,Y \right)}{\sigma _X\sigma _Y}</script><h4 id="同义词检测-tfl"><a href="#同义词检测-tfl" class="headerlink" title="同义词检测(tfl)"></a>同义词检测(tfl)</h4><p>托福考试（TOEFL）数据集包含80 个单选题，每个题目包含一个问题词以及四个选项，要求从四个选项中选出一个与问题词同义的词语。例如：问题“levied”，选项“imposed”、“believed”、“requested”、“correlated”，正确答案为“imposed”。对于每一个问题，需要计算问题词与选项词对应词向量之间的余弦距离，并选用距离最近的选项词，作为答案。在评价词向量时，可以直接使用80 个问题的准确率。</p><h4 id="单词类比-sem、syn"><a href="#单词类比-sem、syn" class="headerlink" title="单词类比(sem、syn)"></a>单词类比(sem、syn)</h4><p>英文单词类比数据集由Mikolov 等人于2013 年的word2vec相关论文中提出，该数据集包含了9000 个语义类比问题以及1 万个句法类比问题。语义类比问题包括国家首都、家庭成员称谓、国家货币等五类问题，如，“‘king’对‘queen’如同‘man’对什么？”，答案为“woman”。句法类比问题有比较级、最高级、名词单复数等九类问题，如“‘dance’对‘dancing’如同‘predict’对什么？”，答案为“predicting”。<br>为了回答这类类比问题，Mikolov 等人根据相似关系词对的词向量之差也相似的特点，提出使用词向量的加减法来完成这一任务。例如，对于问题“‘king’对‘queen’如同‘man’对什么？”，该方法直接从词表中寻找与$ \overrightarrow{queen}-\overrightarrow{king}+\overrightarrow{man} $  最相似的词，作为答案。评价时使用回答问题的准确率。<br>单词类比任务的数据集相对前两个任务规模较大，因此在实验中，结果较为稳定，该指标也成为评价词向量的经典指标。</p><h3 id="词向量用作特征"><a href="#词向量用作特征" class="headerlink" title="词向量用作特征"></a>词向量用作特征</h3><p>词向量可以从无标注文本中学习到句法和词法的特征，很多现有工作直接使用词向量作为机器学习系统的特征，并以此提高系统的性能。</p><h4 id="基于平均词向量的文本分类-avg"><a href="#基于平均词向量的文本分类-avg" class="headerlink" title="基于平均词向量的文本分类(avg)"></a>基于平均词向量的文本分类(avg)</h4><p>该任务直接以文本中各词词向量的加权平均值作为文档的表示，以此为特征，利用Logistic 回归完成文本分类任务。其中权重为文档中各词的词频。可以选用IMDB 数据集做文本分类实验。该数据集包含三部分，其中训练集和测试集各2.5 万篇文档，用来做文本分类的训练和测试；无标注部分共5 万篇文档，用于训练词向量。任务的评价指标为文本分类的准确率。</p><h4 id="命名实体识别-ner"><a href="#命名实体识别-ner" class="headerlink" title="命名实体识别(ner)"></a>命名实体识别(ner)</h4><p>命名实体识别（Named entity recognition，NER）在机器学习框架下，通常作为一个序列标注问题处理。在这一评价指标中，将词向量作为现有命名实体识别系统的额外特征，该系统的性能接近现有系统的最好性能。任务的评价指标为命名实体识别的F1值，测试集可以是CoNLL03 多任务数据集的测试集。</p><h3 id="词向量用作神经网络初始值"><a href="#词向量用作神经网络初始值" class="headerlink" title="词向量用作神经网络初始值"></a>词向量用作神经网络初始值</h3><p>在上一类词向量的用法（将词向量作为特征）中，词向量是模型的固定输入值，在模型的训练过程中，输入值不会改变，只有模型中的参数会改变。然而，将神经网络的初始值赋值为词向量之后，神经网络在训练过程中会改变设置的初始值。因此这两类词向量的用法表面上看非常相似，实质上却是不同的。</p><h4 id="基于卷积神经网络的文本分类-cnn"><a href="#基于卷积神经网络的文本分类-cnn" class="headerlink" title="基于卷积神经网络的文本分类(cnn)"></a>基于卷积神经网络的文本分类(cnn)</h4><p>卷积神经网络（Convolutional neural networks，CNN）是表示文本的有效模型。2014 年，Lebret等人以及Kim等人同时提出用于文本分类任务的卷积神经网络。<br>选取斯坦福情感树库（Stanford Sentiment Treebank）数据集作为文本分类的训练集、验证集和测试集。由于该数据集规模较小，文本分类的效果受网络初始值的影响较大，导致了评价指标的不稳定。为了更客观地评价卷积网络中，不同词向量对文本分类性能的影响，对每一份词向量重复做多次实验。在每次实验中，输入层词表示均初始化为这份词向量，网络结构中的其它参数则初始化为不同的随机值。对于每一次实验，在训练集上训练卷积神经网络，取验证集上准确率最高的点，并报告其在测试集上的准确率。最后将5 组实验的测试集准确率的平均值作为最终的评价指标。</p><h4 id="词性标注-pos"><a href="#词性标注-pos" class="headerlink" title="词性标注(pos)"></a>词性标注(pos)</h4><p>词性标注（part-of-speech tagging）是一个经典的序列标注问题。在这个任务中，使用Collobert 等人提出的网络，对句子中的每个词做序列标注。该任务选用华尔街日报数据集。评价指标为模型在验证集上达到最佳效果时，测试集上的准确率。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了词向量的多种表示方法，主要为word2vec基础上的多种模型，侧重于中文词向量模型。&lt;br&gt;&lt;img src=&quot;http://p5vuwy2ht.bkt.clouddn.com/%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%91%E5%B1%95%E5%9B%BE.jpg&quot; alt=&quot;此处输入图片的描述&quot;&gt;&lt;br&gt;2018年7月23日更新，在NLP课上宗成庆老师在讲词向量时提到，除了基于文本的词汇语义表示模型，还有基于图像、语音、多模态信息学习词汇语义表示等，这一些有空再补充。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>GWE:Learning Chinese Word Representations From Glyphs Of Characters读书笔记</title>
    <link href="http://yoursite.com/2018/07/15/GWE%EF%BC%9ALearning-Chinese-Word-Representations-From-Glyphs-Of-Characters%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/15/GWE：Learning-Chinese-Word-Representations-From-Glyphs-Of-Characters读书笔记/</id>
    <published>2018-07-15T06:27:07.000Z</published>
    <updated>2018-07-15T08:04:52.028Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文GWE:Learning Chinese Word Representations From <strong>Glyphs</strong> Of Characters中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Learning Chinese Word Representations From Glyphs Of Characters》是国立台湾大学2017年在EMNLP发表的，在词向量中引入了convolutional auto-encoder（convAE），提取字的信息，提升了词向量的质量。<br>论文中第二部分关于词向量在汉字领域的相关工作研究做的比较充分，在写论文的时候可以适当参考引用。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型主要有两部分，分别是特征提取器和词向量训练模型。</p><h3 id="Character-Bitmap-Feature-Extraction"><a href="#Character-Bitmap-Feature-Extraction" class="headerlink" title="Character Bitmap Feature Extraction"></a>Character Bitmap Feature Extraction</h3><p>本文使用的是Masci等在2011年提出的convAE，结构图如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE1.png" alt="此处输入图片的描述"><br>得到字的glyph特征。</p><h3 id="词向量训练"><a href="#词向量训练" class="headerlink" title="词向量训练"></a>词向量训练</h3><p>针对词向量训练，作者提出了四种不同的模型。</p><h4 id="Enhanced-by-ContextWord-Glyphs"><a href="#Enhanced-by-ContextWord-Glyphs" class="headerlink" title="Enhanced by ContextWord Glyphs"></a>Enhanced by ContextWord Glyphs</h4><p><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE2.png" alt="此处输入图片的描述"><br>这种模型是在CWE的基础上改进而来，词$w_i$的词向量表示为：</p><script type="math/tex; mode=display">\vec{w}_{i}^{ctxG}=\vec{w}_i+\frac{1}{\left| C\left( i \right) \right|}\sum_{c_j\in C\left( i \right)}{\left( \vec{c}_j+\vec{g}_j \right)}</script><p>其中$\vec{g}_j$是由特征提取器提取出的特征。</p><h4 id="Enhanced-by-TargetWord-Glyphs"><a href="#Enhanced-by-TargetWord-Glyphs" class="headerlink" title="Enhanced by TargetWord Glyphs"></a>Enhanced by TargetWord Glyphs</h4><p><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE3.png" alt="此处输入图片的描述"><br>这种模型将当前词中字的特征与上下文结合，一起预测当前词。</p><h4 id="RNN-Skipgram"><a href="#RNN-Skipgram" class="headerlink" title="RNN-Skipgram"></a>RNN-Skipgram</h4><p>以下两种模型直接从特征中学习词向量，没有使用上下文信息。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE4.png" alt="此处输入图片的描述"><br>在Skip-gram的基础上，以特征作为输入，两层GRU，两层全连接ELU，预测当前词的上下文。</p><h4 id="RNN-GloVe"><a href="#RNN-GloVe" class="headerlink" title="RNN-GloVe"></a>RNN-GloVe</h4><p><img src="http://p5vuwy2ht.bkt.clouddn.com/GWE5.png" alt="此处输入图片的描述"><br>类似于GloVe模型，将原模型中词向量部分改写为当前词的特征。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>实验从Word Similarity、Word Analogy、Case Study三方面对比GWE与其他模型的优劣。其中在有的语料上的表现并不如之前的模型。作者将这种情况归结为“If character in iformation does not play a role in learning word representations, character glyphs may not be useful.”说明不要管模型复杂与否，适合应用场景的才是最好的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文GWE:Learning Chinese Word Representations From &lt;strong&gt;Glyphs&lt;/strong&gt; Of Characters中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>JWE:Joint Embeddings of Chinese Words,Characters,and Fine-grained Subcharacter Components</title>
    <link href="http://yoursite.com/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/07/14/JWE-Joint-Embeddings-of-Chinese-Words-Characters-and-Fine-grained-Subcharacter-Components读书笔记/</id>
    <published>2018-07-14T09:27:28.000Z</published>
    <updated>2018-07-15T06:30:13.801Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文JWE：Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components》是香港科技大学2017年在EMNLP发表的，同样是在词向量生成部分进行了改进，引入了人工总结的<strong>“字件信息”</strong>，提升了词向量的质量。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/JWE.png" alt="此处输入图片的描述"><br>根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起联合学习词向量。其中$w<em>i$是目标词，$w</em>{i-1}$是目标词左边的词，$w<em>{i+1}$是目标词右边的词；$c</em>{i-1}$是目标词左边的词中的单字，$c<em>{i+1}$是目标词右边的词中的单字；$s</em>{i-1}$是目标词左边的词拆分的字件，$s_{i+1}$是目标词右边的词拆分的字件,$s_i$是目标词的字件；<br>模型的目标是最大化以下对数似然：</p><script type="math/tex; mode=display">L\left( w_i \right)=\sum_{k=1}^3{\log P\left( w_i|h_{i_k} \right)}\tag1</script><p>其中,$h<em>{i_1}$,$h</em>{i<em>2}$,$h</em>{i_3}$分别为上下文单词、单字、组件的组合。</p><script type="math/tex; mode=display">P\left( w_i|h_{i_k} \right) =\frac{\exp \left( h_{i_k}^{T}\hat{v}_{w_i} \right)}{\sum_{j=1}^N{\exp \left( h_{i_k}^{T}\hat{v}_{w_j} \right)}}\tag2</script><script type="math/tex; mode=display">h_{i_1}=\frac{1}{2T}\sum_{-T\leqslant j\leqslant T,j\ne 0}{v_{w_{i+j}}}\tag3</script><p>在以上3个公式中，$v<em>{w_i}$,$v</em>{c<em>i}$,$v</em>{s<em>i}$分别为上下文、单字、字件的<strong>“输入向量”</strong>，$\hat{v}</em>{w<em>j}$是<strong>“输出向量”</strong>；$h</em>{i<em>1}$为上下文“输入向量”的均值，同理，$h</em>{i<em>2}$为上下文单字“输入向量”的均值，$h</em>{i_3}$为上下文字件“输入向量”的均值。<br>给定语料$D$,模型的目标是最大化：</p><script type="math/tex; mode=display">L(D)=\sum_{w_i \in D}{L(w_i)}</script><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>文章在word similarity evaluation和word analogy tasks两个任务上对比JWE与之前词向量模型。<br>发现在word analogy tasks任务上取得了显著提高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文JWE：Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>Character and Word Embedding读书报告</title>
    <link href="http://yoursite.com/2018/07/14/Character-and-Word-Embedding%E8%AF%BB%E4%B9%A6%E6%8A%A5%E5%91%8A/"/>
    <id>http://yoursite.com/2018/07/14/Character-and-Word-Embedding读书报告/</id>
    <published>2018-07-14T06:45:41.000Z</published>
    <updated>2018-07-15T06:30:06.375Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文CWE：Joint Learning of Character and Word Embeddings中的核心思想。<br><a id="more"></a></p><h2 id="导语"><a href="#导语" class="headerlink" title="导语"></a>导语</h2><p>论文《Joint Learning of Character and Word Embeddings》是刘知远老师团队2015年在顶会Ijcai上发表的，在词向量生成部分进行了改进，引入了单个汉字的信息（论文主要针对的是中文），提升了词向量生成的质量。因为模型名称叫做“character-enhanced word embeddding model”，故模型简称为CWE。<br>从论文的题目可以看出，这篇paper在进行词向量训练的时候，把组成词语的汉字单独抽取出来，和词语一起进行训练。这样就使那些共享汉字的词语之间产生了联系，因为paper的假设是<strong>“semantically compositional”</strong>的词语中的汉字对词语的意思具有一定的表征作用，比方说词语“智能”。但是在汉语中并不是所有的词语都是semantically compositional，比方说一些<strong>音译词</strong>“巧克力”，“沙发”，再比方说一些实体的名称，比方说一些人名、地名和国家名。在这些词语中，单个汉字的意思可能和本来这个词语要表达的意思是完全没有关系的。在本篇paper中，作者做了大量的工作去把这些没有semantically compositional性质的词语全部人工的挑选出来，对于这些词语不去进行单个字的拆分处理。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="http://p5vuwy2ht.bkt.clouddn.com/CWE1.png" alt="此处输入图片的描述"><br>这篇文章提出的模型是在word2vec的CBOW模型基础上改进而来，模型的优化函数：</p><script type="math/tex; mode=display">\frac{1}{M}\sum_{i=k}^{N-k}logPr(x_i|x_{i-k},....,x_{i+k})\tag{1}</script><p>其中，</p><script type="math/tex; mode=display">Pr(x_i|x_{i-k},....,x_{i+k})=\frac{exp(x_o\cdot x_i)}{\sum_{x_j'\in{Dictionary}}exp(x_o\cdot x_j')}\tag{2}</script><p>在CBOW模型中，context的表示是$w_i$前后窗口内的词向量的相加求均值。</p><script type="math/tex; mode=display">x_o=\frac{1}{2k}\sum_{j=i-k,....i+k}x_j\tag{3}</script><p>而在CWE模型中，对于context中的词语的表征，一方面来自于词向量，还有一部分在自于这些词语中的字的向量，具体的计算方式如下：</p><script type="math/tex; mode=display">x_j=w_j\oplus\frac{1}{N_j}\sum_{k=1}^{N_j}c_k\tag{4}</script><p>其中，其中，$N_j$是单词$w_j$中的汉字个数，$c_k$是字向量。$\oplus$对应的操作有拼接和相加两种方式，paper里说拼接方式虽然增加了模型的复杂度，但是对于效果的提升并不明显，因此后面的模型中直接就采用了相加的方式，公式如下所示： </p><script type="math/tex; mode=display">x_j=\frac{1}{2}(w_j+\frac{1}{N_j}\sum_{k=1}^{N_j}c_k)\tag{5}</script><p>注意上述公式中的$\frac{1}{2}$非常重要，它保证了具有semantically compositional的词语和不具有semantically compositional词语在计算距离时的一致性。同时paper指出，为了简化起见只对context的生成考虑字向量信息，target部分不予考虑。其中对于$\sum_{k=1}^{N_j}c_k$计算部分只是把一个词语中的汉字向量进行<strong>等权</strong>相加，如果利用<strong>attention机制</strong>，可能效果更好。</p><h2 id="单字不同语义的解决办法"><a href="#单字不同语义的解决办法" class="headerlink" title="单字不同语义的解决办法"></a>单字不同语义的解决办法</h2><p>同一个汉字，在不同的词语中可能具有完全不同的语义，如果使用一个向量来表征一个字，那么很可能会无法标识出这些差异性，故使用多个向量来表征同一个汉字，有下面几种方式：</p><h3 id="Position-based-Character-Embedding"><a href="#Position-based-Character-Embedding" class="headerlink" title="Position-based Character Embedding"></a>Position-based Character Embedding</h3><p>从名字可以看出，在该模型中同一个汉字根据其在词语中出现的位置不同，对应不同位置的向量表示形式。分析可知，汉字在词语中出现的位置有：Begin,Middle,End这三种情况，故每一个汉字都有三种向量表示形式，在进行$x<em>j=\frac{1}{2}(w_j+\frac{1}{N_j}\sum</em>{k=1}^{N_j}c_k)$ 生成向量操作的时候，对于$c_k$按照其在词语中出现的位置进行合理的筛选。这种方式比较简单，但是缺点也是比较明显的，它假设的前提是同一个汉字只要位于不同单词的同一个位置就具有相同的语义，这显然在一些情况下是不成立的。</p><h3 id="Cluster-based-Character-Embedding"><a href="#Cluster-based-Character-Embedding" class="headerlink" title="Cluster_based Character Embedding"></a>Cluster_based Character Embedding</h3><p>这种方法看的不是很懂，简单来讲就是：对于每一个汉字提前分配x个字向量，称之为模式向量。利用该词对应的词语的context信息，从一个汉字的所有模式向量中选择一个和context语义计算上最相似的作为该汉字对应的向量。</p><h3 id="Nonparametric-Cluster-based-Character-Embeddings"><a href="#Nonparametric-Cluster-based-Character-Embeddings" class="headerlink" title="Nonparametric Cluster-based Character Embeddings"></a>Nonparametric Cluster-based Character Embeddings</h3><p>该模型和Cluster_based Character Embedding模型是很相似的，唯一不同的是，Cluster_based Character Embedding中的每一个汉字对应的模型向量的数量是一个预先设定的固定值，也就是作为模型的超参数。而在Nonparametric Cluster-based Character Embeddings模型中，该值是一个模型自动学习的值。</p><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><p>1 本文提出的汉字和词语结合的方式就是简单的<strong>向量相加</strong>操作，也许应用复杂一点的操作（比如矩阵变换）等方式可以把二者更加合理地结合在一起；<br>2 在相加的时候，需要给以每一个汉字不同权重，这也和我之前说的一致，可以利用attention机制，只不过在2015年的时候还没有attention的概念。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文CWE：Joint Learning of Character and Word Embeddings中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>word2vec公式推导</title>
    <link href="http://yoursite.com/2018/07/10/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://yoursite.com/2018/07/10/word2vec公式推导/</id>
    <published>2018-07-10T08:47:30.000Z</published>
    <updated>2018-07-15T06:29:54.606Z</updated>
    
    <content type="html"><![CDATA[<p>本文接着word2vec的那篇概述再推导一下word2vec中的公式，也是《word2vec Parameter Learning Explained》论文学习笔记，有一些细节的推导我写在了论文处。<br>这篇论文详细地推导和解释了word2vec模型的参数更新公式，包括：<strong>CBOW</strong>（continuous bag-of-word）模型和<strong>SG</strong>（skip-gram）模型，以及两种参数优化技术：<strong>hierarchical softmax</strong> 和 <strong>negative sampling</strong>.<br><a id="more"></a></p><h2 id="Continuous-Bag-of-Word-Model"><a href="#Continuous-Bag-of-Word-Model" class="headerlink" title="Continuous Bag-of-Word Model"></a>Continuous Bag-of-Word Model</h2><h3 id="One-word-context"><a href="#One-word-context" class="headerlink" title="One-word context"></a>One-word context</h3><p>我们从CBOW模型的最简单版本开始介绍——One-word context。即我们假定context（预测目标单词的上下文信息）只有一个单词，也就是说One-word context 模型是在只要一个上下文单词（one context word）的情况下来预测一个目标单词（one target word）的。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC1.png" alt="此处输入图片的描述"><br>如图1描述的就是One-word context定义之下的神经网络模型。这里我们假设文本词汇量的大小为V,隐藏层的大小为N，相邻层的神经元是全连接的。输入层是一个用one-hot方式编码的单词向量$x=(x_1,…,x_V)$，其中只有一个$x_i$为1，其余均为0。<br>从输入层到隐藏层的权重值可以用一个$V×N$维的矩阵$W$来表示，即 </p><script type="math/tex; mode=display">W= \begin{pmatrix} \omega_{11}&\omega_{12}&...&\omega_{1N}\\ \omega_{21}&\omega_{22}&...&\omega_{2N}\\ ...&...&...&...\\ \omega_{V1}&\omega_{V2}&...&\omega_{VN} \end{pmatrix}</script><p> 其中$W$矩阵的每一行代表的是一个与输入层相关的单词的N维向量表示形式$v<em>ω$。那么假设我们给定了一个输入单词（a context）,其单词向量的第k个元素$x_k=1$，其余均为0，则有<br> $\mathbf h= \mathbf W^Tx=\mathbf W</em>{(k,\bullet)}^T x<em>k=\mathbf v</em>{\omega<em>I}^T\tag{1}$<br> 从（1）式我们可以看出，$h$向量完全是从$W$矩阵第k行复制过来的（同$\mathbf v</em>{\omega<em>I}$均为N维向量）。$\mathbf v</em>{\omega<em>I}$即为输入单词$ω_I$的一种向量表示（其实就是<strong>输入向量</strong>，我们后面会提到）。<br> 分析完输入层到隐藏层之后，我们再看隐藏层到输出层，同样连接权重用一个新的$N × V$矩阵$\mathbf W’={\omega</em>{ij}’ }$来表示如下：</p><script type="math/tex; mode=display"> \mathbf W'= \begin{pmatrix} \omega_{11}'&\omega_{12}'&...&\omega_{1V}'\\ \omega_{21}'&\omega_{22}'&...&\omega_{2V}'\\ ...&...&...&...\\ \omega_{N1}'&\omega_{N2}'&...&\omega_{NV}' \end{pmatrix}</script><p> 通过这些权重，我们可以为词表中的每一个单词都计算出一个得分$μ_j$</p><script type="math/tex; mode=display">\mu_j=\mathbf {v_{\omega_j}'}^T\mathbf h\tag{2}</script><p> 其中，${v_{\omega_j}’}$即为矩阵$W′$的第j列向量（也是N维向量，其实就是单词w的输出向量，我们后面会提到）。<br> 经过以上讨论之后，我们可以使用一种对数-线性分类模型softmax函数来计算单词的后验分布（是多项式分布）</p><script type="math/tex; mode=display">p(\omega_j|\omega_I)=y_j=\frac{\exp(\mu_j)}{\sum_{j'=1}^V\exp(\mu_{j'})}\tag{3}</script><p> 其中， $y_j$表示输出层第j个神经单元的输出值。将（1）式和（2）式代入（3）式我们可以得到：</p><script type="math/tex; mode=display">p(\omega_j|\omega_I)=\frac{\exp({\mathbf v_{\omega_j}'}^T \mathbf v_{\omega_I})}{\sum_{j'=1}^V\exp({\mathbf v_{\omega_j}'}^T \mathbf v_{\omega_I})}\tag{4}</script><p> 注意：正如前文所述，$v<em>ω$和$v′</em>ω$是单词的两种向量表示形式。其中$v<em>ω$实际上是权重矩阵$W$（input-&gt;hidden）的某一行向量，$v′</em>ω$则是权重矩阵$W′$（hidden-&gt;output）的某一列向量。我们将$v<em>ω$和$v′</em>ω$分别称为“输入向量（input vector）”和“输出向量（output vector）”（二者均为N维向量）。</p><h4 id="Update-equation-for-hidden→output-weights"><a href="#Update-equation-for-hidden→output-weights" class="headerlink" title="Update equation for hidden→output weights"></a>Update equation for hidden→output weights</h4><p> 在我们推导hidden→output权重的更新公式的过程中，需要用到哦神经网络的反向传播算法，对这部分内容不熟悉的读者可以参考附录A的内容。<br>由以上描述可知，该模型训练的目标就是求公式（4）的最大值。公式（4）代表的就是给定上下文信息（这里为一个单词$ω_I$）以及其权重矩阵的情况下，预测其实际输出单词（即上下文信息的中心词$ω_O$）的条件概率。</p><script type="math/tex; mode=display">\begin{align}& \max p(\omega_O|\omega_I)=\max y_{j^*}\tag{5}\\&=\max \log y_{j^*}\tag{6}\\&=\mu_{j^*} - \log \sum_{j'=1}^V \exp(\mu_{j'}):=-E\tag{7}\end{align}</script><p>其中，$E=-\log p(\omega_O|\omega_I)$ 为该模型的损失函数（我们需要找出它的最小值），$j^<em>$则为实际输出单词的索引下标。我们注意到该损失函数可以理解为一种特殊情形下的<em>*交叉熵</em></em>计算。<br>现在我们开始推导从隐藏层到输出层的权重矩阵在模型训练过程中的参数更新公式。首先我们对损失函数$E=-\log p(\omega_O|\omega_I)$求关于得分$μ_j$的偏导数，得结果为：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial\mu_j}=y_j-t_j:=e_j\tag{8}</script><p> 其中，$t<em>j=1(j=j^∗)$ ,即当且仅当输出层的第j个神经单元为真实的输出单词时 $t_j$的取值为1。接下来我们根据链式法则求出损失函数$E$关于矩阵$W′$元素 $\omega</em>{ij}’$的偏导数为：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial \omega_{ij}'}=\frac{\partial E}{\partial \mu_j}\cdot \frac{\partial \mu_j}{\partial \omega_{ij}'}=e_j\cdot h_i \tag{9}</script><p> 因此，采用随机梯度下降算法（SGD）,我们最终得到了隐藏层到输出层（hidden→output）权重的更新公式如下：</p><script type="math/tex; mode=display">\begin{align}{\omega_{ij}'}^{(new)}={\omega_{ij}'}^{(old)}-\eta \cdot e_j \cdot h_i\tag{10}\end{align}</script><p>or</p><script type="math/tex; mode=display">\begin{align}{\mathbf v_{\omega_j}'}^{(new)}= {\mathbf v_{\omega_j}'}^{(old)} - \eta \cdot e_j \cdot \mathbf h \space \space for\space j=1,2,...V.\tag{11}\end{align}</script><p>其中， $η&gt;0$为参数更新的学习速率；$e<em>j=y_j−t_j$；$h_i$ 为隐藏层的第i个神经单元；$\mathbf v</em>{\omega_j}’$为$ω_j$的输出向量。<br>由公式（11）我们可以看出：在更新权重参数的过程中，我们需要检查词汇表中的每一个单词，计算出它的输出概率$y_j$，并与期望输出$t_j$（取值只能为0或者1）进行比较。比较过程如下：</p><ul><li>如果$y<em>j&gt;t_j$(“overestimating”)，那么就从向量$\mathbf v</em>{\omega<em>j}’$中减去隐藏向量$h$的一部分（例如$\mathbf v</em>{\omega<em>I}$），这样向量$\mathbf v</em>{\omega<em>j}’$就会与向量$\mathbf v</em>{\omega_I}$相差更远。</li><li>如果$y<em>j&lt;t_j$“underestimating”，这种情况只有在$t_j=1$时，才会发生，此时$ω_j=ω_O$），则将隐藏向量$h$的一部分加入$\mathbf v</em>{\omega<em>O}’$，使得$\mathbf v</em>{\omega<em>O}’$与$\mathbf v</em>{\omega_I}$更接近。</li><li>如果$y<em>j$与$t_j$非常接近，则此时$e_j=y_j−t_j$由于（公式（8））非常接近于0，故更新参数基本上没什么变化。<br>这里需要再次提醒的是：$v</em>ω$和$v′_ω$是单词$ω$的两种不同的向量表示形式。<h4 id="Update-equation-for-input→hidden-weights"><a href="#Update-equation-for-input→hidden-weights" class="headerlink" title="Update equation for input→hidden weights"></a>Update equation for input→hidden weights</h4>在介绍完hidden→output的权重矩阵更新公式之后，我们接着介绍input→hidden的权重矩阵$W$的更新过程。我们继续对损失函数$E$求关于隐藏层$h_i$的偏导数，得： <script type="math/tex; mode=display">\frac{\partial E}{\partial h_i}=\sum_{j=1}^V \frac{\partial E}{\partial \mu_j} \cdot \frac{\partial \mu_j}{\partial h_i}=\sum_{j=1}^V e_j \cdot \omega_{ij}':=EH_i \tag{12}</script>其中$h<em>i$为隐藏层第i个神经单元的输出；$μ_j$在公式（2）中已经定义，表示输出层第j个神经单元的输出；$e_j=y_j−t_j$为输出层第j个单词的预测误差。因此$EH$应该是一个N维向量，它的每一个元素代表的是词汇表中的每个单词的预测误差$e_j$与$ω′</em>{ij}$在j=1到V上的乘积之和。<br>接下来，我们需要求出损失函数$E$关于权重矩阵$W$的偏导数。首先，分解公式（1），我们知道隐藏层激活单元的输出$h_i$是输入层$x$与权重的线性组合，即<script type="math/tex; mode=display">h_i=\sum_{k=1}^V x_k \cdot \omega_{ki} \tag{13}</script>因此对于权重矩阵$W$的每一个元素，我们求关于$E$的偏导数，得到： $$\frac{\partial E}{\partial \omega_{ki}}=\frac{\partial E}{\partial h_i} \cdot \frac{\partial h_i}{\partial \omega_{ki}}=EH_i \cdot x_k \tag{14}$$因此我们利用张量乘积的方式，便可得到：<script type="math/tex; mode=display">\frac{\partial E}{\partial W}=\mathbf x \otimes EH = \mathbf xEH^T \tag{15}</script>我们再次得到了一个$N×V$的矩阵。由于$x$向量只有一个非0元素，因此$\frac{\partial E}{\partial W}$只有一行是N维非0向量$EH^T$，因此矩阵$W$的更新公式为：  $${\mathbf v_{\omega_I}}^{(new)}={\mathbf v_{\omega_I}}^{(old)}-\eta \cdot EH^T \tag{16}$$其中$\mathbf v<em>{\omega_I}$是矩阵$W$的其中一行，是唯一的上下文单词（context word）的“输入向量”,也是矩阵$W$唯一的导数非0的行向量。 除了$\mathbf v</em>{\omega_I}$以外，矩阵$W$的其他行向量在参数更新迭代过程中都会保持不变（因为其导数为0）。<br>与矩阵$W′$的更新过程相似，对于公式（16），我们分析如下：</li><li>如果过高地估计了某个单词$ω_j$作为最终输出单词的概率（即：$y_j&gt;t_j$），则上下文单词$ω_I$（context word ）的输入向量与单词$ω_j$的输出向量在更新的过程中会相差越来越大。</li><li>如果相反，某个单词$ω_j$作为最终输出单词的概率被低估（即：$y_j&lt;t_j$），则单词$ω_I$的输入向量与单词$ω_j$的输出向量在更新过程中会越来越接近。</li><li>如果对于单词$ω_I$的概率预测是准确的，则对于单词的输入向量在更新过程中几乎保持不变。</li></ul><p>因此，上下文单词$ω_I$（context word ）的输入向量的更新取决于词汇表中所有单词的预测误差。预测误差越大，则该单词对于上下文单词的输入向量的更新过程影响越大。</p><p>在介绍完One-word context的CBOW模型之后，我们接着介绍multi-word context下的CBOW模型。</p><h3 id="Multi-word-context"><a href="#Multi-word-context" class="headerlink" title="Multi-word context"></a>Multi-word context</h3><p>根据字面意思我们就可以看出，基于multi-word context的CBOW模型就是利用多个上下文单词来推测中心单词target word的一种模型。其结构如图2所示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC2.png" alt="此处输入图片的描述"></p><p>其隐藏层的输出值的计算过程为：首先将输入的上下文单词（context words）的向量叠加起来并取其平均值，接着与input→hidden的权重矩阵相乘，作为最终的结果，公式如下：</p><script type="math/tex; mode=display">\begin{align}& \mathbf h = \frac{1}{C} \mathbf W^T(\mathbf x_1 + \mathbf x_2 + \cdots +\mathbf x_C)\tag{17}\\& = \frac{1}{C}(\mathbf v_{\omega_1}+\mathbf v_{\omega_2} + \cdots+\mathbf v_{\omega_C})^T\tag{18}\end{align}</script><p>其中$C$为上下文单词的个数，$ω<em>1,…,ω_C$为上下文单词，$v</em>ω$为单词$ω$的输入向量。损失函数为：</p><script type="math/tex; mode=display">\begin{align}& E = - \log p(\omega_O|\omega_{I,1},...,\omega_{I,C})\tag{19}\\& =- \mu_{j^*} + \log \sum_{j'=1}^{V} exp(\mu_{j'})\tag{20}\\& = - {\mathbf v_{\omega_O}'}^T \cdot \mathbf h + \log \sum_{j'=1}^{V} \exp({\mathbf v_{\omega_j}'}^T \cdot \mathbf h)\tag{21}\end{align}</script><p>同样，由hidden→output的权重更新公式与one-word-context模型下的一模一样，即类似于公式（11），我们直接写在下面：</p><script type="math/tex; mode=display">{\mathbf v_{\omega_j}'}^{(new)}={\mathbf v_{\omega_j}'}^{(old)}-\eta \cdot e_j \cdot \mathbf h \space  \space  \space for \space \space  j=1,2,...,V\tag{22}</script><p> 由input→hidden 的权重矩阵更新公式与公式（16）类似，只不过现在我们需要对每一个上下文单词$ω_{I,c}$都执行如下更新公式：<br> $${\mathbf v_{\omega_{I,c}}}^{(new)}={\mathbf v_{\omega_{I,c}}}^{(old)} - \frac{1}{C}\cdot \eta \cdot EH^T \space \space for \space \space c=1,2,...,C.\tag{23}$$<br> 其中 ${\mathbf v_{\omega_{I,c}}}$为上下文context中第c 个单词的输入向量；$η$为正学习速率；$EH=\frac{\partial E}{\partial h_i}$由公式（12）给出。</p><h2 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h2><p>与CBOW模型正好相反，Skip-Gram模型是根据中心单词（target word）来预测其上下文信息（context words）。如图3所示，为Skip-Gram模型的结构示意图。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC3.png" alt="此处输入图片的描述"><br>我们仍然使用$\mathbf v_{\omega_I}$来表示输入层上唯一的那个单词的<strong>输入向量</strong>，因此，我们对于隐藏层的输出值$h$的计算公式与第一节公式（1）相同，表示如下：</p><script type="math/tex; mode=display">\mathbf h = {\mathbf W}_{(k,\bullet)}^T := \mathbf v_{\omega_I}\tag {24}</script><p> 公式（24）显示：$h$向量其实就是input-&gt;hidden权重矩阵$W$的某一行结合输入单词$ω_I$的向量拷贝。在输出层，与CBOW模型的输出为单个多项式分布不同的是，SG模型在输出层输出了C个多项式分布。每个输出都使用相同的hidden-&gt;output矩阵计算：</p><script type="math/tex; mode=display">p\left( \omega _{c,j}=\omega _{O,c}|\omega _I \right) =y_{c,j}=\frac{\exp \left( \mu _{c,j} \right)}{\sum_{j'-1}^V{\exp \left( \mu '_j \right)}} \tag{25}</script><p>   其中，$\omega<em>{c,j}$表示输出层的第c个panel的第j个单词（何为panel?就是输出层的表示每个上下文单词的神经元的组合，图中一种有C个context words，所以总共有C个panel）；$\omega</em>{O,c}$实际上表示的是输出上下文单词（output context words）的第c个单词；$ω<em>I$是唯一的输入单词；$y</em>{c,j}$为输出层的第c个panel上的第j个神经单元的概率输出值；$\mu_{c,j}$表示的是输出层第c个panel的第j个神经元的输入值；<strong>由于输出层的所有panels共享同一权重矩阵</strong>$W′$,因此：</p><script type="math/tex; mode=display">\mu_{c,j}=\mu_j={\mathbf v_{\omega_j}'}^T\cdot \mathbf h, \space for \space c=1,2,...,C\tag{26}</script><p> 其中，$\mathbf v_{\omega_j}’$为词汇表第j个单词$ω_j$的输出向量；同样，它也是取自于hidden→output权重矩阵$W′$的一列。<br> SG模型参数更新公式的推导过程与one-word-context 模型的推导过程大体上一样。这里我们将损失函数变为：</p><script type="math/tex; mode=display">\begin{align}&E=-\log p(\omega_{O,1},\omega_{O,2},...,\omega_{O,C}|\omega_I)\tag{27}\\&=-\log \prod_{c=1}^C \frac{\exp(\mu_{c,j_c^*})}{\sum_{j'=1}^V \exp(\mu_{j'})}\tag{28}\\&=-\sum_{c=1}^C \mu_{j_c^* }+C\cdot\log\sum_{j'=1}^V\exp(\mu_{j'})\tag{29}\end{align}</script><p>其中，$j_c^*$为第c个输出层输出的上下文单词在词汇表中的真实索引。</p><p>在得到损失函数$E$之后，我们对输出层的每一个panel上的所有激活单元的输入值$\mu<em>{c,j}$,均求其关于$E$的偏导数，得：<br> $$\frac{\partial E}{\partial \mu_{c,j}}=y_{c,j}-t_{c,j}:=e_{c,j}\tag {30}$$<br> 其中$e</em>{c,j}$为输出层神经元的预测误差，与公式（8）类似。为了简化符号，我们定义一个$V$维的向量$EI={\{EI_1,...,EI_V\}}$作为所有上下文单词的预测误差之和，$EI_j$用公式定义如下：</p><script type="math/tex; mode=display">EI_j=\sum_{c=1}^C e_{c,j}\tag{31}</script><p> 接下来，我们计算hidden-&gt;output权重矩阵$W′$关于$E$的偏导数为：<br> $$\frac{\partial E}{\partial \omega_{ij}'}=\sum_{c=1}^C\frac{\partial E}{\partial \mu_{c,j}}\cdot\frac{\partial \mu_{c,j}}{\partial \omega_{ij}'}=EI_j\cdot h_i\tag{32}$$<br> 这样，我们就得到了hidden→output权重矩阵$W′$的参数更新公式为：<br> $${\omega_{ij}^{'}}^{(new)}={\omega_{ij}^{'}}^{(old)}-\eta\cdot EI_j\cdot h_i\tag{33}$$<br> 或者</p><script type="math/tex; mode=display">{\mathbf v_{\omega_j}'}^{(new)}={\mathbf v_{\omega_j}'}^{(old)}-\eta \cdot EI_j \cdot \mathbf h \space\space\space for \space j=1,2,...,V.\tag{34}</script><p> 上述参数更新公式的直观概念理解与上文公式（11）无二，除了一点就是：输出层的预测误差的计算是基于多个上下文单词context words,而不是单个目标单词 target word;需注意的是对于每一个训练样本，我们都要利用该参数更新公式来更新hidden→output权重矩阵$W′$的每个元素。</p><p>同样，对于input→hidden权重矩阵$W$的参数更新公式的推导过程，除了考虑要将预测误差$e_j$替换为$EI_j$外，其他也与上文公式（12）到公式（16）类似。这里我们直接给出更新公式：<br> $${\mathbf v_{\omega_I}}^{(new)}={\mathbf v_{\omega_I}}^{(old)}-\eta \cdot EH^T\tag{35}$$<br> 其中，$EH$是一个$N$维向量，组成该向量的每一个元素可以用如下公式表示：</p><script type="math/tex; mode=display">EH_i=\sum_{j=1}^V EI_j\cdot\omega_{ij}'\tag{36}</script><p> 公式（36）的直观理解与公式（16）类似，这里不作描述。</p><h2 id="Optimizing-Computational-Efficiency"><a href="#Optimizing-Computational-Efficiency" class="headerlink" title="Optimizing Computational Efficiency"></a>Optimizing Computational Efficiency</h2><p>总结以上的模型介绍，我们发现所有模型的词汇表中的每个单词都存在两个向量表示形式：输入向量$v<em>ω$与输出向量$v′</em>ω$.对于输入向量的参数学习成本并不高，但对于输出向量的学习成本代价是非常昂贵的。根据更新公式（22）和（23），我们可以发现，为了更新输出向量$v′<em>ω$，对于每一个训练样例，我们必须迭代遍历词汇表中所有的单词$ω_j$，计算出它们的输入值$μ_j$、概率预测值$y_j$（或者SG模型中的$y</em>{c,j}$），预测误差$e<em>j$（或者SG模型的$EI_j$）。最终使用预测误差更新它们的输出向量$v′_j$.<br>显然，对于每一个训练样例都要对所有单词计算上述各值，其成本是昂贵的。特别是对于大型的词汇表，这种计算方式是不切实际的。因此为了解决这个问题，直观的方式是限制必须要更新的训练样例的输出向量的数目。一种有效的实现方式就是：hierarchical softmax（分层softmax），另一种实现通过采样的方式解决，我们在下个章节来讨论。<br>这两种方法都是通过只优化输出向量更新的计算过程来实现的。在我们的公式推导过程中，我们关心的有三个值：（1）$E$，新的目标函数；（2）$\frac{\partial E}{\partial \mathbf v</em>\omega’}$，新的关于输出向量的更新公式；（3）$\frac{\partial E}{\partial \mathbf h}$，为了更新输入向量反向传播的预测误差的加权和。</p><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>Hierarchical softmax 是一种有效的计算 softmax 的方式。该模型使用一棵二叉树来表示词汇表中的所有单词。所有的$V$个单词都在二叉树的叶节点上。非叶子节点一共有$V−1$个。对于每个叶子节点，从根节点root到该叶子节点只有一条路径；这条路径用来评估用该叶子节点代表该叶子节点上单词的概率值。二叉树的结构如图4所示：</p><p> <img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC4.png" alt="此处输入图片的描述"><br> 其中白色的树节点代表的是词汇表中的单词，灰色节点为内部节点。图中高亮显示的是一条从根节点到$ω<em>2$的路径。该条路径的长度为$L(\omega_2)=4$。$n(ω,j)$表示从根节点到单词$ω$的路径上的第j个节点。<br> 在hierarchical softmax模型中，所有的词汇单词没有输出向量表示形式。不同的是，二叉树的每一个内部节点都有一个输出向量${\mathbf v</em>{n(\omega,j)}’}$。因此一个单词作为输出单词的概率计算公式定义如下：</p><script type="math/tex; mode=display">p(\omega = \omega_O)=\prod_{j=1}^{L(\omega)-1}\sigma \bigg(\Big[\Big[n\big(\omega,j+1\big)=ch\big(n\small(\omega,j\small)\big)\Big]\Big]\cdot{\mathbf v_{n(w,j)}'}^T\mathbf h\bigg)\tag{37}</script><p> 其中，$ch(n)$为节点$n$的左孩子节点；$\mathbf v<em>{n(\omega,j)}’$是内部节点$n(\omega,j)$的向量表示（输出向量）；$h$是隐藏层的输出值（在SG模型中，$h=\mathbf v</em>{\omega<em>I}$;而在CBOW模型中，$\mathbf h=\frac{1}{C}\sum</em>{c=1}^C \mathbf v_{\omega_c}$;$[[x]]$是一种特殊的函数定义如下：</p><script type="math/tex; mode=display">[[x]]=\begin{cases}1  & \text{if $x$ is true} \\-1, & \text{otherwise}\end{cases}\tag{38}</script><p>接下来，我们通过一个直观地例子来理解公式（37）。如图4所示，假定我们需要计算单词$ω_2$作为输出单词的概率。我们将这个概率定义为从根节点开始随机游走到叶节点$ω_2$的概率。则在每一个内部节点（包括根节点），我们都需要确定其路径指向左孩子节点还是右孩子节点的概率。我们将经过内部节点的路径指向左孩子的概率定义为：</p><script type="math/tex; mode=display">p(n,left)=\sigma({\mathbf v_n'}^T\cdot\mathbf h)\tag{39}</script><p> 我们可以看出，公式（39）的值取决于内部节点的向量表示$v′_n$和隐藏层的输出值$h$($h$的值取决于输入单词的向量表示)。显然，内部节点的路径指向右孩子的概率则可以表示为：</p><script type="math/tex; mode=display">p(n,right)=1-\sigma({\mathbf v_n'}^T\cdot\mathbf h)=\sigma(-{\mathbf v_n'}^T\cdot \mathbf h)\tag{40}</script><p> 顺着图4中从根节点到单词$ω_2$点的路径，我们可以计算出$ω_2$作为输出单词的概率为：</p><script type="math/tex; mode=display">\begin{align}& p(\omega_2=\omega_O)=p\Big(n(\omega_2,1),left\Big)\cdot p\Big(n(\omega_2,2),left\Big)\cdot p\Big(n(\omega_2,3),right\Big)\tag{41}\\& =\sigma \Big({\mathbf v_{n(\omega_2,1)}'}^T\mathbf h\Big)\cdot\sigma \Big({\mathbf v_{n(\omega_2,2)}'}^T\mathbf h\Big)\cdot\sigma \Big(-{\mathbf v_{n(\omega_2,3)}'}^T\mathbf h\Big)\cdot \tag{42}\end{align}</script><p>不难证明</p><script type="math/tex; mode=display">\sum_{i=1}^{V}p(\omega_i=\omega_O)=1\tag{43}</script><p> 现在我们开始推导内部节点的向量表示形式的参数更新公式。为了简化步骤，我们首先考虑单个上下文单词（one-word context）的模型。<br>为了简化公式，我们定义子公式的简化符号如下：</p><script type="math/tex; mode=display">[[\cdot]]:=[[n(\omega,j+1)=ch(n(\omega,j))]]\tag{44}</script> $$\mathbf v_j':=\mathbf v_{n_{\omega,j}}'\tag{45}$$<p> 则，给定一个训练样例，其误差函数我们可以定义如下：</p><script type="math/tex; mode=display">E=-\log p(\omega = \omega_O|\omega_I)=-\sum_{j=1}^{L(\omega)-1}\log\sigma([[\cdot]]{\mathbf v_j'}^T\mathbf h)\tag{46}</script><p> 对于误差函数$E$，我们取其关于$\mathbf v_j’\mathbf h$的偏导数，得：</p><script type="math/tex; mode=display">E=-\log p(\omega = \omega_O|\omega_I)=-\sum_{j=1}^{L(\omega)-1}\log\sigma([[\cdot]]{\mathbf v_j'}^T\mathbf h)\tag{46}</script><script type="math/tex; mode=display">\begin{align}&\frac{\partial E}{\partial \mathbf v_j'\mathbf h}=\Big(\sigma([[\cdot]]{\mathbf v_j'}^T\mathbf h)-1\Big)[[\cdot]]\tag{47}\\&=\begin{cases}\sigma({\mathbf v_j'}^T\mathbf h)-1 ,&\text{[[.]]=1} \\\sigma({\mathbf v_j'}^T\mathbf h),&\text {[[.]]=-1}\end{cases}\tag{48}\\&=\sigma({\mathbf v_j'}^T\mathbf h)-t_j\tag{49}\end{align}</script><p>其中$t_j=1$（如果$[[⋅]]=1$）或者$t_j=0$（如果$[[⋅]]=−1$）。<br>紧接着我们计算内部节点$n(ω,j)$的向量表示$\mathbf v_j’$关于函数$E$的偏导数，得：</p><script type="math/tex; mode=display">\frac{\partial E}{\partial \mathbf v_j'}=\frac{\partial E}{\partial \mathbf v_j'\mathbf h}\cdot \frac{\partial \mathbf v_j'\mathbf h}{\partial \mathbf v_j'}=\Big(\sigma({\mathbf v_j'}^T\mathbf h)-t_j\Big)\cdot \mathbf h\tag{50}</script><p> 因此，更新公式为：</p><script type="math/tex; mode=display">{\mathbf v_j'}^{(new)}={\mathbf v_j'}^{(old)}-\eta\Big(\sigma({\mathbf v_j'}^T\mathbf h)-t_j\Big)\cdot \mathbf h\space,\space for \space j=1,2,...,L(\omega)-1\tag{51}</script><p> 我们可以将$\sigma({\mathbf v_j’}^T\mathbf h)-t_j$理解为内部节点$n(ω,j)$的预测误差。每一个内部节点的“任务”就是预测其随机游走路径是指向左孩子节点还是指向右孩子节点。$t_j=1$意味着节点$n(ω,j)$的路径指向左孩子节点；$t_j=0$则表示指向右孩子节点。$\sigma({\mathbf v_j’}^T\mathbf h)$是预测结果。对于一个训练实例，如果内部节点的预测值非常接近于真实值，则它的向量表示$\mathbf v_j’$的更新变化很小；否则$\mathbf v_j’$向量指向一个适当的方向是的该实例的预测误差逐渐减小。以上更新公式既能应用于CBOW模型，又能应用于SG模型。当在SG模型中使用该更新公式时，我们需要对C个output context words的每一个单词都重复此更新过程。</p><p>为了使用反向传播该预测误差来学习训练input→hidden的权重，我们对误差函数$E$求关于隐藏层输出值的偏导数，如下：</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial E}{\partial \mathbf h}=\sum_{j=1}^{L(\omega)-1}\frac{\partial E}{\partial \mathbf v_j'\mathbf h}\cdot\frac{\partial \mathbf v_j'\mathbf h}{\partial \mathbf h}\tag{52}\\&=\sum_{j=1}^{L(\omega)-1}\Big(\sigma({\mathbf v_j'}^T\mathbf h)-t_j\Big)\cdot \mathbf v_j'\tag{53}\\&:=EH\tag{54}\end{align}</script><p>接下来我们根据公式（23）便可以获得CBOW模型输入向量的更新公式。对于SG模型，我们需要计算上下文信息中的每个单词的$EH$值,并将$EH$值的和带入公式（35）,就能够得到输入向量的更新公式。<br>从以上更新公式我们可以看出：经过改进的模型Hierarchical softmax的每个训练样例的每个上下文单词的计算复杂度从$O(V)$降为$O(log(V))$级别。但是模型的参数几乎没有什么改变（内部节点对应V-1维向量，而原始模型的单词的输出向量维数为V）。</p><h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>Negative Sampling模型的思想比hierarchical softmax模型更直接了当,即：在每次迭代的过程中，有大量的输出向量需要更新，为了解决这一困难，negative sampling提出了只更新其中一部分输出向量的解决方案。<br>显然，最终需要输出的上下文单词（正样本）在采样的过程中应该保留下来并更新，同时我们需要采集一些单词作为负样本（因此称为“negative sampling”）。在采样的过程中，我们可以任意选择一种概率分布。我们将这种概率分布称为“噪声分布”（the noise distribution），用$P_n(\omega)$来表示。我们可以根据经验选择一种较好的分布。</p><p>在 word2vec中，我们无需使用一种能够产生良好定义的后验多项式分布的负采样形式，本文作者证明了使用下面简单的训练目标函数能够产生可靠的、高质量的 word embeddings:<br> $$E=-\log \sigma({\mathbf v_{\omega_O}'}^T\mathbf h)-\sum_{\omega_j\in W_{neg}} \log \sigma({-\mathbf v_{\omega_j}'}^T\mathbf h)\tag{55}$$<br> 其中$ω<em>O$是输出单词（the positive sample），$\mathbf v</em>{\omega<em>O}’$是输出向量；$h$是隐藏层的输出值：在CBOW模型中$\mathbf h=\frac{1}{C}\sum</em>{c=1}^{C} \mathbf v<em>{\omega_c}$，在SG模型中$\mathbf h=\mathbf v</em>{\omega<em>I}$；$W_{neg}={\{\omega_j|j=1,...,K\}}$是基于分布$P_n(\omega)$采样的一系列单词。<br>为了获得negative sampling模型的词向量更新公式，我们首先计算$E$关于输出单元$ω_j$的输入${\mathbf v</em>{\omega_j}’}^T\mathbf h$的偏导数：</p><script type="math/tex; mode=display">\begin{align}&\frac{\partial E}{\partial{ \mathbf v_{\omega_j}'}^T\mathbf h}=\begin{cases}\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-1 ,&\text{if }\space \omega_j=\omega_O \\\sigma({\mathbf v_{\omega_j}'}^T\mathbf h),&\text {if}\space\omega_j\in W_{neg}\end{cases}\tag{56}\\&\space\space\space\space\space\space\space\space\space\space\space\space\space\space=\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-t_j\tag{57}\end{align}</script><p>其中，当$ω_j$是一个正样本时，$t_j=1$;否则$t_j=0$。接下来我们计算$E$关于单词$ω_j$的输出向量的偏导数：<br> $$\frac{\partial E}{\partial \mathbf v_{\omega_j}'}=\frac{\partial E}{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}\cdot \frac{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}{\partial {\mathbf v_{\omega_j}'}}=\Big(\sigma({\mathbf v_{\omega_j}'}^T \mathbf h)-t_j\Big)\mathbf h \tag{58}$$<br> 因此输出向量的更新公式为：</p><script type="math/tex; mode=display">{\mathbf v_{\omega_j}'}^{(new)}={\mathbf v_{\omega_j}'}^{(old)}-\eta\Big(\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-t_j\Big)\mathbf h\tag{59}</script><p> negative sampling的关键就是公式（59）的更新过程只应用于词汇表的子集${\omega<em>j|\omega_j\in {\omega_O}\bigcup W</em>{neg}}$,而并非应用于整个词汇表。<br>以上更新公式（59）的直观理解与公式（11）类似。公式（59）对两种应用模型CBOW和SG都适用。对于SG模型，我们每次更新一个上下文单词。<br>接着利用反向传播机制，计算E关于隐藏层输出$h$的偏导数：<br> $$\begin{align}&\frac{\partial E}{\partial \mathbf h}=\sum_{\omega_j \in\{\omega_O\}\bigcup W_{neg}}\frac{\partial E}{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}\cdot \frac{\partial {\mathbf v_{\omega_j}'}^T\mathbf h}{\partial \mathbf h}\tag{60}\\&=\sum_{\omega_j \in\{\omega_O\}\bigcup W_{neg}}\Big(\sigma({\mathbf v_{\omega_j}'}^T\mathbf h)-t_j\Big)\mathbf v_{\omega_j}':=EH\tag{61}\end{align}$$<br>将$EH$代入公式（23），我们就可以得到CBOW模型关于输入向量的更新公式；对于SG模型，我们需要计算出每个上下文单词的$EH$值，将$EH$值的和代入公式（35）就能够得到其输入向量的更新公式。</p><h2 id="补充说明"><a href="#补充说明" class="headerlink" title="补充说明"></a>补充说明</h2><p>一开始一直不懂word2vec最后得到的应该是输入向量$v<em>ω$还是输出向量和$v′</em>ω$，我在<a href="https://discuss.gluon.ai/t/topic/4180/3" target="_blank" rel="noopener">这个课程</a>中找到了答案，应该是输入向量$v_ω$。课程中还给出了一个例子：<br>以skipgram为例，考虑window_size=1，给定序列abcd。<br>我们需要最大化：$P(b|a)P(a|b)P(c|b)P(b|c)P(d|c)P(c|d)$<br>你会发现上面有三对相互生成。例如下面这对</p><script type="math/tex; mode=display">P(b|a) P(a|b) =  \frac{ \exp(\mathbf{u}_b^\top \mathbf{v}_a)}{ \sum_i \exp(\mathbf{u}_i^\top \mathbf{v}_a)} \frac{ \exp(\mathbf{u}_a^\top \mathbf{v}_b)}{ \sum_i \exp(\mathbf{u}_i^\top \mathbf{v}_b)}</script><p>$u$和$v$在分子等价（$uv$互换不影响全概率的分子大小），但分母上稍有差别。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文接着word2vec的那篇概述再推导一下word2vec中的公式，也是《word2vec Parameter Learning Explained》论文学习笔记，有一些细节的推导我写在了论文处。&lt;br&gt;这篇论文详细地推导和解释了word2vec模型的参数更新公式，包括：&lt;strong&gt;CBOW&lt;/strong&gt;（continuous bag-of-word）模型和&lt;strong&gt;SG&lt;/strong&gt;（skip-gram）模型，以及两种参数优化技术：&lt;strong&gt;hierarchical softmax&lt;/strong&gt; 和 &lt;strong&gt;negative sampling&lt;/strong&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>cw2vec</title>
    <link href="http://yoursite.com/2018/07/09/cw2vec/"/>
    <id>http://yoursite.com/2018/07/09/cw2vec/</id>
    <published>2018-07-09T02:03:28.000Z</published>
    <updated>2018-07-15T06:29:28.787Z</updated>
    
    <content type="html"><![CDATA[<p>本文总结了论文cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information中的核心思想。<br><a id="more"></a></p><h2 id="导语："><a href="#导语：" class="headerlink" title="导语："></a>导语：</h2><p><strong>词向量</strong>算法是自然语言处理领域的基础算法，在序列标注、问答系统和机器翻译等诸多任务中都发挥了重要作用。词向量算法最早由谷歌在2013年提出的 <strong>word2vec</strong>，在接下来的几年里，该算法也经历不断的改进，但大多是仅适用于拉丁字符构成的单词（比如英文、西班牙语、德语等），结合中文语言特性的词向量研究相对较少。<br>中文经过几千年的发展和演变，是一种<strong>强表意</strong>文字，对于我们而言，即使某个字不认识，都或许可以猜到其含义，机器却很难理解这些。比如，“蘒”这个字我们很可能不认识，但里面有“艹”字头，和“禾”木旁，那它也许就是长得像该字右下角部分的某种植物吧。我们可以总结出中文词语一般包含很少的中文字符（我理解为<strong>笔画</strong>），但是中文字符内部包含了很强的语义信息。<br>本文介绍了蚂蚁金服人工智能部与新加坡科技大学一项最新的合作成果：cw2vec——基于汉字笔画信息的中文词向量算法研究，用科学的方法揭示隐藏在一笔一划之间的秘密。此论文在第32届AAAI大会上被高分录用（其中一位审稿人给出了满分，剩下两位也给出了接近满分的评价）。</p><h2 id="相关工作："><a href="#相关工作：" class="headerlink" title="相关工作："></a>相关工作：</h2><p>早在1954年，语言学家Harris提出“Distributional Hypothesis[‘1’] <strong>（分布式假设</strong>）”：语义相似的单词往往会出现在相似的上下文中。这一假设奠定了后续各种词向量的语言学基础，即用数学模型去刻画单词和其上下文的语义相似度。<br>Bengio et al., 2003 [‘2’] 提出了NNLM（基于神经网络的语言模型），由于每次softmax的计算量很大（分母项的计算时间复杂度$O(V)$，$V$是全词表），相继出现了很多快速近似计算策略。<br>为了解决上述问题，谷歌提出了word2vec [‘3,4’] 算法，其中包含了两种策略，一种叫做Negative Sampling（<strong>负采样</strong>），另一种是hierarchical softmax（<strong>层次softmax</strong>）。Negative Sampling的核心思想：每次softmax计算所有单词太慢，那就随机的选几个算一算好了，当然，训练语料中出现次数越多的单词，也就越容易被选中；而Hierarchical Softmax，简单来说，就是建一棵树状的结构，每次自上而下的从根计算到叶子节点，那么就只有对数时间复杂度了！如何构建这棵树可以使得让树的高度尽量小呢？哈夫曼树。<br>词向量模型的核心是构造单词与其上下文的相似度函数，word2vec工具包里面有两种实现方式，分别是<strong>skipgram</strong>（由当前词预测上下文词）和<strong>cbow</strong>（由上下文词预测当前词）。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec1.jpg" alt="此处输入图片的描述"><br>假设当前单词w是“cat”，而上下文单词c是“sat”，算法的目标是给定w最大化c出现概率(skipgram)。在这个算法中，每个单词都被当作一个整体，利用外部的上下文结构信息去学习得到词向量。<br>那么是否可以充分结合单词内部结构的（<strong>亚词</strong>）信息，将其拆分成更细粒度的结构去增强词向量？英文中每个单词所包含的character（字母）较多，每个字母并没有实际的语义表达能力。对于中文词语而言，中文词语可以拆解成character（汉字）。<br>Chen et al., 2015 [‘5’] 提出了CWE模型，思路是把一个中文词语拆分成若干汉字，然后把原词语的向量表示和其中的每一个汉字的向量表示做平均，然后作为新的词语向量。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec2.jpg" alt="此处输入图片的描述"><br>在该算法中，“智能”是一个上下文词语，先拆解成两个汉字“智”和“能”，然后计算出新的词语向量表示；同理，上下文词语“到来”也得到重新计算。CWE保持当前词语不拆分，这里“时代”保持不变。<br>不难想到，将汉字拆分成<strong>偏旁</strong>或许是一种不错的方式，Sun et al., 2014 [‘6’]和Li et al., 2015 [‘7’] 做过相关的研究。<br>然而偏旁只是汉字的一部分，Yu et al., 2017 [‘8’] 提出了更加细化的拆分（JWE），根据人工总结的“字件”，将汉字拆成一个一个的小模块，把词、汉字和字件一起进行联合学习：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec3.jpg" alt="此处输入图片的描述"><br>其中，$w$,$c$和$s$分别表示词语、汉字和字件模块。字件粒度的拆分也取得了超过仅仅利用偏旁信息的方法。<br>此外，Su and Lee, 2017 [‘9’] 提出了GWE模型，尝试从汉字的图片中利用<strong>卷积</strong>自动编码器来提取特征：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec4.jpg" alt="此处输入图片的描述"><br>从汉字图片提取出特征之后，再结合上下文结构信息学习中文词向量。很遗憾的是，根据其原文的描述，这种方式得到的特征基本没有提升，不过这确实是非常有意思的一次试探。</p><h2 id="问题场景："><a href="#问题场景：" class="headerlink" title="问题场景："></a>问题场景：</h2><p>在中文词向量场景下，将中文词语拆解到汉字粒度，虽然会一定程度上提高中文词向量的质量，但是否仍然存在汉字粒度不能刻画的情况？<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec5.jpg" alt="此处输入图片的描述"><br>可以看出，“木材”和“森林”是两个语义很相关的词语，但是当我们拆解到汉字粒度的时候，“木”和“材”这两个字对比“森”和“材”没有一个是相同的（一般会用一个下标去存储一个词语或汉字），因此对于这个例子而言，汉字粒度拆解是不够的。我们所希望得到的是：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec6.jpg" alt="此处输入图片的描述"><br>“木”和“材”可以分别拆解出“木”和“木”（来源于“材”的左半边）结构，而“森”和“林”分别拆解得到多个“木”的相同结构。此外，可以进一步将汉字拆解成偏旁、字件，对于以上例子可以有效提取出语义结构信息，不过我们也分析到：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec7.jpg" alt="此处输入图片的描述"><br>可以看出，“智”的偏旁恰好是“日”，而“日”不能表达出“智”的语义信息。实际上，偏旁的设计是为了方便在字典中查询汉字，因此结构简单、出现频率高变成了首要原则，并不一定恰好能够表达出该汉字的语义信息。此外，将“智”拆分到字件粒度，将会得到“失”，“口”和“日”三个，很不巧的是，这三个字件也均不能表达其汉字语义。我们需要设计出一种新的方法，来重新定义出词语（或汉字）具有语义的结构：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec8.jpg" alt="此处输入图片的描述"><br>这里，“知”是可以表达出“智”语义的模块，如何得到这样的亚词结构，并结合句子上下文设计模型的优化目标，生成出更好的中文词向量，将是后文要探索的内容。</p><h2 id="模型设计："><a href="#模型设计：" class="headerlink" title="模型设计："></a>模型设计：</h2><pre><code>此论文提出了“n元笔画”的概念。所谓“n元笔画”，即就是中文词语（或汉字）连续的n个笔画构成的语义结构。</code></pre><p><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec9.jpg" alt="此处输入图片的描述"><br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec10.jpg" alt="此处输入图片的描述"><br>如上图，n元笔画的生成共有四个步骤。比如说，“大人”这个词语，可以拆开为两个汉字“大”和“人”，然后将这两个汉字拆分成笔画，再将笔画映射到数字编号，进而利用窗口滑动产生n元笔画。其中，n是一个范围，在上述例子中，我们将n取值为3, 4和5。<br>    在论文中提出了一种基于n元笔画的新型的损失函数：</p><script type="math/tex; mode=display">\mathbb{L}=\sum_{w\in D}{\sum_{c\in T\left( w \right)}{\log \sigma \left( sim\left( w,c \right) \right) +\lambda \mathbb{E}_{c'~P}\left[ \log \sigma \left( -sim\left( w,c \right) \right) \right]}}</script><p>其中，$w$和$c$分别为当前词语和上下文词语，$ \sigma  $ 是sigmoid函数， $ T\left( w \right)  $ 是当前词语划窗内的所有词语集合， $D$是训练语料的全部文本。为了避免传统softmax带来的巨大计算量，我们也采用了负采样的方式。 $ c’ $ 为随机选取的词语，称为“负样例”， $ \lambda  $ 是负样例的个数，而$  \mathbb{E}_{c’~P\left( D \right)}\left[ \cdot \right]  $ 则表示负样例 按照词频分布进行的采样，其中语料中出现次数越多的词语越容易被采样到。相似性函数$ sim\left( \cdot ,\cdot \right)  $  被按照如下构造：</p><script type="math/tex; mode=display">sim\left( w,c \right) =\sum_{q\in S\left( w \right)}{\vec{q}\cdot \vec{c}}</script><p>其中，$ \vec{q} $ 为当前词语对应的一个n元笔画向量，而 是其对应的上下文词语的词向量。我们将当前词语拆解为其对应的n元笔画，但保留每一个上下文词语不进行拆解。$ S\left( w \right)  $ 为词语$w$ 所对应的n元笔画的集合。在算法执行前，我们先扫描每一个词语，生成n元笔画集合，针对每一个n元笔画，都有对应的一个n元笔画向量，在算法开始之前做随机初始化，其向量维度和词向量的维度相同。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec11.jpg" alt="此处输入图片的描述"><br>如上图所示，对于“治理 雾霾 刻不容缓”这句话，假设此刻当前词语恰好是“雾霾”，上下文词语是“治理”和“刻不容缓”。首先我们将当前词语“雾霾”拆解成n元笔画并映射成数字编码，然后划窗得到所有的n元笔画，根据我们设计的损失函数，计算每一个n元笔画和上下文词语的相似度，进而根据损失函数求梯度并对上下文词向量和n元笔画向量进行更新。</p><h2 id="实验分析："><a href="#实验分析：" class="headerlink" title="实验分析："></a>实验分析：</h2><p>为了验证cw2vec算法的效果，我们在公开数据集上，与业界最优的几个词向量算法做了对比:<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec12.jpg" alt="此处输入图片的描述"><br>上图中包括2013年谷歌提出的word2vec [‘2,3’] 的两个模型skipgram和cbow，2014年斯坦福提出的GloVe算法 <a href="http://p5vuwy2ht.bkt.clouddn.com/cw2vec10.jpg" target="_blank" rel="noopener">10</a>，2015年清华大学提出的基于汉字的CWE模型 <a href="http://p5vuwy2ht.bkt.clouddn.com/cw2vec5.jpg" target="_blank" rel="noopener">5</a>，以及2017年最新发表的基于像素和字件的中文词向量算法 [‘8,9’]，可以看出cw2vec在word similarity，word analogy，以及文本分类和命名实体识别的任务中均取得了一致性的提升。同时，我们也展示了不同词向量维度下的实验效果：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec13.jpg" alt="此处输入图片的描述"><br>上图为不同维度下在word analogy测试集上的实验结果，左侧为3cosadd，右侧为3cosmul的测试方法。可以看出我们的算法在不同维度的设置下均取得了不错的效果。此外，我们也在小规模语料上进行了测试：<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec14.jpg" alt="此处输入图片的描述"><br>上图是仅选取20%中文维基百科训练语料，在word similarity下测试的结果，skipgram, cbow和GloVe算法由于没有利用中文的特性信息进行加强，所以在小语料上表现较差，而其余四个算法取得了不错的效果，其中我们的算法在两个数据集上均取得的了最优效果。<br> <img src="http://p5vuwy2ht.bkt.clouddn.com/cw2vec15.jpg" alt="此处输入图片的描述"><br>为了更好的探究不同算法的实际效果，我们专门选取了两个词语做案例分析。第一个是环境相关的“水污染”，然后根据词向量利用向量夹角余弦找到与其语义最接近的词语。GWE找到了一些和“污”字相关的词语，比如“污泥”，“污渍”和“污垢”，而JWE则更加强调后两个字“污染”GloVe找到了一些奇怪的相近词语，比如“循环系统”，“神经系统”。CWE找到的相近词语均包含“水”和“污”这两个字，我们猜测是由于其利用汉字信息直接进行词向量加强的原因。此外，只有cw2vec找到了“水质”这个相关词语，我们认为是由于n元笔画和上下文信息对词向量共同作用的结果。第二个例子，我们特别选择了“孙悟空”这个词语，该角色出现在中国的名著《西游记》和知名日本动漫《七龙珠》中，cw2vec找到的均为相关的角色或著作名称。</p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><ol><li>Harris, Zellig S. “Distributional structure.” Word 1954.</li><li>Bengio, Yoshua, et al. “A neural probabilistic language model.” JMLR 2003.</li><li>Mikolov, Tomas, et al. “Efficient estimation of word representations in vector space.” arXiv preprint arXiv:1301.3781 (2013).</li><li>Mikolov, Tomas, et al. “Distributed representations of words and phrases and their compositionality.” NIPS 2013.</li><li>Chen, Xinxiong, et al. “Joint Learning of Character and Word Embeddings.” IJCAI 2015.</li><li>Sun, Yaming, et al. “Radical-enhanced Chinese character embedding.” ICNIP 2014.</li><li>Li, Yanran, et al. “Component-enhanced Chinese character embeddings.” arXiv preprint arXiv:1508.06669 (2015).</li><li>Yu, Jinxing, et al. “Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components.” EMNLP 2017.</li><li>Su, Tzu-Ray, and Hung-Yi Lee. “Learning Chinese Word Representations From Glyphs Of Characters.” EMNLP 2017.</li><li>Pennington, Jeffrey, et al. “Glove: Global vectors for word representation.” EMNLP 2014.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文总结了论文cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information中的核心思想。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读</title>
    <link href="http://yoursite.com/2018/07/09/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2018/07/09/论文阅读/</id>
    <published>2018-07-09T01:58:47.000Z</published>
    <updated>2018-07-10T08:47:03.887Z</updated>
    
    <content type="html"><![CDATA[<p>整理了一下读过的论文，会继续更新。<br><a id="more"></a></p><h2 id="词向量："><a href="#词向量：" class="headerlink" title="词向量："></a>词向量：</h2><h3 id="已读："><a href="#已读：" class="headerlink" title="已读："></a>已读：</h3><p>cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information<br><a href="https://zkq1314.github.io/2018/07/09/cw2vec/" target="_blank" rel="noopener">cw2vec</a><br>word2vec：Distributed Representations of Words and Phrases and their Compositionality<br>word2vec：Efficient Estimation of Word Representations in Vector Space<br>word2vec Parameter Learning Explained<br><a href="https://zkq1314.github.io/2018/05/03/word2vec/#more" target="_blank" rel="noopener">word2vec</a></p><h3 id="未读："><a href="#未读：" class="headerlink" title="未读："></a>未读：</h3><p>NNLM：A Neural Probabilistic Language Model<br>Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components<br>Learning Chinese Word Representations From Glyphs Of Characters</p><h2 id="文本分类："><a href="#文本分类：" class="headerlink" title="文本分类："></a>文本分类：</h2><h3 id="已读：-1"><a href="#已读：-1" class="headerlink" title="已读："></a>已读：</h3><p>TextCNN：Convolutional neural networks for sentence classification<br>Char-CNN：Character-level convolutional networks for text classification<br>Text-RNN：Recurrent neural network for text classification with multi-task learning<br>RCNN：Recurrent Convolutional Neural Networks for Text Classification</p><h3 id="未读：-1"><a href="#未读：-1" class="headerlink" title="未读："></a>未读：</h3><p>fasttext：Bag of Tricks for Efficient Text Classification<br>TextRNN+Attention：Hierarchical Attention Networks for Document Classification</p><h2 id="机器翻译："><a href="#机器翻译：" class="headerlink" title="机器翻译："></a>机器翻译：</h2><h3 id="未读：-2"><a href="#未读：-2" class="headerlink" title="未读："></a>未读：</h3><p>Attention：NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</p><h2 id="关系抽取："><a href="#关系抽取：" class="headerlink" title="关系抽取："></a>关系抽取：</h2><h3 id="已读：-2"><a href="#已读：-2" class="headerlink" title="已读："></a>已读：</h3><p>Large Scaled Relation Extraction with Reinforcement Learning</p><h2 id="图像分类："><a href="#图像分类：" class="headerlink" title="图像分类："></a>图像分类：</h2><h3 id="已读：-3"><a href="#已读：-3" class="headerlink" title="已读："></a>已读：</h3><p>AlexNet：ImageNet Classification with Deep Convolutional Neural Networks<br>GoogLeNet：Going deeper with convolutions<br>ResNet：Deep Residual Learning for Image Recognition<br>VGG：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整理了一下读过的论文，会继续更新。&lt;br&gt;
    
    </summary>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>BP推导</title>
    <link href="http://yoursite.com/2018/06/07/BP%E6%8E%A8%E5%AF%BC/"/>
    <id>http://yoursite.com/2018/06/07/BP推导/</id>
    <published>2018-06-07T01:32:39.000Z</published>
    <updated>2018-06-07T01:37:07.025Z</updated>
    
    <content type="html"><![CDATA[<p>整理一下BP推导过程，以备查验。<br><a id="more"></a><br><img src="http://p5vuwy2ht.bkt.clouddn.com/bp%E6%8E%A8%E5%AF%BC.png" alt="BP推导"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整理一下BP推导过程，以备查验。&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="BP" scheme="http://yoursite.com/tags/BP/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>剑指offer面试题4find</title>
    <link href="http://yoursite.com/2018/05/14/%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%984find/"/>
    <id>http://yoursite.com/2018/05/14/剑指offer面试题4find/</id>
    <published>2018-05-14T13:29:36.000Z</published>
    <updated>2018-05-14T13:35:03.084Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h2><p>在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p><ul><li>1 2 8  9</li><li>2 4 9  12</li><li>4 7 10 13</li><li>6 8 11 15</li></ul><a id="more"></a><h3 id="解法："><a href="#解法：" class="headerlink" title="解法："></a>解法：</h3><p>可以发现右上角的数字是一行中最大的，又是这一列中最小的，那么我们可以从右上角开始查找。如果当前查找的数字小于目标整数，删除当前行；如果当前查找数字大于目标整数，则删除当前列。直到查找完整个矩阵。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">interview4</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">find</span><span class="params">(<span class="keyword">int</span>[][] matrix,<span class="keyword">int</span> number)</span> </span>&#123;</span><br><span class="line"><span class="keyword">boolean</span> found=<span class="keyword">false</span>;</span><br><span class="line"><span class="keyword">int</span> rows=matrix.length;</span><br><span class="line"><span class="keyword">int</span> columns=matrix[<span class="number">0</span>].length;</span><br><span class="line"><span class="keyword">if</span> (matrix!=<span class="keyword">null</span> &amp;&amp; rows&gt;<span class="number">0</span> &amp;&amp; columns&gt;<span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">int</span> row=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> column=columns-<span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span>(row&lt;rows &amp;&amp; column&gt;<span class="number">1</span>) &#123;</span><br><span class="line"><span class="keyword">if</span>(matrix[row][column]==number) &#123;</span><br><span class="line">found=<span class="keyword">true</span>;</span><br><span class="line">System.out.println(<span class="string">"row:"</span>+row+<span class="string">" column:"</span>+column);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(matrix[row][column]&gt;number)</span><br><span class="line">column--;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span>(matrix[row][column]&lt;number)</span><br><span class="line">row++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> found;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"><span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line"><span class="keyword">int</span>[][] matrix= &#123;&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">9</span>&#125;,&#123;<span class="number">2</span>,<span class="number">4</span>,<span class="number">9</span>,<span class="number">12</span>&#125;,&#123;<span class="number">4</span>,<span class="number">7</span>,<span class="number">10</span>,<span class="number">13</span>&#125;,&#123;<span class="number">6</span>,<span class="number">8</span>,<span class="number">11</span>,<span class="number">15</span>&#125;&#125;;</span><br><span class="line">System.out.println(find(matrix,<span class="number">10</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：&quot;&gt;&lt;a href=&quot;#题目：&quot; class=&quot;headerlink&quot; title=&quot;题目：&quot;&gt;&lt;/a&gt;题目：&lt;/h2&gt;&lt;p&gt;在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 2 8  9&lt;/li&gt;
&lt;li&gt;2 4 9  12&lt;/li&gt;
&lt;li&gt;4 7 10 13&lt;/li&gt;
&lt;li&gt;6 8 11 15&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="剑指offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87offer/"/>
    
    
      <category term="剑指offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87offer/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>剑指offer面试题3duplicate</title>
    <link href="http://yoursite.com/2018/05/14/%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%983duplicate/"/>
    <id>http://yoursite.com/2018/05/14/剑指offer面试题3duplicate/</id>
    <published>2018-05-14T11:27:16.000Z</published>
    <updated>2018-05-14T11:43:58.822Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h2><p>在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。<br><a id="more"></a></p><h3 id="解法一：查找数组中重复的数字，加一个数组空间，判断"><a href="#解法一：查找数组中重复的数字，加一个数组空间，判断" class="headerlink" title="解法一：查找数组中重复的数字，加一个数组空间，判断"></a>解法一：查找数组中重复的数字，加一个数组空间，判断</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">duplicate2</span><span class="params">(<span class="keyword">int</span>[] numbers, <span class="keyword">int</span> length)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span>[] k = <span class="keyword">new</span> <span class="keyword">boolean</span>[length];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (k[numbers[i]] == <span class="keyword">true</span>) &#123;</span><br><span class="line">            System.out.println(numbers[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        k[numbers[i]] = <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>若某数字存在于数组中，对应的k[numbers[i]]被设置为true，当该数字再出现时，即可判断为重复。时间复杂度为$O(n)$,空间复杂度为$O(n)$</p><h3 id="解法二：与下标进行匹配，交换后，如果都在各自的位置上则没有重复的，否则就能找出重复的"><a href="#解法二：与下标进行匹配，交换后，如果都在各自的位置上则没有重复的，否则就能找出重复的" class="headerlink" title="解法二：与下标进行匹配，交换后，如果都在各自的位置上则没有重复的，否则就能找出重复的"></a>解法二：与下标进行匹配，交换后，如果都在各自的位置上则没有重复的，否则就能找出重复的</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">duplicate1</span><span class="params">(<span class="keyword">int</span> nums[],<span class="keyword">int</span> n)</span></span>&#123;  </span><br><span class="line">    <span class="comment">//首先判断数组输入的合法性   </span></span><br><span class="line">    <span class="keyword">if</span>(nums==<span class="keyword">null</span>||n&lt;=<span class="number">0</span>)<span class="keyword">return</span> <span class="keyword">false</span>;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;  </span><br><span class="line">        <span class="keyword">if</span>(nums[i]&lt;<span class="number">0</span>||nums[i]&gt;=n)  </span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)&#123;  </span><br><span class="line">        <span class="keyword">while</span>(nums[i]!=i)&#123;  </span><br><span class="line">            <span class="keyword">if</span>(nums[i]!=nums[nums[i]])&#123;  </span><br><span class="line">                <span class="keyword">int</span> t=nums[i];  </span><br><span class="line">                nums[i]=nums[nums[i]];  </span><br><span class="line">                nums[nums[i]]=nums[i];  </span><br><span class="line">            &#125;<span class="keyword">else</span>  </span><br><span class="line">            &#123;  </span><br><span class="line">                <span class="comment">//dumplication=nums[i];  </span></span><br><span class="line">                System.out.println(nums[i]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尽管有两重循环，但是每个数字最多经过两次交换就能找到属于它的位置，时间复杂度为$O(n)$,空间复杂度为$O(1)$</p><h3 id="解法三：二分查找"><a href="#解法三：二分查找" class="headerlink" title="解法三：二分查找"></a>解法三：二分查找</h3><p>不修改数组找出重复的数字，将数组分为两部分，数数组中1~mid的数的个数与mid+1~n的数字的个数，如果过半则一定存在重复的数字。这种算法不能保证找出所有的重复数字。时间复杂度为$O(nlogn)$，空间复杂度$O(1)$<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">dumplicate3</span><span class="params">(<span class="keyword">int</span> number[],<span class="keyword">int</span> n)</span></span>&#123;  </span><br><span class="line">    <span class="comment">//判断输入的数组的合法性  </span></span><br><span class="line">    <span class="keyword">if</span>(number==<span class="keyword">null</span>||number.length==<span class="number">0</span>)<span class="keyword">return</span> -<span class="number">1</span>;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;number.length;i++)&#123;  </span><br><span class="line">        <span class="keyword">if</span>(number[i]&lt;<span class="number">0</span>||number[i]&gt;=n)<span class="keyword">return</span> -<span class="number">1</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">int</span> start=<span class="number">0</span>;<span class="keyword">int</span> end=n-<span class="number">1</span>;  </span><br><span class="line">    <span class="keyword">while</span>(start&lt;=end)&#123;  </span><br><span class="line">        <span class="keyword">int</span> mid=(start+end)/<span class="number">2</span>+start;  </span><br><span class="line">        <span class="keyword">int</span> count=countRange(number,n,start,mid);  </span><br><span class="line">        <span class="keyword">if</span>(end==start)&#123;  </span><br><span class="line">            <span class="keyword">if</span>(count&gt;<span class="number">1</span>)  </span><br><span class="line">                <span class="keyword">return</span> start;  </span><br><span class="line">            <span class="keyword">else</span>  </span><br><span class="line">                <span class="keyword">break</span>;  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="keyword">if</span>(count&gt;(mid-start+<span class="number">1</span>))  </span><br><span class="line">            end=mid;  </span><br><span class="line">        <span class="keyword">else</span>  </span><br><span class="line">            start=mid+<span class="number">1</span>;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;  </span><br><span class="line">&#125;  </span><br><span class="line"><span class="comment">//统计在特定的区间内的数字的个数，比如数组中1-7内的数字出现的次数  </span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">countRange</span><span class="params">(<span class="keyword">int</span> number[],<span class="keyword">int</span> n,<span class="keyword">int</span> start,<span class="keyword">int</span> end)</span></span>&#123;  </span><br><span class="line">    <span class="keyword">if</span>(number==<span class="keyword">null</span>||n&lt;=<span class="number">0</span>)  </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">int</span> count=<span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;number.length;i++)&#123;  </span><br><span class="line">        <span class="keyword">if</span>(number[i]&gt;=start&amp;&amp;number[i]&lt;=end)count++;  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> count;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：&quot;&gt;&lt;a href=&quot;#题目：&quot; class=&quot;headerlink&quot; title=&quot;题目：&quot;&gt;&lt;/a&gt;题目：&lt;/h2&gt;&lt;p&gt;在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是重复的数字2或者3。&lt;br&gt;
    
    </summary>
    
      <category term="剑指offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87offer/"/>
    
    
      <category term="剑指offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87offer/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>剑指offer面试题2Singleton</title>
    <link href="http://yoursite.com/2018/05/14/%E5%89%91%E6%8C%87offer%E9%9D%A2%E8%AF%95%E9%A2%982Singleton/"/>
    <id>http://yoursite.com/2018/05/14/剑指offer面试题2Singleton/</id>
    <published>2018-05-14T07:32:36.000Z</published>
    <updated>2018-05-14T07:50:22.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h2><p>设计一个类，只能生成该类的一个实例。<br><a id="more"></a></p><h3 id="解法一：只适用于单线程环境"><a href="#解法一：只适用于单线程环境" class="headerlink" title="解法一：只适用于单线程环境"></a>解法一：只适用于单线程环境</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton instance= <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">//构造函数设计为私有的，以禁止在其他类中创建实例</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(instance==<span class="keyword">null</span>)&#123;</span><br><span class="line">            instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Singleton s = Singleton.getInstance();</span><br><span class="line">        Singleton s2 = s;</span><br><span class="line">        Singleton s3 = Singleton.getInstance();</span><br><span class="line">        System.out.println(s == s2);</span><br><span class="line">        System.out.println(s == s3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述代码在Singleton类的静态属性instance中，只有在instance为null的时候才创建一个实例以避免重复创建。同时我们把构造函数定义为私有函数，这样就能确保只创建一个实例。</p><p>可以运行main函数看到结果为，两个true。说明两次调用Singleton类的getInstance()方法，却只创建了一个实例。</p><h3 id="解法二：多线程环境，但效率不高"><a href="#解法二：多线程环境，但效率不高" class="headerlink" title="解法二：多线程环境，但效率不高"></a>解法二：多线程环境，但效率不高</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton instance= <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">//构造函数设计为私有的，以禁止在其他类中创建实例</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> Singleton <span class="title">getInstance</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(instance==<span class="keyword">null</span>)&#123;</span><br><span class="line">            instance = <span class="keyword">new</span> Singleton();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Singleton s = Singleton.getInstance();</span><br><span class="line">        Singleton s2 = s;</span><br><span class="line">        Singleton s3 = Singleton.getInstance();</span><br><span class="line">        System.out.println(s == s2);</span><br><span class="line">        System.out.println(s == s3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为了保证在多线程环境下我们还是只能得到该类的一个实例，只需要在getInstance()方法加上同步关键字sychronized，这样需要可以实现在多线程环境保证只创建一个实例，但每次调用getInstance()方法时都被synchronized关键字锁住了，会引起线程阻塞，会影响程序的性能。</p><h3 id="双重锁定的单例模式"><a href="#双重锁定的单例模式" class="headerlink" title="双重锁定的单例模式"></a>双重锁定的单例模式</h3><p>为了在多线程环境下，不影响程序的性能，不让线程每次调用getInstance()方法时都加锁，而只是在实例未被创建时再加锁，在加锁处理里面还需要判断一次实例是否已存在，这种做法被称为“<strong>双重锁定</strong>”。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton instance= <span class="keyword">null</span>;</span><br><span class="line">    <span class="comment">//构造函数设计为私有的，以禁止在其他类中创建实例</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;<span class="comment">//先判断实例是否存在，若不存在再对类进行加锁处理</span></span><br><span class="line">            <span class="keyword">synchronized</span> (Singleton.class) &#123;    </span><br><span class="line">               <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;    </span><br><span class="line">                   instance = <span class="keyword">new</span> Singleton();   </span><br><span class="line">               &#125;    </span><br><span class="line">            &#125;    </span><br><span class="line">        &#125;    </span><br><span class="line">        <span class="keyword">return</span> instance;   </span><br><span class="line">    &#125;  </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Singleton s = Singleton.getInstance();</span><br><span class="line">        Singleton s2 = s;</span><br><span class="line">        Singleton s3 = Singleton.getInstance();</span><br><span class="line">        System.out.println(s == s2);</span><br><span class="line">        System.out.println(s == s3);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="利用静态构造函数"><a href="#利用静态构造函数" class="headerlink" title="利用静态构造函数"></a>利用静态构造函数</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton1</span> </span>&#123;  </span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton1</span><span class="params">()</span> </span>&#123;&#125;  </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton1 single = <span class="keyword">new</span> Singleton1();  </span><br><span class="line">    <span class="comment">//静态工厂方法   </span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton1 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">        <span class="keyword">return</span> single;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>饿汉式在类创建的同时就已经创建好一个静态的对象供系统使用，以后不再改变，所以天生是线程安全的。</p><h3 id="使用静态内部类实现的单例模式"><a href="#使用静态内部类实现的单例模式" class="headerlink" title="使用静态内部类实现的单例模式"></a>使用静态内部类实现的单例模式</h3><p>在Singleton内部定义了一个私有类型LazyHolder，当第一次使用这个嵌套类型的时候，会调用静态构造函数创建Singleton的实例INSTANCE。如果不调用方法getInstance()就不会创建内部静态类实例，实现<strong>按需创建</strong>。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton</span> </span>&#123;</span><br><span class="line">    <span class="comment">//构造函数设计为私有的，以禁止在其他类中创建实例</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//静态内部类</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LazyHolder</span> </span>&#123;    </span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Singleton INSTANCE = <span class="keyword">new</span> Singleton();    </span><br><span class="line">     &#125;     </span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton <span class="title">getInstance</span><span class="params">()</span> </span>&#123;    </span><br><span class="line">        <span class="keyword">return</span> LazyHolder.INSTANCE;    </span><br><span class="line">     &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;题目：&quot;&gt;&lt;a href=&quot;#题目：&quot; class=&quot;headerlink&quot; title=&quot;题目：&quot;&gt;&lt;/a&gt;题目：&lt;/h2&gt;&lt;p&gt;设计一个类，只能生成该类的一个实例。&lt;br&gt;
    
    </summary>
    
      <category term="剑指offer" scheme="http://yoursite.com/categories/%E5%89%91%E6%8C%87offer/"/>
    
    
      <category term="剑指offer" scheme="http://yoursite.com/tags/%E5%89%91%E6%8C%87offer/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://yoursite.com/2018/05/03/word2vec/"/>
    <id>http://yoursite.com/2018/05/03/word2vec/</id>
    <published>2018-05-03T09:39:25.000Z</published>
    <updated>2018-07-15T06:29:35.784Z</updated>
    
    <content type="html"><![CDATA[<p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。<br><a id="more"></a></p><h2 id="什么是Word2Vec"><a href="#什么是Word2Vec" class="headerlink" title="什么是Word2Vec"></a>什么是Word2Vec</h2><p>word2vec是 Google 在 2013 年中开源的一款将词表征为实数值向量的高效工具 ，采用的模型有 CBOW （Continuous Bag-Of-Words ，即连续的词袋模型）和 Skip-Gram  两种。从直观上理解，Skip-Gram是给定input word来预测上下文。而CBOW是给定上下文，来预测input word。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec1.jpg" alt="此处输入图片的描述"><br>word2vec通过训练，可以把对文本内容的处理简化为$K$维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。因此，word2vec 输出的词向量可以被用来做很多 NLP 相关的工作，比如聚类、找同义词、词性分析等 。而 word2vec被人广为传颂的地方是其向量的加法组合运算 （Additive  Compositionality ），官网上的例子是:vector(‘Paris’)  - vector(‘France’) + vector(‘Italy’) ≈vector(‘Rome’) ，vector(‘king’) - vector(‘man’) + vector(‘woman’) ≈ vector(‘queen’)。</p><h2 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h2><h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><h4 id="One-hot-Representation"><a href="#One-hot-Representation" class="headerlink" title="One-hot Representation"></a>One-hot Representation</h4><p>NLP 相关任务中最常见的第一步是创建一个词表库并把每个词顺序编号。这实际就是词表示方法中的 One-hot Representation，这种方法把每个词顺序编号，每个词就是一个很长的向量 ，向量的维度等于词表大小，只有对应位置上的数字为 1，其他都为 0。当然在实际应用中，一般 采用稀疏编码存储，主要采用词的编号 。<br>这种表示方法一个最大的问题是<strong>无法捕捉词与之间的相似度</strong>，就算是近义词也无法从向量中看出任何关系。 此外这种表示方法还容易发生<strong>维数灾难</strong>，尤其是在Deep Learning相关的一些应用中。</p><h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h4><p>Distributed representation 最早由 Hinton在 1986 年提出 。其基本思想是通过训练将每个词映射成 $K$维实数向量（ $K$一般为模型中的超参数 ），通过词之间的距离（比如 cosine相似度 、欧氏距离等）来判断它们之间的语义相似度。而 word2vec 使用的就是这种 Distributed representation的词向量表示方法。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Skip-Gram模型的基础形式非常简单，为了更清楚地解释模型，我们先从最一般的基础模型来看Word2Vec（下文中所有的Word2Vec都是指Skip-Gram模型）。<br>Word2Vec模型实际上分为了两个部分，第一部分为建立模型，第二部分是通过模型获取嵌入词向量。Word2Vec的整个建模过程实际上与<strong>自编码器</strong>（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵——后面我们将会看到这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”。基于训练数据建模的过程，我们给它一个名字叫“<strong>Fake Task</strong>”，意味着建模并不是我们最终的目的。</p><blockquote><p>上面提到的这种方法实际上会在无监督特征学习（unsupervised feature learning）中见到，最常见的就是自编码器（auto-encoder）：通过在隐层将输入进行编码压缩，继而在输出层将数据解码恢复初始状态，训练完成后，我们会将输出层“砍掉”，仅保留隐层。</p></blockquote><h3 id="Fake-Task"><a href="#Fake-Task" class="headerlink" title="Fake Task"></a>Fake Task</h3><p>我们在上面提到，训练模型的真正目的是获得模型基于训练数据学得的隐层权重。为了得到这些权重，我们首先要构建一个完整的神经网络作为我们的“Fake Task”，后面再返回来看通过“Fake Task”我们如何间接地得到这些词向量。<br>接下来我们来看看如何训练我们的神经网络。假如我们有一个句子“The dog barked at the mailman”。</p><ol><li>首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；</li><li>有了input word以后，我们再定义一个叫做skip_window的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。如果我们设置skip_window=2，那么我们最终获得窗口中的词（包括input word在内）就是<strong>[‘The’, ‘dog’，’barked’, ‘at’]</strong>。skip_window=2代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小span=2x2=4。另一个参数叫num_skips，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当skip_window=2，num_skips=2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 <strong>(‘dog’, ‘barked’)，(‘dog’, ‘the’)</strong>。</li><li>神经网络基于这些训练数据将会输出一个概率分布，这个概率代表着我们的词典中的每个词是output word的可能性。这句话有点绕，我们来看个栗子。第二步中我们在设置skip_window和num_skips=2的情况下获得了两组训练数据。假如我们先拿一组数据 (‘dog’, ‘barked’) 来训练神经网络，那么模型通过学习这个训练样本，会告诉我们词汇表中每个单词是“barked”的概率大小。</li></ol><p>模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。举个栗子，如果我们向神经网络模型中输入一个单词“Soviet“，那么最终模型的输出概率中，像“Union”， ”Russia“这种相关词的概率将远高于像”watermelon“，”kangaroo“非相关词的概率。因为”Union“，”Russia“在文本中更大可能在”Soviet“的窗口中出现。我们将通过给神经网络输入文本中成对的单词来训练它完成上面所说的概率计算。下面的图中给出了一些我们的训练样本的例子。我们选定句子“The quick brown fox jumps over lazy dog”，设定我们的窗口大小为2（window_size=2），也就是说我们仅选输入词前后各两个词和输入词进行组合。下图中，蓝色代表input word，方框内代表位于窗口内的单词。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec2.jpg" alt="此处输入图片的描述"><br>我们的模型将会从每对单词出现的次数中习得统计结果。例如，我们的神经网络可能会得到更多类似（“Soviet“，”Union“）这样的训练样本对，而对于（”Soviet“，”Sasquatch“）这样的组合却看到的很少。因此，当我们的模型完成训练后，给定一个单词”Soviet“作为输入，输出的结果中”Union“或者”Russia“要比”Sasquatch“被赋予更高的概率。</p><h3 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h3><p>我们如何来表示这些单词呢？首先，我们都知道神经网络只能接受数值输入，我们不可能把一个单词字符串作为输入，因此我们得想个办法来表示这些单词。最常用的办法就是基于训练文档来构建我们自己的词汇表（vocabulary）再对单词进行one-hot编码。<br>假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表。我们对这10000个单词进行one-hot编码，得到的每个单词都是一个10000维的向量，向量每个维度的值只有0或者1，假如单词ants在词汇表中的出现位置为第3个，那么ants的向量就是一个第三维度取值为1，其他维都为0的10000维的向量（ants=[0, 0, 1, 0, …, 0]）。<br>还是上面的例子，“The dog barked at the mailman”，那么我们基于这个句子，可以构建一个大小为5的词汇表（忽略大小写和标点符号）：(“the”, “dog”, “barked”, “at”, “mailman”)，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。<br>模型的输入如果为一个10000维的向量，那么输出也是一个10000维度（词汇表的大小）的向量，它包含了10000个概率，每一个概率代表着当前词是输入样本中output word的概率大小。<br>下图是我们神经网络的结构：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec3.jpg" alt="此处输入图片的描述"><br>隐层没有使用任何激活函数，但是输出层使用了sotfmax。<br>我们基于成对的单词来对神经网络进行训练，训练样本是 ( input word, output word ) 这样的单词对，input word和output word都是one-hot编码的向量。最终模型的输出是一个概率分布。</p><h3 id="隐层"><a href="#隐层" class="headerlink" title="隐层"></a>隐层</h3><p>说完单词的编码和训练样本的选取，我们来看下我们的隐层。如果我们现在想用300个特征来表示一个单词（即每个词可以被表示为300维的向量）。那么隐层的权重矩阵应该为10000行，300列（隐层有300个结点）。<br>Google在最新发布的基于Google news数据集训练的模型中使用的就是300个特征的词向量。词向量的维度是一个可以调节的超参数（在Python的gensim包中封装的Word2Vec接口默认的词向量大小为100， window_size为5）。<br>看下面的图片，左右两张图分别从不同角度代表了输入层-隐层的权重矩阵。左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量。从右边的图来看，每一行实际上代表了每个单词的词向量。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec4.jpg" alt="此处输入图片的描述"><br>所以我们最终的目标就是<strong>学习这个隐层的权重矩阵</strong>。<br>我们现在回来接着通过模型的定义来训练我们的这个模型。<br>上面我们提到，input word和output word都会被我们进行one-hot编码。仔细想一下，我们的输入被one-hot编码以后大多数维度上都是0（实际上仅有一个位置为1），所以这个向量相当稀疏，那么会造成什么结果呢。如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘，它会消耗相当大的计算资源，为了高效计算，它仅仅会选择矩阵中对应的向量中维度值为1的索引行（这句话很绕），看图就明白。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec5.jpg" alt="此处输入图片的描述"><br>我们来看一下上图中的矩阵运算，左边分别是1 x 5和5 x 3的矩阵，结果应该是1 x 3的矩阵，按照矩阵乘法的规则，结果的第一行第一列元素为0 x 17 + 0 x 23 + 0 x 4 + 1 x 10 + 0 x 11 = 10，同理可得其余两个元素为12，19。如果10000个维度的矩阵采用这样的计算方式是十分低效的。<br>为了有效地进行计算，这种稀疏状态下不会进行矩阵乘法计算，可以看到矩阵的计算的结果实际上是矩阵对应的向量中值为1的索引，上面的例子中，左边向量中取值为1的对应维度为3（下标从0开始），那么计算结果就是矩阵的第3行（下标从0开始）—— [10, 12, 19]，这样模型中的隐层权重矩阵便成了一个”查找表“（lookup table），进行矩阵计算时，直接去查输入向量中取值为1的维度下对应的那些权重值。隐层的输出就是每个输入单词的“<strong>嵌入词向量</strong>”。</p><h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p>经过神经网络隐层的计算，ants这个词会从一个1 x 10000的向量变成1 x 300的向量，再被输入到输出层。输出层是一个softmax回归分类器，它的每个结点将会输出一个0-1之间的值（概率），这些所有输出层神经元结点的概率之和为1。<br>下面是一个例子，训练样本为 (input word: “ants”， output word: “car”) 的计算示意图。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec6.jpg" alt="此处输入图片的描述"></p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>在第一部分讲解完成后，我们会发现Word2Vec模型是一个超级大的神经网络（权重矩阵规模非常大）。<br>举个栗子，我们拥有10000个单词的词汇表，我们如果想嵌入300维的词向量，那么我们的输入-隐层权重矩阵和隐层-输出层的权重矩阵都会有 10000 x 300 = 300万个权重，在如此庞大的神经网络中进行梯度下降是相当慢的。更糟糕的是，你需要大量的训练数据来调整这些权重并且避免过拟合。百万数量级的权重矩阵和亿万数量级的训练样本意味着训练这个模型将会是个灾难（太凶残了）。<br>Word2Vec 的作者在它的第二篇论文中强调了这些问题，下面是作者在第二篇论文中的三个创新：</p><ol><li>将常见的单词组合（word pairs）或者词组作为单个“words”来处理。</li><li>对高频次单词进行抽样来减少训练样本的个数。</li><li>对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担。</li></ol><p>事实证明，对常用词抽样并且对优化目标采用“negative sampling”不仅降低了训练过程中的计算负担，还提高了训练的词向量的质量。</p><h3 id="Word-pairs-and-“phases”"><a href="#Word-pairs-and-“phases”" class="headerlink" title="Word pairs and “phases”"></a>Word pairs and “phases”</h3><p>论文的作者指出，一些单词组合（或者词组）的含义和拆开以后具有完全不同的意义。比如“Boston Globe”是一种报刊的名字，而单独的“Boston”和“Globe”这样单个的单词却表达不出这样的含义。因此，在文章中只要出现“Boston Globe”，我们就应该把它作为一个单独的词来生成其词向量，而不是将其拆开。同样的例子还有“New York”，“United Stated”等。<br>在Google发布的模型中，它本身的训练样本中有来自Google News数据集中的1000亿的单词，但是除了单个单词以外，单词组合（或词组）又有3百万之多。</p><h3 id="对高频词抽样"><a href="#对高频词抽样" class="headerlink" title="对高频词抽样"></a>对高频词抽样</h3><p>在模型部分的讲解中，我们展示了训练样本是如何从原始文档中生成出来的，这里我再重复一次。我们的原始文本为“The quick brown fox jumps over the laze dog”，如果我使用大小为2的窗口，那么我们可以得到图中展示的那些训练样本。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec2.jpg" alt="此处输入图片的描述"><br>但是对于“the”这种常用高频单词，这样的处理方式会存在下面两个问题：</p><ol><li>当我们得到成对的单词训练样本时，(“fox”, “the”) 这样的训练样本并不会给我们提供关于“fox”更多的语义信息，因为“the”在每个单词的上下文中几乎都会出现。</li><li>由于在文本中“the”这样的常用词出现概率很大，因此我们将会有大量的（”the“，…）这样的训练样本，而这些样本数量远远超过了我们学习“the”这个词向量所需的训练样本数。</li></ol><p>Word2Vec通过“抽样”模式来解决这种高频词问题。它的基本思想如下：对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。<br>如果我们设置窗口大小（即），并且从我们的文本中删除所有的“the”，那么会有下面的结果：</p><ol><li>由于我们删除了文本中所有的“the”，那么在我们的训练样本中，“the”这个词永远也不会出现在我们的上下文窗口中。</li><li>当“the”作为input word时，我们的训练样本数至少会减少10个。</li></ol><blockquote><p>这句话应该这么理解，假如我们的文本中仅出现了一个“the”，那么当这个“the”作为input word时，我们设置span=10，此时会得到10个训练样本 (“the”, …) ，如果删掉这个“the”，我们就会减少10个训练样本。实际中我们的文本中不止一个“the”，因此当“the”作为input word的时候，至少会减少10个训练样本。</p></blockquote><p>上面提到的这两个影响结果实际上就帮助我们解决了高频词带来的问题。</p><h3 id="抽样率"><a href="#抽样率" class="headerlink" title="抽样率"></a>抽样率</h3><p>word2vec的C语言代码实现了一个计算在词汇表中保留某个词概率的公式。<br>$ω_i$ 是一个单词，$Z(ω_i)$ 是 $ω_i$ 这个单词在所有语料中出现的频次。举个例子，如果单词“peanut”在10亿规模大小的语料中出现了1000次，那么 Z(peanut) = 1000/1000000000 = 1e - 6。</p><p>在代码中还有一个参数叫“sample”，这个参数代表一个阈值，默认值为0.001（在gensim包中的Word2Vec类说明中，这个参数默认为0.001，文档中对这个参数的解释为“ threshold for configuring which higher-frequency words are randomly downsampled”）。这个值越小意味着这个单词被保留下来的概率越小（即有越大的概率被我们删除）。<br>$P(ω_i)$ 代表着保留某个单词的概率：</p><script type="math/tex; mode=display">P\left( w_i \right) =\left( \sqrt{\frac{z\left( w_i \right)}{0.001}}+1 \right) \cdot \frac{0.001}{z\left( w_i \right)}</script><blockquote><p>你将会注意到论文中定义的函数与这里有些区别，但是这里版本是C代码中实现的，是比较权威的。论文中公式为：</p><script type="math/tex; mode=display">P\left( w_i \right) =1-\sqrt{\frac{t}{f\left( w_i \right)}}</script></blockquote><p><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec8.jpg" alt="此处输入图片的描述"><br>图中x轴代表着 $Z(ω_i)$ ，即单词 $ω_i$ 在语料中出现频率，y轴代表某个单词被保留的概率。对于一个庞大的语料来说，单个单词的出现频率不会很大，即使是常用词，也不可能特别大。<br>从这个图中，我们可以看到，随着单词出现频率的增高，它被采样保留的概率越来越小。</p><h3 id="负采样（negative-sampling）"><a href="#负采样（negative-sampling）" class="headerlink" title="负采样（negative sampling）"></a>负采样（negative sampling）</h3><p>训练一个神经网络意味着要输入训练样本并且不断调整神经元的权重，从而不断提高对目标的准确预测。每当神经网络经过一个训练样本的训练，它的权重就会进行一次调整。<br>正如我们上面所讨论的，vocabulary的大小决定了我们的Skip-Gram神经网络将会拥有大规模的权重矩阵，所有的这些权重需要通过我们数以亿计的训练样本来进行调整，这是非常消耗计算资源的，并且实际中训练起来会非常慢。<br>负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。<br>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative word”。<br>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。</p><blockquote><p>在论文中，作者指出指出对于小规模数据集，选择5-20个negative words会比较好，对于大规模数据集可以仅选择2-5个negative words。</p></blockquote><p>回忆一下我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新 300 x 6 = 1800 个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。</p><h3 id="如何选择negative-words"><a href="#如何选择negative-words" class="headerlink" title="如何选择negative words"></a>如何选择negative words</h3><p>我们使用“一元模型分布（unigram distribution）”来选择“negative words”。<br>要注意的一点是，一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words。<br>在word2vec的C语言实现中，你可以看到对于这个概率的实现公式。每个单词被选为“negative words”的概率计算公式与其出现的频次有关。<br>代码中的公式实现如下：</p><script type="math/tex; mode=display">P\left( w_i \right) =\frac{f\left( w_i \right) ^{\text{3/}4}}{\sum_{j=0}^n{\left( f\left( w_j \right) ^{\text{3/}4} \right)}}</script><p>每个单词被赋予一个权重，即 $f(ω_i)$， 它代表着单词出现的频次。<br>公式中开3/4的根号完全是基于经验的，论文中提到这个公式的效果要比其它公式更加出色。你可以在google的搜索栏中输入“plot y = x^(3/4) and y = x”，然后看到这两幅图（如下图），仔细观察x在[0,1]区间内时y的取值，$x^{3/4}$ 有一小段弧形，取值在 y = x 函数之上。<br><img src="http://p5vuwy2ht.bkt.clouddn.com/word2vec10.png" alt="此处输入图片的描述"><br>负采样的C语言实现非常的有趣。unigram table有一个包含了一亿个元素的数组，这个数组是由词汇表中每个单词的索引号填充的，并且这个数组中有重复，也就是说有些单词会出现多次。那么每个单词的索引在这个数组中出现的次数该如何决定呢，有公式，也就是说计算出的<strong>负采样概率*1亿=单词在表中出现的次数</strong>。<br>有了这张表以后，每次去我们进行负采样时，只需要在0-1亿范围内生成一个随机数，然后选择表中索引号为这个随机数的那个单词作为我们的negative word即可。一个单词的负采样概率越大，那么它在这个表中出现的次数就越多，它被选中的概率就越大。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型，它被大量地用在自然语言处理（NLP）中。&lt;br&gt;
    
    </summary>
    
      <category term="词向量" scheme="http://yoursite.com/categories/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow常用方法</title>
    <link href="http://yoursite.com/2018/04/21/tensorflow%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/04/21/tensorflow常用方法/</id>
    <published>2018-04-21T01:47:36.000Z</published>
    <updated>2018-04-23T06:41:24.999Z</updated>
    
    <content type="html"><![CDATA[<p>记载一些tensorflow常用方法，方便以后查找。<br><a id="more"></a></p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>激活函数不会更改输入数据的维度，也就是输入和输出的维度是相同的。TensorFlow 中有如下激活函数，它们定义在tensorflow-1.1.0/tensorflow/python/ops/nn.py 文件中。加粗的是常用的函数。</p><h3 id="tf-nn-relu"><a href="#tf-nn-relu" class="headerlink" title="tf.nn.relu()"></a><strong>tf.nn.relu()</strong></h3><p>图片3</p><script type="math/tex; mode=display">y=\begin{cases}0& (x\le0)\\x& (x>0)\end{cases}</script><p>relu 在x<0 时硬饱和。由于x="">0 时导数为1，所以，relu 能够在x&gt;0 时保持梯度不衰减，从而缓解梯度消失问题，还能够更很地收敛，并提供了神经网络的稀疏表达能力。但是，随着训练的进行，部分输入会落到硬饱和区，导致对应的权重无法更新，称为“神经元死亡”。softplus可以看作是ReLU 的平滑版本.</0></p><h3 id="tf-nn-sigmoid"><a href="#tf-nn-sigmoid" class="headerlink" title="tf.nn.sigmoid()"></a><strong>tf.nn.sigmoid()</strong></h3><p>图片1</p><script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}</script><p>sigmoid 函数的优点在于，它的输出映射在$(0,1)$内，单调连续，非常适合用作输出层，并且求导比较容易。但是，它也有缺点，因为软饱和性(软饱和是指激活函数$h(x)$在取值趋于无穷大时，它的一阶导数趋于0。硬饱和是指当$|x| &gt; c$时，其中$c$ 为常数，$f ‘(x)=0$。relu 就是一类左侧硬饱和激活函数)，一旦输入落入饱和区，$f ‘(x)$就会变得接近于0，很容易产生梯度消失。</p><h3 id="tf-nn-tanh"><a href="#tf-nn-tanh" class="headerlink" title="tf.nn.tanh()"></a><strong>tf.nn.tanh()</strong></h3><p>图片2</p><script type="math/tex; mode=display">tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p>tanh 函数也具有软饱和性。因为它的输出以0 为中心，收敛速度比sigmoid 要很。但是仍<br>无法解决梯度消失的问题。</p><h3 id="tf-nn-elu"><a href="#tf-nn-elu" class="headerlink" title="tf.nn.elu()"></a>tf.nn.elu()</h3><h3 id="tf-nn-bias-add"><a href="#tf-nn-bias-add" class="headerlink" title="tf.nn.bias_add()"></a>tf.nn.bias_add()</h3><h3 id="tf-nn-crelu"><a href="#tf-nn-crelu" class="headerlink" title="tf.nn.crelu()"></a>tf.nn.crelu()</h3><h3 id="tf-nn-relu6"><a href="#tf-nn-relu6" class="headerlink" title="tf.nn.relu6()"></a>tf.nn.relu6()</h3><h3 id="tf-nn-softplus"><a href="#tf-nn-softplus" class="headerlink" title="tf.nn.softplus()"></a><strong>tf.nn.softplus()</strong></h3><script type="math/tex; mode=display">f(x)=log(e^x+1)</script><h3 id="tf-nn-softsign"><a href="#tf-nn-softsign" class="headerlink" title="tf.nn.softsign()"></a>tf.nn.softsign()</h3><script type="math/tex; mode=display">f(x)=\frac{x}{|x|+1}</script><h3 id="tf-nn-dropout"><a href="#tf-nn-dropout" class="headerlink" title="tf.nn.dropout()"></a>tf.nn.dropout()</h3><p>防止过拟合，用来舍弃某些神经元</p><h3 id="选择策略"><a href="#选择策略" class="headerlink" title="选择策略"></a>选择策略</h3><p>当输入数据特征相差明显时，用tanh 的效果会很好，且在循环过程中会不断扩大特征效果并显示出来。当特征相差不明显时，sigmoid 效果比较好。同时，用sigmoid 和tanh 作为激活函数时，需要对输入进行规范化，否则激活后的值全部都进入平坦区，隐层的输出会全部趋同，丧失原有的特征表达。而relu 会好很多，有时可以不需要输入规范化来避免上述情况。<br>因此，现在大部分的卷积神经网络都采用relu 作为激活函数。我估计大概有85%～90%的神经网络会采用ReLU，10%～15%的神经网络会采用tanh，尤其用在自然语言处理上。</p><h2 id="分类函数"><a href="#分类函数" class="headerlink" title="分类函数"></a>分类函数</h2><h3 id="sigmoid-cross-entropy-with-logits"><a href="#sigmoid-cross-entropy-with-logits" class="headerlink" title="sigmoid_cross_entropy_with_logits"></a>sigmoid_cross_entropy_with_logits</h3><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><h3 id="log-softmax"><a href="#log-softmax" class="headerlink" title="log_softmax"></a>log_softmax</h3><h3 id="softmax-cross-entropy-with-logits"><a href="#softmax-cross-entropy-with-logits" class="headerlink" title="softmax_cross_entropy_with_logits"></a>softmax_cross_entropy_with_logits</h3><h3 id="sparse-softmax-cross-entropy-with-logits"><a href="#sparse-softmax-cross-entropy-with-logits" class="headerlink" title="sparse_softmax_cross_entropy_with_logits"></a>sparse_softmax_cross_entropy_with_logits</h3><h2 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h2><p>BGD、SGD、Momentum 和Nesterov Momentum 是手动指定学习率的，其余算法能够自动调节学习率。</p><h3 id="梯度下降法（BGD-和SGD）"><a href="#梯度下降法（BGD-和SGD）" class="headerlink" title="梯度下降法（BGD 和SGD）"></a>梯度下降法（BGD 和SGD）</h3><p>class tf.train.GradientDescentOptimizer<br>BGD 的全称是batch gradient descent，即批梯度下降。这种方法是利用现有参数对训练集中的每一个输入生成一个估计输出$y_i$，然后跟实际输出$y_i$ 比较，统计所有误差，求平均以后得到平均误差，以此作为更新参数的依据。它的迭代过程为：<br>（1）提取训练集中的所有内容${x_1, …, x_n}$，以及相关的输出$y_i$；<br>（2）计算梯度和误差并更新参数。<br>这种方法的优点是，使用所有训练数据计算，能够保证收敛，并且不需要逐渐减少学习率；缺点是，每一步都需要使用所有的训练数据，随着训练的进行，速度会越来越慢。那么，如果将训练数据拆分成一个个批次（batch），每次抽取一批数据来更新参数，是不是会加速训练呢？这就是最常用的SGD。<br>SGD 的全称是stochastic gradient descent，即随机梯度下降。因为这种方法的主要思想是将数据集拆分成一个个批次（batch），随机抽取一个批次来计算并更新参数，所以也称为MBGD（minibatch gradient descent）。SGD 在每一次迭代计算mini-batch 的梯度，然后对参数进行更新。与BGD 相比，SGD 在训练数据集很大时，仍能以较很的速度收敛。但是，它仍然会有下面两个缺点。<br>（1）由于抽取不可避免地梯度会有误差，需要手动调整学习率（learning rate），但是选择合适的学习率又比较困难。尤其在训练时，我们常常想对常出现的特征更新速度很一些，而对不常出现的特征更新速度慢一些，而SGD 在更新参数时对所有参数采用一样的学习率，因此无法满足要求。<br>（2）SGD 容易收敛到局部最优，并且在某些情况下可能被困在鞍点。为了解决学习率固定的问题，又引入了Momentum 法。</p><h3 id="Momentum法（Momentum-和Nesterov-Momentum）"><a href="#Momentum法（Momentum-和Nesterov-Momentum）" class="headerlink" title="Momentum法（Momentum 和Nesterov Momentum）"></a>Momentum法（Momentum 和Nesterov Momentum）</h3><p>class tf.train.MomentumOptimizer<br>Momentum 是模拟物理学中动量的概念，更新时在一定程度上保留之前的更新方向，利用当前的批次再微调本次的更新参数，因此引入了一个新的变量v（速度），作为前几次梯度的累加。因此，Momentum 能够更新学习率，在下降初期，前后梯度方向一致时，能够加速学习；在下降的中后期，在局部最小值的附近来回震荡时，能够抑制震荡，加很收敛。<br>Nesterov Momentum 法由Ilya Sutskever 在Nesterov 工作的启发下提出的，是对传统Momentum法的一项改进，其基本思路如图所示。<br>图片4<br>标准Momentum 法首先计算一个梯度（短的1 号线），然后在加速更新梯度的方向进行一个大的跳跃（长的1 号线）；Nesterov 项首先在原来加速的梯度方向进行一个大的跳跃（2 号线），然后在该位置计算梯度值（3 号线），然后用这个梯度值修正最终的更新方向（4 号线）。</p><h3 id="Adagrad法（Adagrad-和AdagradDAO）"><a href="#Adagrad法（Adagrad-和AdagradDAO）" class="headerlink" title="Adagrad法（Adagrad 和AdagradDAO）"></a>Adagrad法（Adagrad 和AdagradDAO）</h3><p>class tf.train.AdagradOptimizer<br>class tf.train.AdagradDAOptimizer<br>Adagrad 法能够自适应地为各个参数分配不同的学习率，能够控制每个维度的梯度方向。这种方法的优点是能够实现学习率的自动更改：如果本次更新时梯度大，学习率就衰减得很一些；如果这次更新时梯度小，学习率衰减得就慢一些。</p><h3 id="Adadelta法"><a href="#Adadelta法" class="headerlink" title="Adadelta法"></a>Adadelta法</h3><p>class tf.train.AdadeltaOptimizer<br>Adagrad 法仍然存在一些问题：其学习率单调递减，在训练的后期学习率非常小，并且需要手动设置一个全局的初始学习率。Adadelta 法用一阶的方法，近似模拟二阶牛顿法，解决了这些问题。</p><h3 id="RMSProp-法"><a href="#RMSProp-法" class="headerlink" title="RMSProp 法"></a>RMSProp 法</h3><p>class tf.train.RMSPropOptimizer<br>RMSProp 法与Momentum 法类似，通过引入一个衰减系数，使每一回合都衰减一定比例。在实践中，对循环神经网络（RNN）效果很好。</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>class tf.train.AdamOptimizer<br>Adam 的名称来源于自适应矩估计（adaptive moment estimation）。Adam 法根据损失函数针对每个参数的梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。</p><h3 id="Ftrl法"><a href="#Ftrl法" class="headerlink" title="Ftrl法"></a>Ftrl法</h3><p>class tf.train.FtrlOptimizer</p><h2 id="模型存储与加载"><a href="#模型存储与加载" class="headerlink" title="模型存储与加载"></a>模型存储与加载</h2><h3 id="模型存储"><a href="#模型存储" class="headerlink" title="模型存储"></a>模型存储</h3><p>模型存储主要是建立一个tf.train.Saver()来保存变量，并且指定保存的位置，一般模型的扩展名为<strong>.ckpt</strong>。<br><a href="https://github.com/zkq1314/CodeReading/blob/master/10_save_restore_net.py" target="_blank" rel="noopener">使用样例</a></p><h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><p>如果有已经训练好的模型变量文件，可以用saver.restore 来进行模型加载：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.initialize_all_variables().run()</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(ckpt_dir)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">    print(ckpt.model_checkpoint_path)</span><br><span class="line">    saver.restore(sess, ckpt.model_checkpoint_path) <span class="comment"># 加载所有的参数</span></span><br><span class="line">    <span class="comment"># 从这里开始就可以直接使用模型进行预测，或者接着继续训练了</span></span><br></pre></td></tr></table></figure></p><h3 id="图的存储与加载"><a href="#图的存储与加载" class="headerlink" title="图的存储与加载"></a>图的存储与加载</h3><p>当仅保存图模型时，才将图写入二进制协议文件中，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v = tf.Variable(<span class="number">0</span>, name=<span class="string">'my_variable'</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">tf.train.write_graph(sess.graph_def, <span class="string">'/tmp/tfmodel'</span>, <span class="string">'train.pbtxt'</span>)</span><br></pre></td></tr></table></figure></p><p>当读取时，又从协议文件中读取出来：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> _sess:</span><br><span class="line">    <span class="keyword">with</span> gfile.FastGFile(<span class="string">"/tmp/tfmodel/train.pbtxt"</span>,<span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        graph_def = tf.GraphDef()</span><br><span class="line">        graph_def.ParseFromString(f.read())</span><br><span class="line">        _sess.graph.as_default()</span><br><span class="line">        tf.import_graph_def(graph_def, name=<span class="string">'tfgraph'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><p>TensorFlow 作为符号编程框架，需要先构建数据流图，再读取数据，随后进行模型训练。TensorFlow 官方网站给出了以下读取数据3 种方法。</p><ul><li>预加载数据（preloaded data）：在TensorFlow 图中定义常量或变量来保存所有数据。</li><li>填充数据（feeding）：Python 产生数据，再把数据填充后端。</li><li>从文件读取数据（reading from file）：从文件中直接读取，让队列管理器从文件中读取数据。</li></ul><h3 id="预加载数据"><a href="#预加载数据" class="headerlink" title="预加载数据"></a>预加载数据</h3><p>预加载数据的示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x1 = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">x2 = tf.constant([<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y = tf.add(x1, x2)</span><br></pre></td></tr></table></figure></p><p>这种方式的缺点在于，将数据直接嵌在数据流图中，当训练数据较大时，很消耗内存。</p><h3 id="填充数据"><a href="#填充数据" class="headerlink" title="填充数据"></a>填充数据</h3><p>使用sess.run()中的feed_dict 参数，将Python 产生的数据填充给后端。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 设计图</span></span><br><span class="line">a1 = tf.placeholder(tf.int16)</span><br><span class="line">a2 = tf.placeholder(tf.int16)</span><br><span class="line">b = tf.add(x1, x2)</span><br><span class="line"><span class="comment"># 用Python 产生数据</span></span><br><span class="line">li1 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">li2 = [<span class="number">4</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 打开一个会话，将数据填充给后端</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"><span class="keyword">print</span> sess.run(b, feed_dict=&#123;a1: li1, a2: li2&#125;)</span><br></pre></td></tr></table></figure></p><p>填充的方式也有数据量大、消耗内存等缺点，并且数据类型转换等中间环节增加了不小开销。这时最好用第三种方法，在图中定义好文件读取的方法，让TensorFlow 自己从文件中读取数据，并解码成可使用的样本集。</p><h3 id="从文件读取数据"><a href="#从文件读取数据" class="headerlink" title="从文件读取数据"></a>从文件读取数据</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记载一些tensorflow常用方法，方便以后查找。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost</title>
    <link href="http://yoursite.com/2018/04/19/AdaBoost/"/>
    <id>http://yoursite.com/2018/04/19/AdaBoost/</id>
    <published>2018-04-19T06:25:06.000Z</published>
    <updated>2018-04-19T06:43:48.885Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h1><h2 id="1-AdaBoost算法"><a href="#1-AdaBoost算法" class="headerlink" title="1. AdaBoost算法"></a>1. AdaBoost算法</h2><p>Boosting是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。<br><a id="more"></a><br>AdaBoost算法是Boosting中的代表算法。对于提升方法来说，有两个问题需要解决：</p><ol><li>在每一轮中如何改变训练数据的权值或概率分布；</li><li>如何将弱分类器组合成一个强分类器。</li></ol><p>AdaBoost的做法为：</p><ol><li>对于第一个问题，AdaBoost提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值；</li><li>对于第二个问题，AdaBoost采用加权多数表决的方式。</li></ol><p>AdaBoost的主要优点：</p><ul><li>AdaBoost作为分类器时，分类精度很高；</li><li>在AdaBoost的框架下，可以使用各种回归分类模型来构建弱学习器，十分灵活；</li><li>作为简单的二分类器时，构造简单，结果可理解；</li><li>不容易发生过拟合。</li></ul><p>AdaBoost的主要缺点：</p><ul><li>对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li></ul><p>现在叙述AdaBoost算法，假设给定一个二类分类的训练数据集</p><script type="math/tex; mode=display">T = \{(\pmb{x}_1,y_1), (\pmb{x}_2,y_2), \cdots, (\pmb{x}_N,y_N)\}</script><p>其中，每个样例点由实例和标记组成，实例$\pmb{x}_i \in \mathbf{R}^n$，标记$y_i \in {-1,+1}$，AdaBoost算法如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost1.png" alt="此处输入图片的描述"></p><p>对AdaBoost算法作如下说明：<br>步骤（1）假设训练数据集具有均匀的权值分布，即每个训练样本在基分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(\pmb{x})$。<br>步骤（2）AdaBoost反复学习基本分类器，在每一轮$m=1,2,\cdots,M$顺次地执行下列操作：<br>（i）使用当前分布$D_m$加权的训练数据集，学习基本分类器$D_m(\pmb{x})$；<br>（ii）计算基本分类器$G_m (\pmb{x})$在加权训练数据集上的分类错误率：</p><script type="math/tex; mode=display">e_m = \sum_{i=1}^N P(G_m(\pmb{x}_i) \neq y_i) = \sum_{G_m(\pmb{x}_i) \neq y_i} w_{mi} \tag{8.8}</script><p>$w<em>{mi}$表示第$m$轮中第$i$个实例的权值，$\sum</em>{i=1}^N w_{mi} = 1$。这表明，$G_m(\pmb{x})$在加权的训练数据集上的分类误差率是被$G_m(\pmb{x})$误分类样本的权值和。<br>（iii）计算基本分类器$G_m(\pmb{x})$的系数$\alpha_m$。$\alpha_m$表示$G_m(\pmb{x})$在最终分类器中的重要性。由式（8.2）可知，当$e_m \leq \frac{1}{2}$时，$\alpha_m \geq 0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。<br>（iiii）更新训练数据的权值分布为下一轮做准备。式（8.4）可以写成：</p><script type="math/tex; mode=display">w_{m+1,i} =     \begin{cases}        \frac{w_{mi}}{Z_m} e^{- \alpha_m}, \quad G_m(\pmb{x}_i) = y_i \\         \frac{w_{mi}}{Z_m} e^{\alpha_m}, \quad G_m(\pmb{x}_i) \neq y_i    \end{cases}</script><p>由此可知，被基本分类器$G_m(\pmb{x})$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两者相比较，误分类样本的权值被放大$e^{2 \alpha_m} = \frac{1 - e_m}{e_m}$倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中其不同的作用，这是AdaBoost的一个特点。  </p><p>步骤（3）线性组合$f(\pmb{x})$实现$M$个基本分类器的加权表决。系数$\alpha_m$表示了基本分类器$G_m(\pmb{x})$的重要性。这里，所有$\alpha_m$之和并不为1，$f(\pmb{x})$的符号决定了实例$\pmb{x}$的类，$f(\pmb{x})$的绝对值表示分类的确信度，利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。  </p><h2 id="2-AdaBoost算法的训练误差分析"><a href="#2-AdaBoost算法的训练误差分析" class="headerlink" title="2. AdaBoost算法的训练误差分析"></a>2. AdaBoost算法的训练误差分析</h2><p>AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。<br><strong>定理1</strong>：AdaBoost算法最终分类器的训练误差界为</p><script type="math/tex; mode=display">\frac{1}{N} \sum_{i=1}^N I(G_m(\pmb{x}_i) \neq y_i) \leq \frac{1}{N} \sum_{i=1} \exp(- y_i f(\pmb{x}_i))= \prod_{m} Z_m</script><p><strong>证明</strong>：当$G_m(\pmb{x}_i) \neq y_i$时，$y_i f(\pmb{x}_i) &lt; 0$，因而$\exp(- y_i f(\pmb{x}_i)) \geq 1$，由此可直接推导出前半部分。<br>后半部分的推导需要用到$Z_m$的定义式（8.5）及式（8.4）的变形：</p><script type="math/tex; mode=display">w_{mi} \exp(- \alpha_m y_i G_m (\pmb{x}_i)) = Z_m w_{m+1,i}</script><p>现推导如下：</p><script type="math/tex; mode=display">\begin{align}    \frac{1}{N} \sum_i \exp(-y_i f(\pmb{x}_i)) &= \frac{1}{N} \sum_i \exp(- \sum_{m=1}^M \alpha_m y_i G_m(\pmb{x}_i)) \nonumber \\    &= \sum_i w_{1i} \prod_{m=1}^M \exp (- \alpha_m y_i G_m(\pmb{x}_i)) \nonumber \\    &= Z_1 \sum_i w_{2i} \prod_{m=2}^M \exp(- \alpha_m y_i G_m(\pmb{x}_i)) \nonumber \\    &= Z_1 Z_2 \sum_i w_{3i} \prod_{m=3}^M \exp(- \alpha_m y_i G_m(\pmb{x}_i)) \nonumber \\    &= \cdots \nonumber \\    &= Z_1 Z_2 \cdots Z_{M-1} \sum_i w_{Mi} \exp(- \alpha_M y_i G_M(\pmb{x}_i)) \nonumber \\    &= \prod_{m=1}^M Z_m \nonumber\end{align}</script><p>这一定理说明，可以在每一轮中选取适当的$G_m$使得$Z_m$最小，从而使训练误差下降最快。  </p><p><strong>定理2</strong>：二类分类问题AdaBoost的训练误差界</p><script type="math/tex; mode=display">\prod_{m=1}^M Z_m = \prod_{m=1}^M [2 \sqrt{e_m(1-e_m)}] = \prod_{m=1}^M \sqrt{(1-4 \gamma_m^2)} \leq \exp(-2 \sum_{m=1}^M \gamma_m^2)</script><p>式中$\gamma_m = \frac{1}{2} - e_m$。<br><strong>证明</strong>：由$Z_m$的定义式（8.5）及式（8.8）得</p><script type="math/tex; mode=display">\begin{align}    Z_m &= \sum_{i=1}^N w_{mi} \exp(- \alpha_m y_i G_m(\pmb{x}_i)) \nonumber \\    &= \sum_{y_i = G_m(\pmb{x}_i)} w_{mi} e^{- \alpha_m} + \sum_{y_i \neq G_m(\pmb{x}_i)} w_{mi} e^{\alpha_m} \nonumber \\    &= (1-e_m) e^{-\alpha_m} + e_m e^{\alpha_m} \nonumber \\    &=  2 \sqrt{e_m (1 - e_m)} \nonumber \\    &= \sqrt{1 - 4 \gamma_m^2} \nonumber\end{align}</script><p>其中对于$\alpha_m$的化简需要将$\alpha_m = \frac{1}{2} \log \frac{1 - e_m}{e_m}$带入。<br>至于不等式</p><script type="math/tex; mode=display">\prod_{m=1}^M \sqrt{(1-4 \gamma_m^2)} \leq \exp(-2 \sum_{m=1}^M \gamma_m^2)</script><p>则可先由$e^x$和$\sqrt{1-x}$在点$x=0$的泰勒展开式推出不等式$\sqrt{(1-4 \gamma_m^2)} \leq \exp(-2 \gamma_m^2)$，进而得到。  </p><p><strong>推论</strong>：如果存在$\gamma &gt; 0$，对所有$m$有$\gamma_m \geq \gamma$，则</p><script type="math/tex; mode=display">\frac{1}{N} \sum_{i=1}^N I(G_m(\pmb{x}_i) \neq y_i) \leq \exp(-2M \gamma^2)</script><p>这表明在此条件下AdaBoost的训练误差是以指数速率下降的。<br>注意，AdaBoost算法不需要知道下界$\gamma$，与早期的提升方法不同，AdaBoost具有适应性，即它梦适应弱分类器各自的训练误差率。  </p><h2 id="3-AdaBoost算法的解释"><a href="#3-AdaBoost算法的解释" class="headerlink" title="3. AdaBoost算法的解释"></a>3. AdaBoost算法的解释</h2><p>AdaBoost算法还有一个解释，即可认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类学习算法。  </p><h3 id="3-1-前向分步算法"><a href="#3-1-前向分步算法" class="headerlink" title="3.1. 前向分步算法"></a>3.1. 前向分步算法</h3><p>考虑加法模型（additive model）</p><script type="math/tex; mode=display">f(\pmb{x}) = \sum_{i=1}^M \beta_m b(\pmb{x};\gamma_m)</script><p>其中，$b(\pmb{x};\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。<br>在给定训练数据及损失函数$L(y,f(\pmb{x}))$的条件下，学习算法模型$f(\pmb{x})$称为经验风险最小化即损失函数极小化问题：</p><script type="math/tex; mode=display">\min_{\beta_m,\gamma_m} \sum_{i=1}^N L(y_i, \sum_{m=1}^M \beta_m b(\pmb{x}_i;\gamma_m))</script><p>通常这是一个复杂的优化问题。前向分步算法（forward stagewise algorithm）求解这一优化问题的想法：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。<br>具体地，每步只需要优化如下损失函数：</p><script type="math/tex; mode=display">\min_{\beta,\gamma} \sum_{i=1}^N L(y_i,\beta b(\pmb{x}_i;\gamma))</script><p>给定训练数据集$T={(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots, (\pmb{x}_N,y_N)}$，$\pmb{x} \in \mathbf{R}^n$，$y_i \in {-1,+1}$，损失函数$L(y,f(\pmb{x}))$和基函数的集合${b(\pmb{x};\gamma)}$，学习加法模型$f(\pmb{x})$的前向分步算法如下：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost2.png" alt="此处输入图片的描述"></p><p>这样，前向分步算法将同时求解从$m=1$到$M$所有的参数$\beta_m,\gamma_m$的优化问题简化为逐次求解各个$\beta_m,\gamma_m$的优化问题。  </p><h3 id="3-2-前向分步算法与AdaBoost"><a href="#3-2-前向分步算法与AdaBoost" class="headerlink" title="3.2. 前向分步算法与AdaBoost"></a>3.2. 前向分步算法与AdaBoost</h3><p><strong>定理</strong>：AdaBoost算法是前向分步加法算法的特例。模型是由基本分类器组成的加法模型，损失函数是指数函数。<br><strong>证明</strong>：前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器</p><script type="math/tex; mode=display">f(\pmb{x}) = \sum_{i=1}^M \alpha_m G_m(\pmb{x})</script><p>由基本分类器$G_m(\pmb{x})$及其系数$\alpha_m$组成，$m=1,2,\cdots,M$。前向分步算法逐一学习基函数，这一过程与AdaBoost算法注意学习基本分类器的过程一致。<br>下面证明前向分步算法的损失函数是指数损失函数</p><script type="math/tex; mode=display">L(y,f(\pmb{x})) = \exp[-yf(\pmb{x})]</script><p>时，其学习的具体操作等价于AdaBoost算法学习的具体操作。<br>假设经过$m-1$轮迭代前向分步算法已经得到$f_{m-1} (\pmb{x})$：</p><script type="math/tex; mode=display">\begin{align}    f_{m-1}(\pmb{x}) &= f_{m-2}(\pmb{x}) + \alpha_{m-1} G_{m-1} (\pmb{x}) \nonumber \\    &=\alpha_1 G_1 (\pmb{x})+ \cdots + \alpha_{m-1} G_{m-1} (\pmb{x}) \nonumber\end{align}</script><p>在第$m$轮迭代得到$\alpha_m,G_m(\pmb{x})$和$f_m(\pmb{x})$。</p><script type="math/tex; mode=display">f_m(\pmb{x}) = f_{m-1}(\pmb{x}) + \alpha_m G_m(\pmb{x})</script><p>目标是使前向分步算法得到的$\alpha_m$和$G_m(\pmb{x})$使$f(\pmb{x})$在训练数据集$T$上的指数损失最小，即</p><script type="math/tex; mode=display">(\alpha_m,G_m(\pmb{x})) = \arg \min_{\alpha,G} \sum_{i=1}^N \exp[-y_i(f_{m-1}(\pmb{x}_i) + \alpha G(\pmb{x}_i))]</script><p>上式还可以表示成</p><script type="math/tex; mode=display">(\alpha_m,G_m(\pmb{x})) = \arg \min_{\alpha,G} \sum_{i=1}^N \bar{w}_{mi} \exp[- y_i \alpha G(\pmb{x}_i)]</script><p>其中，$\bar{w}<em>{mi} = \exp[-y_i f</em>{m-1}(\pmb{x}<em>i)]$，因为$\bar{w}</em>{mi}$既不依赖$\alpha$也不依赖于$G$,所以与最小化无关。但$\bar{w}<em>{mi}$依赖于$f</em>{m-1}(\pmb{x})$，随着每一轮迭代而发生改变。<br>现在需要证明上式达到最小的$\alpha_m^<em>$和$G_m^</em>(\pmb{x})$就是AdaBoost算法所得到的$\alpha_m$和$G_m(\pmb{x})$，求解上式可分为两步：  </p><p>（1）求解$G_m^* (\pmb{x})$。<br>对任意$\alpha &gt; 0$,使式中最小的$G(\pmb{x})$由下式得到：</p><script type="math/tex; mode=display">G_m^* (\pmb{x}) = \arg \min_G \sum_{i=1}^N \bar{w}_{mi} I(y_i \neq G(\pmb{x}_i))</script><p>其中，$\bar{w}<em>{mi} = \exp[-y_i f</em>{m-1}(\pmb{x}_i)]$。<br>此分类器$G_m^*(\pmb{x})$即为AdaBoost算法的基本分类器$G_m(\pmb{x})$，因为它是使第$m$轮加权训练数据分类误差率最小的基本分类器。  </p><p>（2）求解$\alpha_m^*$。  </p><script type="math/tex; mode=display">\begin{align}    h(\alpha) &=\sum_{i=1}^N \bar{w}_{mi} \exp[-y_i \alpha G(\pmb{x}_i)] \nonumber \\    &= \sum_{y_i = G_m(\pmb{x}_i)} \bar{w}_{mi} e^{- \alpha} + \sum_{y_i \neq G_m(\pmb{x}_i)} \bar{w}_{mi} e^{\alpha} \nonumber \\    &= (e^{\alpha} - e^{- \alpha}) \sum_{i=1}^N \bar{w}_{mi} I(y_i \neq G(\pmb{x}_i)) + e^{- \alpha} \sum_{i=1}^N \bar{w}_{mi} \nonumber\end{align}</script><p>将以求得的$G_m^*(\pmb{x})$带入上式，对$\alpha$求导并使导数为0，即求得使原式最小的$\alpha$。<br>令$e_m$表示分类错误率：</p><script type="math/tex; mode=display">e_m = \frac{\sum_{i=1}^N \bar{w}_{mi} I(y_i \neq G_m(\pmb{x}_i))}{\sum_{i=1}^N \bar{w}_{mi}} = \sum_{i=1}^N w_{mi} I(y_i \neq G_m(\pmb{x}_i))</script><p>则有</p><script type="math/tex; mode=display">\begin{equation}\frac{\partial h(\alpha)}{\partial \alpha} = (e^{\alpha} + e^{- \alpha}) \sum_{i=1}^N \bar{w}_{mi} I(y_i \neq G_m(\pmb{x}_i)) - e^{- \alpha} \sum_{i=1}^N \bar{w}_{mi} = 0 \nonumber \\(e^{\alpha} + e^{- \alpha}) \frac{\sum_{i=1}^N \bar{w}_{mi} I(y_i \neq G_m(\pmb{x}_i))}{\sum_{i=1}^N \bar{w}_{mi}} - e^{- \alpha} = 0 \nonumber \\(e^{\alpha} + e^{- \alpha}) e_m - e^{- \alpha} = 0 \nonumber\end{equation}</script><p>求得</p><script type="math/tex; mode=display">\alpha_m^* = \frac{1}{2} \log \frac{1-e_m}{e_m}</script><p>这里的$\alpha_m^*$与AdaBoost算法在第2（c）步求得的$\alpha_m$完全一致。<br>最后再看每一轮样本权值的更新，由</p><script type="math/tex; mode=display">f_m(\pmb{x}) = f_{m-1}(\pmb{x}) + \alpha_m G_m(\pmb{x})</script><p>以及$\bar{w}<em>{mi} = \exp[-y_i f</em>{m-1}(\pmb{x}_i)]$，可得</p><script type="math/tex; mode=display">\bar{w}_{m+1,i} = \bar{w}_{m,i} \exp[-y_i \alpha_m G_m(\pmb{x})]</script><p>这里与AdaBoost算法第2（d）步的样本权值更新只相差规范化因子，因而等价。 </p><h2 id="4-AdaBoost多分类和回归"><a href="#4-AdaBoost多分类和回归" class="headerlink" title="4. AdaBoost多分类和回归"></a>4. AdaBoost多分类和回归</h2><h3 id="4-1-多分类"><a href="#4-1-多分类" class="headerlink" title="4.1. 多分类"></a>4.1. 多分类</h3><p>原始的AdaBoost算法是针对二分类问题的，多分类问题采用的也是将其分解为多个二分类问题。而SAMME和SAMME.R算法不采用这种思路，只是二分类的推广，也是加法模型，指数损失函数和前向分步算法，而且每个弱分类器的正确率只要大于0.5即可。<br>假设训练集$T = {(\pmb{x}<em>1,y_1),(\pmb{x}_2,y_2),\cdots,(\pmb{x}_n,y_n)}$，训练集在第$k$个弱分类器上的样本权重是$(w</em>{k1},w<em>{k2},\cdots,w</em>{kn}),w_{1i} = \frac{1}{n},i=1,2,\cdots,n$，输出$y_i \in {1,2,\cdots,K}$，$K$是类别个数。  </p><p><strong>SAMME</strong></p><p><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost3.png" alt="此处输入图片的描述"><br><strong>SAMME.R</strong><br><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost4.png" alt="此处输入图片的描述"><br>参考文献：Zhu J, Zou H, Rosset S, et al. Multi-class AdaBoost[J]. Statistics &amp; Its Interface, 2006, 2(3):349-360.</p><h3 id="4-2-回归"><a href="#4-2-回归" class="headerlink" title="4.2. 回归"></a>4.2. 回归</h3><p>假设训练集为$T={(\pmb{x}_1,y_1),(\pmb{x}_2,y_2),\cdots,(\pmb{x}_N,y_N)}$。<br>输入：训练数据集，弱学习算法和弱学习器个数T<br>输出：最终的强学习器$G(\pmb{x})$  </p><ol><li>初始化样本权重$D<em>1 = (w</em>{11},w<em>{12},\cdots,w</em>{1n}),\quad w_{1i}=\frac{1}{n},\quad i=1,2,\cdots,N$。   </li><li>对$m=1,2,\cdots,T$：<br> &emsp;&emsp;1. 使用具有样本权重$D<em>m$的训练数据和弱学习器算法得到弱学习器$G_m(\pmb{x})$;<br> &emsp;&emsp;2. 计算弱学习器在训练集上上的误差$e</em>{mi} = L(|y_i - G_m(\pmb{x}_i)|)$，损失函数$L$可以具有任意形式但必须保证$L \in [0,1]$，令$E_m = sup|y_i - G_m(\pmb{x}_i)|,i=1,2,\cdots,N$，即训练集上的最大误差，然后我们会有三种可选用的损失函数：  <pre><code> &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;1. 线性误差：$e_{mi} = \frac{|y_i - G_m(\pmb{x}_i)|}{E_m}$；   &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;2. 平方误差：$e_{mi} = \frac{|y_i - G_m(\pmb{x}_i)|^2}{E_m}$；   &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;3. 指数误差：$e_{mi} = 1 - \exp[\frac{|y_i - G_m(\pmb{x}_i)|}{E_m}]$；  </code></pre> &emsp;&emsp;3. 利用每个样本的相对误差，计算回归误差率$e<em>m = \sum</em>{i=1}^N w<em>{mi} e</em>{mi}$；<br> &emsp;&emsp;4. 计算弱学习器的权重系数$\alpha<em>m = \frac{e_m}{1 - e_m}$；<br> &emsp;&emsp;5. 更新样本权重$w</em>{m+1,i} = \frac{w<em>{mi}}{Z_m} e_m^{1 - e</em>{mi}}$，其中$Z<em>m$为归一化因子，$Z_m = \sum</em>{i=1}^N w<em>{mi} e_m^{1 - e</em>{mi}}$；  </li><li>根据$T$个弱学习器得到最终的强学习器，利用强学习器进行预测的策略为：<script type="math/tex; mode=display">G = inf\{y \in Y: \sum_{m:G_m \leq y} \log(\frac{1}{e_m}) \geq \frac{1}{2} \sum_m \log(\frac{1}{e_m}) \}</script>计算的是加权中值，对于实例$\pmb{x}_i$，每一个弱学习器$G_m$都会产生输出概率$y_i^{(m)}$，并且弱学习器$G_m$对应的权重系数为$\alpha_m$，我们对所有弱学习器的输出值进行排序并重新标记：<script type="math/tex; mode=display">y_i^{(1)} < y_i^{(2)} < \cdots < y_i^{(T)}</script>但保持每个学习器$G_m$与其对应的权重系数$\alpha_m$不变。按照上述排序依次累加各弱学习器对应的$\log(\frac{1}{e_m})$直到找到满足公式中不等式的最小的$m$值，则将弱学习器$G_m$的预测值作为最终强学习器的预测值。</li></ol><p>参考文献：Drucker H. Improving Regressors using Boosting Techniques[C]// Fourteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc. 1997:107-115.</p><h2 id="5-sklearn使用"><a href="#5-sklearn使用" class="headerlink" title="5. sklearn使用"></a>5. sklearn使用</h2><p>sklearn中AdaBoost类库包含AdaBoostClassifier和AdaBoostRegressor两个，其中AdaBoostClassifier用于分类，AdaBoostRegressor用于回归。  </p><p>AdaBoostClassifier：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">ensemble</span>.<span class="title">AdaboostClassifier</span><span class="params">(base_estimator=None, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">1.0</span>, algorithm=<span class="string">'SAMME.R'</span>, random_state=None)</span></span></span><br></pre></td></tr></table></figure><p>AdaBoostRegressor:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">ensemble</span>.<span class="title">AdaBoostRegressor</span><span class="params">(base_estimator=None, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">1.0</span>, loss=’linear’, random_state=None)</span></span></span><br></pre></td></tr></table></figure><p><strong>框架参数</strong><br><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost5.png" alt="此处输入图片的描述"><br><strong>algorithm</strong>：AdaBoostClassifier中有一个algorithm参数，而在AdaBoostRegressor中没有。这是因为sklearn中实现了两种AdaBoost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用的是上文介绍过的二分类AdaBoost算法的扩展，即用对样本集分类效果作为弱学习器的权重。而SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值为SAMME.R。如果使用SAMME.R，则弱分类学习器base_estimator必须限制使用支持概率预测的分类器，而SAMME算法则没有这个限制。  </p><p><strong>函数</strong><br><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost6.png" alt="此处输入图片的描述"><br><strong>属性</strong><br><img src="http://p5vuwy2ht.bkt.clouddn.com/adaboost7.png" alt="此处输入图片的描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;AdaBoost&quot;&gt;&lt;a href=&quot;#AdaBoost&quot; class=&quot;headerlink&quot; title=&quot;AdaBoost&quot;&gt;&lt;/a&gt;AdaBoost&lt;/h1&gt;&lt;h2 id=&quot;1-AdaBoost算法&quot;&gt;&lt;a href=&quot;#1-AdaBoost算法&quot; class=&quot;headerlink&quot; title=&quot;1. AdaBoost算法&quot;&gt;&lt;/a&gt;1. AdaBoost算法&lt;/h2&gt;&lt;p&gt;Boosting是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Boosting" scheme="http://yoursite.com/tags/Boosting/"/>
    
  </entry>
  
  <entry>
    <title>SMO-序列最小优化</title>
    <link href="http://yoursite.com/2018/04/18/SMO-%E5%BA%8F%E5%88%97%E6%9C%80%E5%B0%8F%E4%BC%98%E5%8C%96/"/>
    <id>http://yoursite.com/2018/04/18/SMO-序列最小优化/</id>
    <published>2018-04-18T05:44:07.000Z</published>
    <updated>2018-04-18T07:57:39.227Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SMO-序列最小最优化算法"><a href="#SMO-序列最小最优化算法" class="headerlink" title="SMO-序列最小最优化算法"></a>SMO-序列最小最优化算法</h1><p>支持向量机的学习问题可以形式化为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解，并且有许多最优化算法可以用于这一问题的求解。序列最小最优化算法就是其中一种。<br><a id="more"></a><br>序列最小最优化算法（sequential minimal optimization，SMO）是一种启发式算法，其基本思路：如果所有变量的解都满足此问题的KKT条件，那么这个最优化问题的解就得到了。因为<strong>KKT条件是该最优化问题的充分必要条件</strong>。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题，这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样可以大大提高整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。<br>整个SMO算法包括两部分：</p><ul><li>求解两个变量二次规划的解析方法；</li><li>选择变量的启发式方法。</li></ul><p>在支持向量机中，SMO算法要解决如下的凸二次规划的对偶问题：</p><script type="math/tex; mode=display">\begin{align}    & \min_{\pmb{\alpha}} \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(\pmb{x}_i, \pmb{x}_j) - \sum_{i=1}^N \alpha_i \nonumber \\    & s.t. \quad \sum_{i=1}^N \alpha_i y_i = 0 \nonumber \\    & \qquad \quad 0 \leq \alpha_i \leq C, \quad i=1,2,\cdots,N \nonumber\end{align}</script><h2 id="1-两个变量二次规划的求解问题"><a href="#1-两个变量二次规划的求解问题" class="headerlink" title="1. 两个变量二次规划的求解问题"></a>1. 两个变量二次规划的求解问题</h2><p>假设选择的两个变量是$\alpha_1,\alpha_2$，其他变量$\alpha_i(i=3,4,\cdots,N)$是固定的。于是SMO要解决的对偶问题的子问题可以写成：</p><script type="math/tex; mode=display">\begin{align}    & \min_{\alpha_1, \alpha_2} \quad W(\alpha_1,\alpha_2) = \frac{1}{2} K_{11} \alpha_1^2 + \frac{1}{2} K_{22} \alpha_2^2 + y_1 y_2 K_{12} \alpha_1 \alpha_2 - (\alpha_1 + \alpha_2)  \\    & \qquad \qquad \qquad \qquad + y_1 \alpha_1 \sum_{i=3}^N y_i \alpha_i K_{i1} + y_2 \alpha_2 \sum_{i=3}^N y_i \alpha_i K_{i2} \nonumber \\    & s.t. \qquad \alpha_1y_1 + \alpha_2y_2 = - \sum_{i=3}^N y_i \alpha_i = \varsigma  \\    & \qquad \qquad 0 \leq \alpha_i \leq C, \quad i=1,2 \end{align}</script><p>其中，$K_{ij} = K(\pmb{x}_i, \pmb{x}_j), i,j=1,2,\cdots,N$，$\varsigma$为常数，由于目标函数式中不含$\alpha_1$和$\alpha_2$的项为常数，因此省略。<br>由于只有两个变量$(\alpha_1,\alpha_2)$，约束可以用二维空间中的图形表示：<br><img src="http://p5vuwy2ht.bkt.clouddn.com/SMO1.png" alt="此处输入图片的描述"></p><p>对偶问题中的不等式约束使得$(\alpha_1,\alpha_2)$在盒子$[0,C] \times [0,C]$内，等式约束使$(\alpha_1,\alpha_2)$在平行于盒子$[0,C] \times [0,C]$的对角线的直线上。因此要求的是目标函数在一条平行于对角线的线段上的最优值。这使得两个变量的最优化问题成为实质上的单变量的最优化问题，我们可以考虑为变量$\alpha_2$的最优化问题。<br>假设原对偶问题的初始可行解为$\alpha_1^{old}, \alpha_2^{old}$，最优解为$\alpha_1^{new}, \alpha_2^{new}$，并且假设在沿着约束方向未经剪辑时$\alpha_2$的最优解为$\alpha_2^{new,unc}$。<br>由于$\alpha_2^{new}$需要满足不等式约束，所以其取值范围为</p><script type="math/tex; mode=display">L \leq \alpha_2^{new} \leq H</script><p>其中，$l$与$H$是$\alpha_2^{new}$所在的对角线段端点的界：<br>如果$y_1 \neq y_2$，则</p><script type="math/tex; mode=display">L = \max(0,\alpha_2^{old} - \alpha_1^{old}), \quad H = \min(C, C + \alpha_2^{old} - \alpha_1^{old})</script><p>如果$y_1 = y_2$，则</p><script type="math/tex; mode=display">L = \max(0, \alpha_2^{old} + \alpha_1^{old} - C), \quad H = \min (C,\alpha_2^{old} + \alpha_1^{old})</script><p>首先求解沿着约束方向未经剪辑即未考虑不等式约束时$\alpha_2$的最优解$\alpha_2^{new,unc}$；然后再求剪辑后$\alpha_2$的解$\alpha_2^{new}$。为了叙述简单，记</p><script type="math/tex; mode=display">g(\pmb{x}) = \sum_{i=1}^N \alpha_i y_i K(\pmb{x}_i, \pmb{x}) + b</script><p>令</p><script type="math/tex; mode=display">E_i = g(\pmb{x}_i) - y_i = (\sum_{j=1}^N \alpha_j y_j K(\pmb{x}_j,\pmb{x}_i)+b)-y_i, \quad i=1,2</script><p>当$i=1,2$时，$E_i$为函数$g(\pmb{x})$对输入$\pmb{x}_i$的预测值与真实输出$y_i$之差。  </p><p><strong>定理</strong>：最优化问题$(1) \sim (3)$沿着约束方向向未经剪辑时的解是</p><script type="math/tex; mode=display">\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1 - E_2)}{\eta}</script><p>其中</p><script type="math/tex; mode=display">\eta = K_{11} + K_{22} - 2 K_{12} = ||\phi(\pmb{x}_1) - \phi(\pmb{x}_2)||^2</script><p>$\phi(\pmb{x})$是输入空间到特征空间的映射。<br>经剪辑后$\alpha_2$的解是</p><script type="math/tex; mode=display">\alpha_2^{new} = \begin{cases}    H, \qquad \qquad \alpha_2^{new,unc} > H \\    \alpha_2^{new,unc}, \quad L \leq \alpha_2^{new,unc} \leq H \\    L, \qquad \qquad \alpha_2^{new,unc} < L\end{cases}</script><p>由$\alpha_2$求得$\alpha_1^{new}$是</p><script type="math/tex; mode=display">\alpha_1^{new} = \alpha_1^{old} + y_1 y_2 (\alpha_2^{old} - \alpha_2^{new})</script><p><strong>证明</strong>：引进记号</p><script type="math/tex; mode=display">v_i = \sum_{j=3}^N \alpha_j y_j K(\pmb{x}_i,\pmb{x}_j) = g(\pmb{x}_i) - \sum_{j=1}^2 \alpha_j y_j K(\pmb{x}_i,\pmb{x}_j) - b, \quad i=1,2</script><p>目标函数可写成</p><script type="math/tex; mode=display">W(\alpha_1,\alpha_2) = \frac{1}{2} K_{11} \alpha_1^2 + \frac{1}{2} K_{22} \alpha_2^2 + y_1 y_2 K_{12} \alpha_1 \alpha_2 - (\alpha_1 + \alpha_2) + y_1 v_1 \alpha_1 + y_2 v_2 \alpha_2</script><p>由$\alpha_1 y_1 = \varsigma - \alpha_2 y_2$及$y_i^2 = 1$，可将$\alpha_1$表示为</p><script type="math/tex; mode=display">\alpha_1 = (\varsigma - y_2 \alpha_2) y_1</script><p>带入目标函数，得到只是$\alpha_2$的函数的目标函数：</p><script type="math/tex; mode=display">\begin{align}W(\alpha_2) = & \frac{1}{2} K_{11} (\varsigma - \alpha_2 y_2)^2 + \frac{1}{2} K_{22} \alpha_2^2 + y_2 K_{12} (\varsigma - \alpha_2 y_2) \alpha_2 - (\varsigma - \alpha_2 y_2) y_1 - \alpha_2 \nonumber \\& + v1(\varsigma - \alpha_2 y_2) + y_2 v_2 \alpha_2 \nonumber \end{align}</script><p>对$\alpha_2$求导数</p><script type="math/tex; mode=display">\frac{\partial W}{\partial \alpha_2} = K_{11} \alpha_2 + K_{22} \alpha_2 - 2K_{12} \alpha_2 - K_{11} \varsigma y_2 + K_{12} \varsigma y_2 -1 - v_1 y_2 + y_2 v_2</script><p>令其为0，得到</p><script type="math/tex; mode=display">\begin{align}    (K_{11}+K_{22}-2K_{12}) \alpha_2 & = y_2(y_2 - y_1 + \varsigma K_{11} - \varsigma K_{12} + v_1 - v_2) \nonumber \\    &= y_2 [y_2 - y_1 + \varsigma K_{11} - \varsigma K_{12} + (g(\pmb{x}_1) - \sum_{j=1}^2 y_j \alpha_j K_{1j} - b) \nonumber \\    & \quad - (g(\pmb{x}_2) - \sum_{j=1}^2 y_j \alpha_j K_{2j} - b)] \nonumber \end{align}</script><p>将$\varsigma = \alpha_1^{old} y_1 + \alpha_2^{old} y_2$带入，得到</p><script type="math/tex; mode=display">\begin{align}    (K_{11} + K_{22} - 2K_{12}) \alpha_2^{new,unc} &= y_2((K_{11} + K_{22} - 2K_{12})\alpha_2^{old}y_2 + y_2 - y_1  + g(\pmb{x}_1) - g(\pmb{x}_2)) \nonumber \\    &= (K_{11} + K_{22} - 2K_{12}) + y_2(E_1 - E_2) \nonumber\end{align}</script><p>将$\eta = K<em>{11} + K</em>{22} - 2K_{12}$代入，得到</p><script type="math/tex; mode=display">\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1 - E_2)}{\eta}</script><p>由等式约束得到$\alpha_1^{new}$。  </p><h2 id="2-变量的选择方法"><a href="#2-变量的选择方法" class="headerlink" title="2. 变量的选择方法"></a>2. 变量的选择方法</h2><p>SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。  </p><h3 id="2-1-第一个变量的选择"><a href="#2-1-第一个变量的选择" class="headerlink" title="2.1. 第一个变量的选择"></a>2.1. 第一个变量的选择</h3><p>SMO称选择第一个变量的过程为外层循环。外层循环在训练样本中选取<strong>违反KKT条件最严重的样本点，并将其对应的变量作为第一个变量</strong>。具体地，检验训练样本点$(\pmb{x}_i,y_i)$是否满足KKT条件，即</p><script type="math/tex; mode=display">\begin{equation}    \alpha_i = 0 \Leftrightarrow y_i g(\pmb{x}_i) \geq 1 \nonumber \\    0 < \alpha_i < C \Leftrightarrow y_i g(\pmb{x}_i) = 1 \nonumber \\    \alpha_i = C \Leftrightarrow y_i g(\pmb{x}_i) \leq 1 \nonumber\end{equation}</script><p>其中，$g(\pmb{x}<em>i) = \sum</em>{j=1}^N \alpha_j y_j K(\pmb{x}_i,\pmb{x}_j) + b$。<br>在检验过程中，外层循环首先遍历所有满足条件$0 &lt; \alpha_i &lt; C$的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件。如果这些样本点都满足KKT条件，那么遍历整个训练集，检验它们是否满足KKT条件。  </p><h3 id="2-2-第二个变量的选择"><a href="#2-2-第二个变量的选择" class="headerlink" title="2.2. 第二个变量的选择"></a>2.2. 第二个变量的选择</h3><p>SMO算法称选择第二个变量的过程为内层循环。假设在外层循环中已经找到第一个变量$\alpha_1$，现在要在内层循环中找到第二个变量$\alpha_2$。<strong>第二个变量选择的标准是希望能使$\alpha_2$有足够大的变化</strong>。<br>由定理可知，$\alpha_2^{new}$是依赖于$|E_1 - E_2|$的，为了加快计算速度，一种简单的做法是选择$\alpha_2$使其对应的$|E_1 - E_2|$最大。由于$\alpha_1$已经确定，因此$E_1$也确定了。如果$E_1$是正的，那么选择最小的$E_i$作为$E_2$；如果$E_1$是负的，那么选择最大的$E_i$作为$E_2$。为了节省计算时间，可将所有的$E_i$值保存在一个列表中。<br>在特殊情况下，如果内层循环通过以上方法选择的$\alpha_2$不能使目标函数有足够的下降，那么采用以下启发式规则继续选择$\alpha_2$。遍历在间隔边界上的支持向量点，依次将其对应的向量作为$\alpha_2$试用，直到目标函数有足够的下降。若找不到合适的$\alpha_2$，那么遍历训练数据集；若仍找不到合适的$\alpha_2$，则放弃第一个$\alpha_1$，再通过外层循环寻找另外的$\alpha_1$。  </p><h3 id="2-3-计算阈值-b-和差值-E-i"><a href="#2-3-计算阈值-b-和差值-E-i" class="headerlink" title="2.3. 计算阈值$b$和差值$E_i$"></a>2.3. 计算阈值$b$和差值$E_i$</h3><p>在每次完成两个变量的优化后，都要重新计算阈值$b$。当$0 &lt; \alpha_1^{new} &lt; C$时，由KKT条件可知：</p><script type="math/tex; mode=display">\sum_{i=1}^N \alpha_i y_i K_{i1} + b = y_1</script><p>于是</p><script type="math/tex; mode=display">b_1^{new} = y_1 - \sum_{i=3}^N \alpha_i y_iK_{i1} - \alpha_1^{new} y_1 K_{11} - \alpha_2^{new} y_2 K_{21} \tag{4}</script><p>由$E_1$的定义式有</p><script type="math/tex; mode=display">E_1 = \sum_{i=3}^N \alpha_i y_i K_{i1} + \alpha_1^{old} y_1 K_{11} + \alpha_2^{old} y_2 K_{21} + b^{old} - y_1</script><p>式$(4)$的前两项可写成：</p><script type="math/tex; mode=display">y_1 - \sum_{i=3}^N \alpha_i y_i K_{i1} = - E_1 + \alpha_1^{old} y_1 K_{11} + \alpha_2^{old} y_2 K_{21} + b^{old}</script><p>带入式$(4)$可得</p><script type="math/tex; mode=display">b_1^{new} = - E_1 - y_1 K_{11} (\alpha_1^{new} - \alpha_1^{old}) - y_2 K_{21} (\alpha_2^{new} - \alpha_2^{old}) + b^{old}</script><p>同样，如果$0&lt;\alpha_2^{new} &lt; C$，那么</p><script type="math/tex; mode=display">b_2^{new} = - E_2 - y_1 K_{12} (\alpha_1^{new} - \alpha_1^{old}) - y_2 K_{22} (\alpha_2^{new} - \alpha_2^{old}) + b^{old}</script><p>如果$\alpha_1^{new}, \alpha_2^{new}$同时满足条件$0&lt;\alpha_i^{new} &lt; C, \quad i=1,2$，那么$b_1^{new} = b_2^{new}$。如果$\alpha_1^{new}, \alpha_2^{new}$是$0$或者$C$，那么$b_1^{new}$和$b_2^{new}$以及它们之间的数都是符合KKT条件的阈值，这时选择它们的中点作为$b^{new}$。<br>在每次完成两个变量的优化之后，还必须更新对应的$E_i$值，并将它们保存在列表中。$E_i$值的更新要用的$b^{new}$值，以及所有支持向量对应的$\alpha_j$：</p><script type="math/tex; mode=display">E_i^{new} = \sum_{S} y_j \alpha_j K(\pmb{x}_i, \pmb{x}_j) + b^{new} - y_i</script><p>其中，$S$是所有支持向量$\pmb{x}_j$的集合。  </p><p><img src="http://p5vuwy2ht.bkt.clouddn.com/SMO2.png" alt="此处输入图片的描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;SMO-序列最小最优化算法&quot;&gt;&lt;a href=&quot;#SMO-序列最小最优化算法&quot; class=&quot;headerlink&quot; title=&quot;SMO-序列最小最优化算法&quot;&gt;&lt;/a&gt;SMO-序列最小最优化算法&lt;/h1&gt;&lt;p&gt;支持向量机的学习问题可以形式化为求解凸二次规划问题，这样的凸二次规划问题具有全局最优解，并且有许多最优化算法可以用于这一问题的求解。序列最小最优化算法就是其中一种。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
